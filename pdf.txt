XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290, 2019.

[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.

[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.

[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294–6305, 2017.

[20] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.

[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759, 2016.

[22] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993, 2019.

[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
ﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep., 2011.

[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.

[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[28] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network for multiple-choice

reading comprehension. arXiv preprint arXiv:1903.03033, 2019.

[29] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB
Endowment, 11(3):269–282, 2017.

[30] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach-
man, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint
arXiv:1611.09830, 2016.

[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848, 2019.

13

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290, 2019.

[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.

[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.

[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294–6305, 2017.

[20] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.

[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759, 2016.

[22] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993, 2019.

[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
ﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep., 2011.

[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.

[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[28] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network for multiple-choice

reading comprehension. arXiv preprint arXiv:1903.03033, 2019.

[29] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB
Endowment, 11(3):269–282, 2017.

[30] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach-
man, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint
arXiv:1611.09830, 2016.

[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848, 2019.

13

[36] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for document
ranking. In Proceedings of the 40th International ACM SIGIR conference on research and
development in information retrieval, pages 763–772. ACM, 2017.

[37] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval, pages 55–64. ACM, 2017.

[38] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax

bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

[39] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381,
2019.

[40] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.

[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision, pages 19–27, 2015.

14

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290, 2019.

[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.

[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.

[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294–6305, 2017.

[20] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.

[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759, 2016.

[22] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993, 2019.

[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
ﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep., 2011.

[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.

[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[28] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network for multiple-choice

reading comprehension. arXiv preprint arXiv:1903.03033, 2019.

[29] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB
Endowment, 11(3):269–282, 2017.

[30] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach-
man, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint
arXiv:1611.09830, 2016.

[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848, 2019.

13

[36] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for document
ranking. In Proceedings of the 40th International ACM SIGIR conference on research and
development in information retrieval, pages 763–772. ACM, 2017.

[37] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval, pages 55–64. ACM, 2017.

[38] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax

bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

[39] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381,
2019.

[40] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.

[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision, pages 19–27, 2015.

14

A Target-Aware Representation via Two-Stream Self-Attention

A.1 A Concrete Example of How Standard LM Parameterization Fails

In this section, we provide a concrete example to show how the standard language model parameteri-
zation fails under the permutation objective, as discussed in Section 2.3. Speciﬁcally, let’s consider
two different permutations z(1) and z(2) satisfying the following relationship

Then, substituting the two permutations respectively into the naive parameterization, we have

z(1)
<t = z(2)

but

<t = z<t

t = i (cid:54)= j = z(2)
z(1)
(cid:124)
(cid:125)
= pθ(Xj = x | xz<t )

exp(cid:0)e(x)(cid:62)h(xz<t )(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)h(xz<t ))

(cid:80)

(cid:123)(cid:122)

=

.

t

.

t =j, z(2)
z(1)

<t =z<t

(cid:125)
(cid:124)
pθ(Xi = x | xz<t)

(cid:123)(cid:122)

t =i, z(1)
z(1)

<t =z<t

Effectively, two different target positions i and j share exactly the same model prediction. However,
the ground-truth distribution of two positions should certainly be different.

A.2 Two-Stream Attention

Here, we provide the implementation details of the two-stream attention with a Transformer-XL
backbone.

Initial represetation:

∀t = 1, . . . , T :

ht = e(xt)

and gt = w

Cached layer-m content represetation (memory) from previous segment: ˜h(m)
For the Transformer-XL layer m = 1,··· , M, attention with relative positional encoding and
position-wise feed-forward are consecutively employed to update the represetntations:

∀t = 1, . . . , T :

ˆ
h(m)
zt

h(m)
zt

g(m)
ˆ
zt

g(m)
zt

= LayerNorm

h(m−1)

+ RelAttn

h(m−1)

,

= LayerNorm

+ PosFF

= LayerNorm

g(m−1)

+ RelAttn

g(m−1)

,

(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)
(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)

z≤t

z≤t

zt

(cid:16)
(cid:16)ˆh(m)
(cid:17)(cid:17)
(cid:16)
(cid:16)
(cid:17)(cid:17)

zt

zt

g(m)
ˆ
zt

zt

(cid:16)
(cid:16)ˆh(m)
(cid:16)
(cid:16)

zt

zt

g(m)
ˆ
zt

Target-aware prediction distribution:

pθ(Xzt = x | xz<t ) =

= LayerNorm

+ PosFF

(cid:17)

(cid:16)

(cid:16)

(cid:80)

exp

e(x)(cid:62)g(M )

zt

x(cid:48) exp

e(x(cid:48))(cid:62)g(M )

zt

(cid:17) ,

A.3 Hyperparameters

A.3.1 Pretraining Hyperparameters

The hyperparameters used for pretraining XLNet are shown in Table 7.

A.3.2 Hyperparameters for Finetuning

The hyperparameters used for ﬁnetuning XLNet on various tasks are shown in Table 8. “Layer-wise
decay” means exponentially decaying the learning rates of individual layers in a top-down manner.
For example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is α, then
the learning rate of layer m is lα24−m.

15

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290, 2019.

[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.

[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.

[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294–6305, 2017.

[20] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.

[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759, 2016.

[22] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993, 2019.

[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
ﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep., 2011.

[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.

[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[28] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network for multiple-choice

reading comprehension. arXiv preprint arXiv:1903.03033, 2019.

[29] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB
Endowment, 11(3):269–282, 2017.

[30] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach-
man, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint
arXiv:1611.09830, 2016.

[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848, 2019.

13

[36] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for document
ranking. In Proceedings of the 40th International ACM SIGIR conference on research and
development in information retrieval, pages 763–772. ACM, 2017.

[37] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval, pages 55–64. ACM, 2017.

[38] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax

bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

[39] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381,
2019.

[40] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.

[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision, pages 19–27, 2015.

14

A Target-Aware Representation via Two-Stream Self-Attention

A.1 A Concrete Example of How Standard LM Parameterization Fails

In this section, we provide a concrete example to show how the standard language model parameteri-
zation fails under the permutation objective, as discussed in Section 2.3. Speciﬁcally, let’s consider
two different permutations z(1) and z(2) satisfying the following relationship

Then, substituting the two permutations respectively into the naive parameterization, we have

z(1)
<t = z(2)

but

<t = z<t

t = i (cid:54)= j = z(2)
z(1)
(cid:124)
(cid:125)
= pθ(Xj = x | xz<t )

exp(cid:0)e(x)(cid:62)h(xz<t )(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)h(xz<t ))

(cid:80)

(cid:123)(cid:122)

=

.

t

.

t =j, z(2)
z(1)

<t =z<t

(cid:125)
(cid:124)
pθ(Xi = x | xz<t)

(cid:123)(cid:122)

t =i, z(1)
z(1)

<t =z<t

Effectively, two different target positions i and j share exactly the same model prediction. However,
the ground-truth distribution of two positions should certainly be different.

A.2 Two-Stream Attention

Here, we provide the implementation details of the two-stream attention with a Transformer-XL
backbone.

Initial represetation:

∀t = 1, . . . , T :

ht = e(xt)

and gt = w

Cached layer-m content represetation (memory) from previous segment: ˜h(m)
For the Transformer-XL layer m = 1,··· , M, attention with relative positional encoding and
position-wise feed-forward are consecutively employed to update the represetntations:

∀t = 1, . . . , T :

ˆ
h(m)
zt

h(m)
zt

g(m)
ˆ
zt

g(m)
zt

= LayerNorm

h(m−1)

+ RelAttn

h(m−1)

,

= LayerNorm

+ PosFF

= LayerNorm

g(m−1)

+ RelAttn

g(m−1)

,

(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)
(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)

z≤t

z≤t

zt

(cid:16)
(cid:16)ˆh(m)
(cid:17)(cid:17)
(cid:16)
(cid:16)
(cid:17)(cid:17)

zt

zt

g(m)
ˆ
zt

zt

(cid:16)
(cid:16)ˆh(m)
(cid:16)
(cid:16)

zt

zt

g(m)
ˆ
zt

Target-aware prediction distribution:

pθ(Xzt = x | xz<t ) =

= LayerNorm

+ PosFF

(cid:17)

(cid:16)

(cid:16)

(cid:80)

exp

e(x)(cid:62)g(M )

zt

x(cid:48) exp

e(x(cid:48))(cid:62)g(M )

zt

(cid:17) ,

A.3 Hyperparameters

A.3.1 Pretraining Hyperparameters

The hyperparameters used for pretraining XLNet are shown in Table 7.

A.3.2 Hyperparameters for Finetuning

The hyperparameters used for ﬁnetuning XLNet on various tasks are shown in Table 8. “Layer-wise
decay” means exponentially decaying the learning rates of individual layers in a top-down manner.
For example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is α, then
the learning rate of layer m is lα24−m.

15

Hparam
Number of layers
Hidden size
Number of attention heads
Attention head size
FFN inner hidden size
Dropout
Attention dropout
Partial prediction K
Max sequence length
Memory length
Batch size
Learning rate
Number of steps
Warmup steps
Learning rate decay
Adam epsilon
Weight decay

Value

24
1024
16
64
4096
0.1
0.1
6
512
384
2048
1e-5
500K
20,000
linear
1e-6
0.01

Table 7: Hyperparameters for pretraining.

Hparam
Dropout
Attention dropout
Max sequence length
Batch size
Learning rate
Number of steps
Learning rate decay
Weight decay
Adam epsilon
Layer-wise lr decay

RACE SQuAD MNLI Yelp-5

512
32
2e-5
12K

0.1
0.1

512
48
3e-5
8K

128
128
3e-5
10K

linear
0.00

512
128
2e-5
10K

1e-6
1.0

1e-6
1.0
Table 8: Hyperparameters for ﬁnetuning.

1e-6
1.0

1e-6
0.75

A.4 Visualizing Memory and Permutation

In this section, we provide a detailed visualization of the proposed permutation language modeling
objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use
attention masks to permute the factorization order, and the difference of the two attention streams.
As shown in Figure 3 and 4, given the current position zt, the attention mask is decided by the
permutation (or factorization order) z such that only tokens the occur before zt in the permutation can
be attended; i.e., positions zi with i < t. Moreover, comparing Figure 3 and 4, we can see how the
query stream and the content stream work differently with a speciﬁc permutation through attention
masks. The main difference is that the query stream cannot do self-attention and does not have access
to the token at the position, while the content stream performs normal self-attention.

16

XLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290, 2019.

[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.

[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.

[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294–6305, 2017.

[20] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.

[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759, 2016.

[22] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993, 2019.

[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
ﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep., 2011.

[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.

[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[28] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network for multiple-choice

reading comprehension. arXiv preprint arXiv:1903.03033, 2019.

[29] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB
Endowment, 11(3):269–282, 2017.

[30] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach-
man, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint
arXiv:1611.09830, 2016.

[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848, 2019.

13

[36] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for document
ranking. In Proceedings of the 40th International ACM SIGIR conference on research and
development in information retrieval, pages 763–772. ACM, 2017.

[37] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval, pages 55–64. ACM, 2017.

[38] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax

bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

[39] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381,
2019.

[40] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.

[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision, pages 19–27, 2015.

14

A Target-Aware Representation via Two-Stream Self-Attention

A.1 A Concrete Example of How Standard LM Parameterization Fails

In this section, we provide a concrete example to show how the standard language model parameteri-
zation fails under the permutation objective, as discussed in Section 2.3. Speciﬁcally, let’s consider
two different permutations z(1) and z(2) satisfying the following relationship

Then, substituting the two permutations respectively into the naive parameterization, we have

z(1)
<t = z(2)

but

<t = z<t

t = i (cid:54)= j = z(2)
z(1)
(cid:124)
(cid:125)
= pθ(Xj = x | xz<t )

exp(cid:0)e(x)(cid:62)h(xz<t )(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)h(xz<t ))

(cid:80)

(cid:123)(cid:122)

=

.

t

.

t =j, z(2)
z(1)

<t =z<t

(cid:125)
(cid:124)
pθ(Xi = x | xz<t)

(cid:123)(cid:122)

t =i, z(1)
z(1)

<t =z<t

Effectively, two different target positions i and j share exactly the same model prediction. However,
the ground-truth distribution of two positions should certainly be different.

A.2 Two-Stream Attention

Here, we provide the implementation details of the two-stream attention with a Transformer-XL
backbone.

Initial represetation:

∀t = 1, . . . , T :

ht = e(xt)

and gt = w

Cached layer-m content represetation (memory) from previous segment: ˜h(m)
For the Transformer-XL layer m = 1,··· , M, attention with relative positional encoding and
position-wise feed-forward are consecutively employed to update the represetntations:

∀t = 1, . . . , T :

ˆ
h(m)
zt

h(m)
zt

g(m)
ˆ
zt

g(m)
zt

= LayerNorm

h(m−1)

+ RelAttn

h(m−1)

,

= LayerNorm

+ PosFF

= LayerNorm

g(m−1)

+ RelAttn

g(m−1)

,

(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)
(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)

z≤t

z≤t

zt

(cid:16)
(cid:16)ˆh(m)
(cid:17)(cid:17)
(cid:16)
(cid:16)
(cid:17)(cid:17)

zt

zt

g(m)
ˆ
zt

zt

(cid:16)
(cid:16)ˆh(m)
(cid:16)
(cid:16)

zt

zt

g(m)
ˆ
zt

Target-aware prediction distribution:

pθ(Xzt = x | xz<t ) =

= LayerNorm

+ PosFF

(cid:17)

(cid:16)

(cid:16)

(cid:80)

exp

e(x)(cid:62)g(M )

zt

x(cid:48) exp

e(x(cid:48))(cid:62)g(M )

zt

(cid:17) ,

A.3 Hyperparameters

A.3.1 Pretraining Hyperparameters

The hyperparameters used for pretraining XLNet are shown in Table 7.

A.3.2 Hyperparameters for Finetuning

The hyperparameters used for ﬁnetuning XLNet on various tasks are shown in Table 8. “Layer-wise
decay” means exponentially decaying the learning rates of individual layers in a top-down manner.
For example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is α, then
the learning rate of layer m is lα24−m.

15

Hparam
Number of layers
Hidden size
Number of attention heads
Attention head size
FFN inner hidden size
Dropout
Attention dropout
Partial prediction K
Max sequence length
Memory length
Batch size
Learning rate
Number of steps
Warmup steps
Learning rate decay
Adam epsilon
Weight decay

Value

24
1024
16
64
4096
0.1
0.1
6
512
384
2048
1e-5
500K
20,000
linear
1e-6
0.01

Table 7: Hyperparameters for pretraining.

Hparam
Dropout
Attention dropout
Max sequence length
Batch size
Learning rate
Number of steps
Learning rate decay
Weight decay
Adam epsilon
Layer-wise lr decay

RACE SQuAD MNLI Yelp-5

512
32
2e-5
12K

0.1
0.1

512
48
3e-5
8K

128
128
3e-5
10K

linear
0.00

512
128
2e-5
10K

1e-6
1.0

1e-6
1.0
Table 8: Hyperparameters for ﬁnetuning.

1e-6
1.0

1e-6
0.75

A.4 Visualizing Memory and Permutation

In this section, we provide a detailed visualization of the proposed permutation language modeling
objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use
attention masks to permute the factorization order, and the difference of the two attention streams.
As shown in Figure 3 and 4, given the current position zt, the attention mask is decided by the
permutation (or factorization order) z such that only tokens the occur before zt in the permutation can
be attended; i.e., positions zi with i < t. Moreover, comparing Figure 3 and 4, we can see how the
query stream and the content stream work differently with a speciﬁc permutation through attention
masks. The main difference is that the query stream cannot do self-attention and does not have access
to the token at the position, while the content stream performs normal self-attention.

16

Figure 3: A detailed illustration of the content stream of the proposed objective with both the joint
view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].
Note that if we ignore the query representation, the computation in this ﬁgure is simply the standard
self-attention, though with a particular attention mask.

17

Position-3ViewPosition-2 Viewwwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)h%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)Position-4Viewwwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)Position-1 ViewSplit View of the Content Stream(Factorization order: 3 à2 à4 à1)Joint View oftheContent Stream(Factorization order: 3 à2 à4 à1)wwwwmem(+)x#x%x’x(g#(%)g%(%)g’(%)g((%)mem(%)h#(%)h%(%)h’(%)h((%)g#(’)g%(’)g’(’)g((’)h#(’)h%(’)h’(’)h((’)Split ViewXLNet: Generalized Autoregressive Pretraining

for Language Understanding

Zhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell1,

Ruslan Salakhutdinov1, Quoc V. Le2

1Carnegie Mellon University, 2Google Brain

{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and
achieves state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.1.

1

Introduction

modeling factorizes the likelihood into a forward product p(x) =(cid:81)T
one p(x) =(cid:81)1

Unsupervised representation learning has been highly successful in the domain of natural language
processing [7, 19, 24, 25, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale
unlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [7, 24, 25]. Speciﬁcally, given a text sequence x = (x1,··· , xT ), AR language
t=1 p(xt | x<t) or a backward
t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [10], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

∗Equal contribution. Order determined by swapping the one in [9].
1Pretrained models and code are available at https://github.com/zihangdai/xlnet

Preprint. Under review.

arXiv:1906.08237v1  [cs.CL]  19 Jun 2019

bidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at
ﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.
• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations
of the factorization order. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.
• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.

In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.
• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.
• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.

Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding
tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classiﬁcation tasks
including Yelp and IMDB, and the ClueWeb09-B document ranking task. Under a set of fair
comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks.
Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there
are several key differences. Previous models are orderless, while XLNet is essentially order-aware
with positional encodings. This is important for language understanding because an orderless model
is degenerated to bag-of-words, lacking basic expressivity. The above difference results from the
fundamental difference in motivation—previous models aim to improve density estimation by baking
an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts.

2 Proposed Method

2.1 Background

In this section, we ﬁrst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence x = [x1,··· , xT ], AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:

,

(1)

exp(cid:0)hθ(x1:t−1)(cid:62)e(xt)(cid:1)
(cid:80)

x(cid:48) exp (hθ(x1:t−1)(cid:62)e(x(cid:48)))

(cid:88) t

=1

T

(cid:88) t

=1

T

max

θ

log pθ(x) =

log pθ(xt | x<t) =

log

where hθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-
ers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.
Speciﬁcally, for a text sequence x, BERT ﬁrst constructs a corrupted version ˆx by randomly setting
a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The

2

t e(xt)(cid:1)
exp(cid:0)Hθ(ˆx)(cid:62)
x(cid:48) exp(cid:0)Hθ(ˆx)(cid:62)
t e(x(cid:48))(cid:1) ,
(cid:80)

(2)

mt log

(cid:88) t

=1

T

training objective is to reconstruct ¯x from ˆx:

log pθ(¯x | ˆx) ≈ T(cid:88)

t=1

max

θ

mt log pθ(xt | ˆx) =

where mt = 1 indicates xt is masked, and Hθ is a Transformer that maps a length-T text sequence x
into a sequence of hidden vectors Hθ(x) = [Hθ(x)1, Hθ(x)2,··· , Hθ(x)T ]. The pros and cons of
the two pretraining objectives are compared in the following aspects:
• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint
conditional probability p(¯x | ˆx) based on an independence assumption that all masked tokens ¯x
are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes
pθ(x) using the product rule that holds universally without such an independence assumption.
• Input noise: The input to BERT contains artiﬁcial symbols like [MASK] that never occur in
downstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing [MASK] with original
tokens as in [10] does not solve the problem because original tokens can be only used with a small
probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.
• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up
to position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.

2.2 Objective: Permutation Language Modeling

Figure 1: Illustration of the permutation language modeling objective for predicting x3 given the
same input sequence x but with different factorization orders.

According to the comparison above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.

3

x"x#x$x%h"(#)h#(#)h$(#)h"($)h#($)h$($)Factorization order: 3 à2 à4 à1x"x#x$x%h#(#)h"($)h#($)h$($)h%($)Factorization order: 1 à4 à2 à3h"(#)h$(#)h%(#)h%(#)h%($)mem(+)mem(+)x"x#x$x%h"(#)h#(#)h"($)h#($)h%($)Factorization order: 2 à4 à3 à1h$(#)h%(#)h$($)x"x#x$x%h"(#)h#(#)h$(#)h%(#)h"($)h#($)h$($)h%($)Factorization order: 4 à3 à1 à2mem(+)mem(+)mem(#)mem(#)mem(#)mem(+)x%x%x%x%Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective
that not only retains the beneﬁts of AR models but also allows models to capture bidirectional
contexts. Speciﬁcally, for a sequence x of length T , there are T ! different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence
[1, 2, . . . , T ]. We use zt and z<t to denote the t-th element and the ﬁrst t−1 elements of a permutation
z ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:

(cid:34) T(cid:88)

t=1

(cid:35)

max

θ

Ez∼ZT

log pθ(xzt | xz<t)

.

(3)

Essentially, for a text sequence x, we sample a factorization order z at a time and decompose the
likelihood pθ(x) according to factorization order. Since the same model parameter θ is shared across
all factorization orders during training, in expectation, xt has seen every possible element xi (cid:54)= xt in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts
into the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune
discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during ﬁnetuning.
To provide an overall picture, we show an example of predicting the token x3 given the same input
sequence x but under different factorization orders in Figure 1.

2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations

Figure 2: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content xzt. (c): Overview of the
permutation language modeling training with two-stream attention.

While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution pθ(Xzt | xz<t ) using the standard Softmax formulation, i.e., pθ(Xzt =
x | xz<t) =
, where hθ(xz<t) denotes the hidden representation of xz<t
produced by the shared Transformer network after proper masking. Now notice that the representation
hθ(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful

exp(e(x)(cid:62)hθ(xz<t ))
x(cid:48) exp(e(x(cid:48))(cid:62)hθ(xz<t ))

(cid:80)

4

Sample a factorization order:3 à2 à4 à1Attention Maskse(x$)we(x’)we(x()we(x))wh$($)g$($)h’($)g’($)h(($)g(($)h)($)g)($)h$(’)g$(’)h’(’)g’(’)h((’)g((’)h)(’)g)(’)Content stream:can see selfQuery stream:cannot see selfx$x’x(x)Masked Two-stream AttentionMasked Two-stream Attention(c)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)h$($)g$($)AttentionQK, Vh$($)g$($)AttentionQK, V(b)(a)h$(,)g$(,)h’(,)g’(,)h((,)g((,)h)(,)g)(,)representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:

pθ(Xzt = x | xz<t) =

,

(4)

exp(cid:0)e(x)(cid:62)gθ(xz<t , zt)(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)gθ(xz<t , zt))

(cid:80)

where gθ(xz<t, zt) denotes a new type of representations which additionally take the target position
zt as input.
Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate gθ(xz<t, zt) remains a non-trivial problem. Among other
possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather
information from the context xz<t through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
xzt, gθ(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective
becomes trivial; (2) to predict the other tokens xzj with j > t, gθ(xz<t , zt) should also encode the
content xzt to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:
• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the
standard hidden states in Transformer. This representation encodes both the context and xzt itself.
• The query representation gθ(xz<t, zt), or abbreviated as gzt, which only has access to the contex-

tual information xz<t and the position zt, but not the content xzt, as discussed above.

Computationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)
i = w,
while the content stream is set to the corresponding word embedding, i.e. h(0)
i = e(xi). For each
self-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated
with a shared set of parameters as follows (illustrated in Figures 2 (a) and (b)):

g(m)
zt
h(m)
zt

← Attention(Q = g(m−1)
← Attention(Q = h(m−1)

zt

, KV = h(m−1)
, KV = h(m−1)

z<t

; θ),

; θ),

z≤t

zt

(query stream: use zt but cannot see xzt)
(content stream: use both zt and xzt).

where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the
content representations is exactly the same as the standard self-attention, so during ﬁnetuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation g(M )
Partial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last
tokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target
subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,

to compute Eq. (4).

zt

(cid:104)
(cid:105)
log pθ(xz>c | xz≤c )

= Ez∼ZT

max

θ

Ez∼ZT


 |z|(cid:88)


.
log pθ(xzt | xz<t )

t=c+1

(5)

Note that z>c is chosen as the target because it possesses the longest context in the sequence given the
current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected
for predictions; i.e., |z| /(|z| − c) ≈ K. For unselected tokens, their query representations need not
be computed, which saves speed and memory.

2.4

Incorporating Ideas from Transformer-XL

Since our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [9], into our pretraining framework, and name our method after it.

2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.

5

We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from previous segments. Without loss of generality, suppose we have two segments taken from
a long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1··· T ] and
[T + 1··· 2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then
cache the obtained content representations ˜h(m) for each layer m. Then, for the next segment x, the
attention update with memory can be written as

h(m)
zt

← Attention(Q = h(m−1)

zt

, KV =

(cid:104)˜h(m−1), h(m−1)

(cid:105)

z≤t

; θ)

where [., .] denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 2 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.4 for more detailed illustration).

2.5 Modeling Multiple Segments

Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Speciﬁcally, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS],
where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.7).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if
i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,
where s+ and s− are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are within the same segment, as opposed to considering which
speciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When i attends to j, the segment encoding sij is used
to compute an attention weight aij = (qi + b)(cid:62)sij, where qi is the query vector as in a standard
attention operation and b is a learnable head-speciﬁc bias vector. Finally, the value aij is added to
the normal attention weight. There are two beneﬁts of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of
ﬁnetuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.

2.6 Discussion and Analysis

2.6.1 Comparison with BERT

Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting
tokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize

6

log p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:

JBERT = log p(New | is a city) + log p(York | is a city),

JXLNet = log p(New | is a city) + log p(York | New, is a city).

Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and
contains “denser” effective training signals.
To prove a general point beyond one example, we now turn to more formal expressions. Inspired
by previous work [38], given a sequence x = [x1,··· , xT ], we deﬁne a set of target-context pairs
of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we
want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For
example, given the above sentence, the pairs of interest I could be instantiated as:
I =
.
Note that I is merely a virtual notion without unique ground truth, and our analysis will hold
regardless of how I is instantiated.
Given a set of target tokens T and a set of non-target tokens N = x\T , BERT and XLNet both
maximize log p(T | N ) but with different formulations:
log p(x | N ); JXLNet =

(cid:110)(cid:0)x = York,U = {New}(cid:1), (cid:0)x = York,U = {city}(cid:1), (cid:0)x = York,U = {New, city}(cid:1), ···(cid:111)

log p(x | N ∪ T<x)

JBERT =

(cid:88) x

∈T

(cid:88) x

∈T

where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist
of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair
(x,U) ∈ I such that U ⊆ Vx, then the loss term log p(x | Vx) provides a training signal to the
dependency between x and U. For convenience, we say a target-context pair (x,U) ∈ I is covered
by a model (objective) if U ⊆ Vx.
Given the deﬁnition, let’s consider two cases:
• If U ⊆ N , the dependency (x,U) is covered by both BERT and XLNet.
• If U ⊆ N ∪ T<x and U ∩ T<x (cid:54)= ∅, the dependency can only be covered by XLNet but not BERT.
As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet
objective contains more effective training signals, which empirically leads to better performance in
Section 3.

2.6.2 Comparison with Language Modeling

Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT [25]
is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}).
XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a
limitation of AR language modeling can be critical in real-world applications. For example, consider
a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead”
and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not
dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the
answer by the standard approach that employs softmax over all token representations. More formally,
consider a context-target pair (x,U):
• If U ∩ T<x (cid:54)= ∅, where T<x denotes the tokens prior to x in the original sequence, AR language
• In comparison, XLNet is able to cover all dependencies in expectation.
Approaches like ELMo [24] concatenate forward and backward language models in a shallow manner,
which is not sufﬁcient for modeling deep interactions between the two directions.

modeling is not able to cover the dependency.

7

RACE
GPT [25]
BERT [22]
BERT+OCN∗ [28]
BERT+DCMN∗ [39]
XLNet

Accuracy Middle High
57.4
70.1
71.5
71.8
80.21

62.9
76.6
78.4
79.5
85.45

59.0
72.0
73.5
74.1
81.75

Table 1: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task. ∗
indicates using ensembles. “Middle” and “High” in RACE are two subsets representing middle and high school
difﬁculty levels. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes
(aka BERT-Large). Our single model outperforms the best ensemble by 7.6 points in accuracy.

2.6.3 Bridging the Gap Between Language Modeling and Pretraining

With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing
research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining
due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2. It
has even been challenged by some machine learning practitioners whether language modeling is a
meaningful pursuit if it does not directly improve downstream tasks 4. XLNet generalizes language
modeling and bridges such a gap. As a result, it further “justiﬁes” language modeling research.
Moreover, it becomes possible to leverage the rapid progress of language modeling research for
pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness
of the latest language modeling progress.

3 Experiments

3.1 Pretraining and Implementation

Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [23],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 78GB text respectively. After tokenization with SentencePiece [16], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. The sequence length and memory length are set to 512 and 384
respectively. We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer,
linear learning rate decay and a batch size of 2048, which takes about 2.5 days. It was observed that
the model still underﬁts the data at the end of training but continuing training did not help downstream
tasks, which indicates that given the optimization algorithm, the model does not have enough capacity
to fully leverage the data scale. However, in this work, we refrain from training a larger model as
its practical usage for ﬁnetuning might be limited. Further, we train an XLNet-Base, analogous to
BERT-Base, on BooksCorpus and Wikipedia only, for ablation study and fair comparison with BERT.
Related results are presented in Section 3.7.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant K as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]
except otherwise speciﬁed5. We employ an idea of span-based prediction, where we ﬁrst sample a
length L ∈ [1,··· , 5], and then randomly select a consecutive span of L tokens as prediction targets
within a context of (KL) tokens.

3The problem of language modeling is essentially density estimation for text data.
4https://openreview.net/forum?id=HJePno0cYm
5Hyperparameters for pretraining and ﬁnetuning are in Appendix A.3.

8

F1

EM

EM

84.1
88.95

SQuAD2.0

BERT† [10]

SQuAD1.1
Dev set results without data augmentation
78.98
BERT [10]
86.12
XLNet
Test set results on leaderboard, with data augmentation (as of June 19, 2019)
Human [27]
85.15
85.23
ATB
BERT∗ [10]
85.88
86.35
XLNet

91.22 BERT+N-Gram+Self-Training [10]
92.64
93.16 BERT+DAE+AoA
95.08 XLNet

90.9
94.52 XLNet

82.30
86.94
87.43
89.90

SG-Net

F1

81.77
88.79

87.72
87.93
88.62
89.13

Table 2: A single model XLNet outperforms human and the best ensemble by 7.6 EM and 2.5 EM on SQuAD1.1.
∗ means ensembles, † marks our runs with the ofﬁcial code.

Model
CNN [14]
DPCNN [14]
Mixed VAT [30, 20]
ULMFiT [13]
BERT [35]
XLNet

IMDB Yelp-2 Yelp-5 DBpedia

-
-

4.32
4.6
4.51
3.79

2.90
2.64

-

2.16
1.89
1.55

32.39
30.58

-

29.98
29.32
27.80

0.84
0.88
0.70
0.80
0.64
0.62

AG Amazon-2 Amazon-5
6.57
6.87
4.95
5.01

36.24
34.81

3.79
3.32

-
-

-
-

-

4.49

2.63
2.40

34.17
32.26

Table 3: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).

3.2 RACE Dataset

The RACE dataset [17] contains near 100K questions taken from the English exams for middle and
high school Chinese students in the age range between 12 to 18, with the answers generated by human
experts. This is one of the most difﬁcult reading comprehension datasets that involve challenging
reasoning questions. Moreover, the average length of the passages in RACE are longer than 300,
which is signiﬁcantly longer than other popular reading comprehension datasets such as SQuAD [26].
As a result, this dataset serves as a challenging benchmark for long text understanding. We use a
sequence length of 640 during ﬁnetuning. As shown in Table 1, a single model XLNet outperforms
the best ensemble by 7.6 points in accuracy. It is also clear that XLNet substantially outperforms
other pretrained models such as BERT and GPT. Since RACE contains relatively long passages, we
believe one of the reasons why XLNet obtains substantial gains on this dataset is that the integration
of the Transformer-XL architecture improves the capability of modeling long text, besides the AR
objective. More analysis on the sequence length is presented in Section 3.7.

3.3 SQuAD Dataset

SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [27] contains
questions that always have a corresponding answer in the given passages, while SQuAD2.0 [26]
introduces unanswerable questions. To ﬁnetune an XLNet on SQuAD2.0, we jointly apply a logistic
regression loss for answerability prediction similar to classiﬁcation tasks and a standard span extrac-
tion loss for question answering [10]. Since v1.1 and v2.0 share the same answerable questions in the
training set, we simply remove the answerability prediction part from the model ﬁnetuned on v2.0 for
evaluation on v1.1. As the top leaderboard entries all employ some form of data augmentation, we
jointly train an XLNet on SQuAD2.0 and NewsQA [31] for our leaderboard submission. As shown
in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming
a series of BERT-based methods. Notably, on v1.1, an XLNet single model outperforms human and
the best ensemble by 7.6 and 2.5 points in EM. Finally, for direct comparison with BERT to eliminate
the effects of additional tricks in leaderboard submissions, we compare XLNet against BERT on the
dev set. XLNet substantially outperforms BERT by 3.6 and 7.0 points in F1 for v1.1 and v2.0.

9

MNLI

88.0
89.2

70.4
83.8

92.3
93.9

91.3
91.8

93.2
95.6

86.7/85.9

86.6/-
89.8/-

QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI

Model
Single-task single models on dev
BERT [2]
XLNet
Single-task single models on test
BERT [10]
Multi-task ensembles on test (from leaderboard as of June 19, 2019)
Snorkel∗ [29]
ALICE∗
MT-DNN∗ [18]
XLNet∗
Table 4: Results on GLUE. ∗ indicates using ensembles, and † denotes single-task results in a multi-task row.
All results are based on a 24-layer architecture with similar model sizes (aka BERT-Large). See the upper-most
rows for direct comparison with BERT and the lower-most rows for comparison with state-of-the-art results on
the public leaderboard.

87.6/87.2
88.2/87.9
87.9/87.4
90.2/89.7†

89.9
90.7
89.9
90.3†

96.2
95.2
96.5
96.8†

93.9
95.7
96.0
98.6†

63.8
68.6
68.4
67.8

90.1
91.1
91.1
91.6

65.1
80.8
89.0
90.4

91.5
92.6
92.7
93.0

80.9
83.5
86.3
86.3

60.6
63.6

90.0
91.8

89.3

70.1

94.9

89.3

60.5

87.6

65.1

91.1

-
-

Model
DRMM [12]
KNRM [8]
Conv [8]
BERT†
XLNet

NDCG@20 ERR@20

24.3
26.9
28.7
30.53
31.10

13.8
14.9
18.1
18.67
20.28

Table 5: Comparison with state-of-the-art results on the test set of ClueWeb09-B, a document ranking task. †
indicates our implementations.

3.4 Text Classiﬁcation

Following previous work on text classiﬁcation [40, 20], we evaluate XLNet on the following bench-
marks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. According to Table 3,
XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate
by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively
compared to BERT.

3.5 GLUE Dataset

The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels
are removed from the publicly released version, and all the practitioners must submit their predictions
on the evaluation server to obtain test set results. In Table 4, we present results of multiple settings,
including single-task and multi-task, as well as single models and ensembles. In the multi-task
setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and
ﬁnetune the network on the other datasets. Only single-task training is employed for the four large
datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [18] for our test set
submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a
standard classiﬁcation paradigm. For WNLI, we use the loss described in [15]. A multi-task ensemble
XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard. On the most
widely-benchmarked task MNLI, XLNet improves the “matched” and “mismatched” settings by 2.0
and 1.8 points respectively. Note that the leaderboard competitors employ improved techniques over
BERT such as distillation, modiﬁed multi-task losses, or meta learning, but still underperform XLNet
which does not employ additional tricks besides using a standard multi-task learning method. Since
the leaderboard is not intended for ablation study or hyperparameter tuning, we only evaluated our
best multi-task models on the test set. To obtain a direct comparison with BERT, we run a single-task
XLNet on the dev set. As shown in the upper-most rows of Table 4, XLNet consistently outperforms
BERT, with an improvement of 13.4 points, 3.2 points, 3.0 points, 2.4 points, 1.8 points on RTE,
MNLI, CoLA, SST-2, and STS-B respectively.

10

# Model

1 BERT-Base
2 DAE + Transformer-XL
3 XLNet-Base (K = 7)
4 XLNet-Base (K = 6)
5
6
7
8

- memory
- span-based pred
- bidirectional data
+ next-sent pred

RACE

64.3
65.03
66.05
66.66
65.55
65.95
66.34
66.76

SQuAD2.0
EM
F1
73.66
76.30
76.80
79.56
81.33
78.46
78.18
80.98
77.27
80.15
77.91
80.61
77.87
80.65
79.83
76.94

MNLI
m/mm

84.34/84.65
84.88/84.45
85.84/85.43
85.63/85.12
85.32/85.05
85.49/85.02
85.31/84.99
85.32/85.09

SST-2

92.78
92.60
92.66
93.35
92.78
93.12
92.66
92.89

Table 6: Ablation study. The results of BERT on RACE are taken from [39]. We run BERT on the other datasets
using the ofﬁcial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter
to control the optimization difﬁculty (see Section 2.3). All models are pretrained on the same data.

3.6 ClueWeb09-B Dataset

Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-
mance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on
50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval
method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations
instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word
embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries
without ﬁnetuning, and employ a kernel pooling network [37] to rank the documents. According to
Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the
same training procedure as ours. This illustrates that XLNet learns better low-level word embeddings
than BERT. Note that for fair comparison we exclude the results (19.55 in ERR@20, slightly worse
than ours) in [36] as it uses additional entity-related data.

3.7 Ablation Study

We perform an ablation study to understand the importance of each design choice based on four
datasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:
• The effectiveness of the permutation language modeling objective, especially compared to the
• The importance of using Transformer-XL as the backbone neural architecture and employing
• The necessity of some implementation details including span-based prediction, the bidirectional

denoising auto-encoding objective used by BERT.

segment-level recurrence (i.e. using memory).

input pipeline, and next-sentence prediction.

With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
Examining rows 1 - 4 of Table 6, we see the two full XLNet-Base models trained with different values
of K signiﬁcantly outperform both BERT and the DAE trained Transformer-XL across tasks, showing
the superiority of the permutation language modeling objective. Meanwhile, it is also interesting
to see that the DAE trained Transformer-XL achieves better performance than BERT on tasks with
long text such as RACE and SQuAD, suggesting the excellence of Transformer-XL in language
modeling also beneﬁts pretraining. Next, if we remove the memory caching mechanism (row 5), the
performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.
In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play
important roles in XLNet. Finally, we unexpectedly ﬁnd the the next-sentence prediction objective
proposed in the original BERT does not necessarily lead to an improvement in our setting. Instead, it
tends to harm the performance except for the RACE dataset. Hence, when we train XLNet-Large, we
exclude the next-sentence prediction objective.

11

4 Conclusions

XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and careful design
of the two-stream attention mechanism. XLNet achieves state-of-the-art results various tasks with
substantial improvement. In the future, we envision applications of XLNet to a wider set of tasks
such as vision and reinforcement learning.

Acknowledgments

The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Youlong Cheng and Yanping Huang for providing ideas to improve our TPU implementation,
Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and
RS were supported by the Ofﬁce of Naval Research grant N000141812861, the National Science
Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY
were supported in part by NSF under the grant IIS-1546329 and by the DOE-Ofﬁce of Science under
the grant ASCR #KJ040201.

References

[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level

language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.

[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-

mous preprint under review, 2018.

[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.

arXiv preprint arXiv:1809.10853, 2018.

[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.

[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural

information processing systems, pages 3079–3087, 2015.

[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international
conference on web search and data mining, pages 126–134. ACM, 2018.

[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860, 2019.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning, pages 881–889,
2015.

[12] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM, 2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146, 2018.

[14] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 562–570, 2017.

12

[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint
arXiv:1905.06290, 2019.

[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale

reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.

[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks

for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.

[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294–6305, 2017.

[20] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-

supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.

[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759, 2016.

[22] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with

external knowledge. arXiv preprint arXiv:1902.00993, 2019.

[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
ﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep., 2011.

[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint
arXiv:1802.05365, 2018.

[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.

[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable

questions for squad. arXiv preprint arXiv:1806.03822, 2018.

[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions

for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[28] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network for multiple-choice

reading comprehension. arXiv preprint arXiv:1903.03033, 2019.

[29] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher
Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB
Endowment, 11(3):269–282, 2017.

[30] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks

for semi-supervised text classiﬁcation via mixed objective function. 2018.

[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach-
man, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint
arXiv:1611.09830, 2016.

[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.

[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data

augmentation. arXiv preprint arXiv:1904.12848, 2019.

13

[36] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for document
ranking. In Proceedings of the 40th International ACM SIGIR conference on research and
development in information retrieval, pages 763–772. ACM, 2017.

[37] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval, pages 55–64. ACM, 2017.

[38] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax

bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

[39] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381,
2019.

[40] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.

[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision, pages 19–27, 2015.

14

A Target-Aware Representation via Two-Stream Self-Attention

A.1 A Concrete Example of How Standard LM Parameterization Fails

In this section, we provide a concrete example to show how the standard language model parameteri-
zation fails under the permutation objective, as discussed in Section 2.3. Speciﬁcally, let’s consider
two different permutations z(1) and z(2) satisfying the following relationship

Then, substituting the two permutations respectively into the naive parameterization, we have

z(1)
<t = z(2)

but

<t = z<t

t = i (cid:54)= j = z(2)
z(1)
(cid:124)
(cid:125)
= pθ(Xj = x | xz<t )

exp(cid:0)e(x)(cid:62)h(xz<t )(cid:1)

x(cid:48) exp (e(x(cid:48))(cid:62)h(xz<t ))

(cid:80)

(cid:123)(cid:122)

=

.

t

.

t =j, z(2)
z(1)

<t =z<t

(cid:125)
(cid:124)
pθ(Xi = x | xz<t)

(cid:123)(cid:122)

t =i, z(1)
z(1)

<t =z<t

Effectively, two different target positions i and j share exactly the same model prediction. However,
the ground-truth distribution of two positions should certainly be different.

A.2 Two-Stream Attention

Here, we provide the implementation details of the two-stream attention with a Transformer-XL
backbone.

Initial represetation:

∀t = 1, . . . , T :

ht = e(xt)

and gt = w

Cached layer-m content represetation (memory) from previous segment: ˜h(m)
For the Transformer-XL layer m = 1,··· , M, attention with relative positional encoding and
position-wise feed-forward are consecutively employed to update the represetntations:

∀t = 1, . . . , T :

ˆ
h(m)
zt

h(m)
zt

g(m)
ˆ
zt

g(m)
zt

= LayerNorm

h(m−1)

+ RelAttn

h(m−1)

,

= LayerNorm

+ PosFF

= LayerNorm

g(m−1)

+ RelAttn

g(m−1)

,

(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)
(cid:104)˜h(m−1), h(m−1)
(cid:105)(cid:17)(cid:17)

z≤t

z≤t

zt

(cid:16)
(cid:16)ˆh(m)
(cid:17)(cid:17)
(cid:16)
(cid:16)
(cid:17)(cid:17)

zt

zt

g(m)
ˆ
zt

zt

(cid:16)
(cid:16)ˆh(m)
(cid:16)
(cid:16)

zt

zt

g(m)
ˆ
zt

Target-aware prediction distribution:

pθ(Xzt = x | xz<t ) =

= LayerNorm

+ PosFF

(cid:17)

(cid:16)

(cid:16)

(cid:80)

exp

e(x)(cid:62)g(M )

zt

x(cid:48) exp

e(x(cid:48))(cid:62)g(M )

zt

(cid:17) ,

A.3 Hyperparameters

A.3.1 Pretraining Hyperparameters

The hyperparameters used for pretraining XLNet are shown in Table 7.

A.3.2 Hyperparameters for Finetuning

The hyperparameters used for ﬁnetuning XLNet on various tasks are shown in Table 8. “Layer-wise
decay” means exponentially decaying the learning rates of individual layers in a top-down manner.
For example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is α, then
the learning rate of layer m is lα24−m.

15

Hparam
Number of layers
Hidden size
Number of attention heads
Attention head size
FFN inner hidden size
Dropout
Attention dropout
Partial prediction K
Max sequence length
Memory length
Batch size
Learning rate
Number of steps
Warmup steps
Learning rate decay
Adam epsilon
Weight decay

Value

24
1024
16
64
4096
0.1
0.1
6
512
384
2048
1e-5
500K
20,000
linear
1e-6
0.01

Table 7: Hyperparameters for pretraining.

Hparam
Dropout
Attention dropout
Max sequence length
Batch size
Learning rate
Number of steps
Learning rate decay
Weight decay
Adam epsilon
Layer-wise lr decay

RACE SQuAD MNLI Yelp-5

512
32
2e-5
12K

0.1
0.1

512
48
3e-5
8K

128
128
3e-5
10K

linear
0.00

512
128
2e-5
10K

1e-6
1.0

1e-6
1.0
Table 8: Hyperparameters for ﬁnetuning.

1e-6
1.0

1e-6
0.75

A.4 Visualizing Memory and Permutation

In this section, we provide a detailed visualization of the proposed permutation language modeling
objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use
attention masks to permute the factorization order, and the difference of the two attention streams.
As shown in Figure 3 and 4, given the current position zt, the attention mask is decided by the
permutation (or factorization order) z such that only tokens the occur before zt in the permutation can
be attended; i.e., positions zi with i < t. Moreover, comparing Figure 3 and 4, we can see how the
query stream and the content stream work differently with a speciﬁc permutation through attention
masks. The main difference is that the query stream cannot do self-attention and does not have access
to the token at the position, while the content stream performs normal self-attention.

16

Figure 3: A detailed illustration of the content stream of the proposed objective with both the joint
view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].
Note that if we ignore the query representation, the computation in this ﬁgure is simply the standard
self-attention, though with a particular attention mask.

17

Position-3ViewPosition-2 Viewwwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)h%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)Position-4Viewwwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)Position-1 ViewSplit View of the Content Stream(Factorization order: 3 à2 à4 à1)Joint View oftheContent Stream(Factorization order: 3 à2 à4 à1)wwwwmem(+)x#x%x’x(g#(%)g%(%)g’(%)g((%)mem(%)h#(%)h%(%)h’(%)h((%)g#(’)g%(’)g’(’)g((’)h#(’)h%(’)h’(’)h((’)Split ViewFigure 4: A detailed illustration of the query stream of the proposed objective with both the joint
view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].
The dash arrows indicate that the query stream cannot access the token (content) at the same position,
but only the location information.

18

wwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)h%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)wwwwg#(%)g%(%)g’(%)g#(’)g%(’)g’(’)g((%)g((’)mem(+)mem(%)x#x%x’x(h#(%)h%(%)h’(%)h((%)h#(’)h%(’)h’(’)h((’)Position-3ViewPosition-2 ViewPosition-4ViewPosition-1 ViewSplit View of the Query Stream(Factorization order: 3 à2 à4 à1)Split Viewwwwwmem(+)x#x%x’x(g#(%)g%(%)g’(%)g((%)mem(%)h#(%)h%(%)h’(%)h((%)g#(’)g%(’)g’(’)g((’)h#(’)h%(’)h’(’)h((’)Joint View oftheQuery Stream(Factorization order: 3 à2 à4 à1)