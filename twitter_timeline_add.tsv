user_id	user_name	follower	following	tweet	date	arxiv_url	arxiv_title	arxiv_abstract	arxiv_p_subject	arxiv_s_subject
imenurok	那須音トウ:????	1,970	670	@maguroIsland 確かこちら https://t.co/NMaZpURHeL	2019/6/25 23:13					
tSato_as	T. Sato	124	150	これ、near-MCh が sub-MCh と半々とかそれ以上に見えるのは、もっとWD density あげたりとか、transition density 変えると印象変わってくるのかな？ https://t.co/ArtnwwbvOL	2019/6/26 6:24					
dakuton	Yuji Tokuda	325	210	GPUが数の暴力でとても真似できないけど、ハッシュタグを足がかりに未ラベルの動画データで行動認識するというのはなかなか興味深い。 Large-scale weakly-supervised pre-training for video action recognition https://t.co/XoSPpv6m4V	2019/6/26 9:50					
tripdancer0916	Kohei Ichikawa	2,664	2,122	またすごいものが出てきた。148ページの大作。  A free energy principle for a particular physics - Karl Friston https://t.co/0GlJZvgn2n	2019/6/26 10:49					
Kenji_Sugisaki	杉﨑 研司 (Kenji Sugisaki)	360	127	Machine learning methods in quantum computing theory https://t.co/D0JrcYkzqL あとで読む	2019/6/26 12:14					
yutakashino	Yuta Kashino	9,885	186	XLNet: Generalized Autoregressive Pretraining for Language Understanding https://t.co/smBZWVZx1R ほんとですね，512 TPU 2.5日だと，Cloud TPU v3は一つあたり$8/hだから，2654万円かかる…．もうNLPの研究も圧倒的な計算力で殴る時代になってますね… https://t.co/K55jMrXKlI	2019/6/26 12:20					
tripdancer0916	Kohei Ichikawa	2,664	2,122	ネットワークが生み出す情報量とそれを人が受け取る時の効率性を情報理論の枠組みで定義した上で実際の情報伝達システム（文章や音楽など）のネットワークが高い情報量と効率性を達成していることを示した。 また階層性とモジュール性からそれが実現することも明らかにした。 https://t.co/0j2T50naRS https://t.co/zOj1j9uxL3	2019/6/26 12:20					
Kenji_Sugisaki	杉﨑 研司 (Kenji Sugisaki)	360	127	Beyond the swap test: optimal estimation of quantum state overlap https://t.co/5HYS7cdVCF まだ中身読んでないけど気になる論文。あとで読む。	2019/6/26 12:21					
autumn_good_35	Autumn Good	1,755	271	奇虎360による論文。  『While theprimitive (fully homomorphic encryption) itself is secure, protocols and applications can easily be completelyinsecure.』  Danger of using fully homomorphic encryption: A look at Microsoft SEAL [PDF] https://t.co/2l6ykSt5BZ https://t.co/71rIgA0ygw	2019/6/26 12:31					
U1KURI	くりりん（Yuichi KRHR）	70	148	甘利先生  Jacot et al. Neural Tangent Kernel  関数空間での解析 https://t.co/2LcMX5aeLz  カーネル学習、ガウシアンプロセスに出来る。 ※ 距離空間のパラメータ数が無限である、という条件  どんな局所解も、ほぼ最適解になる！  →　実は学習は線形問題！！！   #wba_sympo4	2019/6/26 14:02					
shion_honda	Shion Honda	1,234	242	Deep learning for molecular design [Elton+, 2019, Mol. Syst. Des. Eng.] 過去2年間のDLによる分子設計手法に関する論文45本をまとめたサーベイ。RNN、GANなど機械学習一般の話が手厚いが、最後に強化学習の報酬設計などドメイン固有の議論もある。 https://t.co/RruXfCPFPF #NowReading https://t.co/QY3Cpv0Ug7	2019/6/26 15:29					
TomiyaAkio	3刷出ます「ディープラーニングと物理学」 A.Tomiya	1,262	504	個人的には、https://t.co/Ltq63t7B3K の続きの話がおもしろかったな	2019/6/26 17:32					
overleo	overleo	189	0	BERTを超えたXLNetの紹介 - akihiro_f - Medium: 概要https://t.co/khdHR0GZWP XLNetは2019/6/19に、”XLNet: Generalized Autoregressive Pretraining for Language Understanding”と題してArxivに投稿された論文です。一言(?)でいうと… https://t.co/p03Aej2nmw [ml]	2019/6/26 17:35					
m3yrin	typo	127	276	宝くじ券仮説、NNの中で初期値と相性?の良い有効な構造のみが生き残っていく話(らしい)  |The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks | https://t.co/FQOgC7cDml	2019/6/26 17:37					
qard_t	T-QARD channel	516	0	https://t.co/3IVdpQF3wQ  展望としては2Dもいけるぞ！とのこと。 https://t.co/Xs7ihnvjAy	2019/6/26 18:47					
SeURa_Nue	せゆーら / Se-U-Ra	3,667	420	@takumiwish https://t.co/IkpmVNd6Uu  この辺じゃないですかね(推察)	2019/6/26 20:06					
yu__ya4	Yuya Matsumura	1,642	1,037	論文 https://t.co/pR58iCGQvA  サービスの性質上，ユーザ間の取引が A-&gt;B-&gt;C-&gt;A ってふうに循環してたら怪しいって仮説おいてそれを特徴量にしてるの面白い（最終，そこまで効いてなさげやったけど）。 こういうドメイン知識あるからこそのってやつ好き。	2019/6/26 20:28					
Nextremer_nb_o	nb.o	51	113	もとの論文とおなじことがかいてあるってことか。  https://t.co/2ufSUdfWLS	2019/6/26 21:53					
hir_kurashige	Hiroki Kurashige	225	80	Fristonの例の論文(というかモノグラフ)、最終的には自分の研究に関わるだろうし、読んで(解読して)みようと思う。その上でせっかくだからこじんまりとした報告会でも開くか。これは輪読には向かないから、私が解読して、私が説明して、他の人がそれにツッコミ入れる会。  https://t.co/CMQnxJUf2o	2019/6/26 22:05					
re_hako_moon	はこつき＠VR	43	49	https://t.co/Vpv23Ke83J 単眼カメラ画像から物体の三次元メッシュを再構成する。面と線によるSkeltonを一度構成し、3DCNNによって高解像度なVolume表現を得る。さらにMarching Cubesでメッシュに変換し、GCNNでRefinementしてきれいなメッシュを出力。	2019/6/26 22:16					
nicklaw296	ニクロー	316	321	@puni_kyopro https://t.co/JDUF973x7O とりあえずいろんな分野で使えそうな感じの	2019/6/26 22:44					
kogomi21361	kogomi	311	334	ベシ圏の原著が公開されてると聞いて..... ふふふ…英語を勉強できる素材が増えたな…（建前） https://t.co/ZXXMFKXZTx	2019/6/26 22:52					
Yukihiro0036	YukihiroMasuoka	125	211	これのことですかね https://t.co/fzkDrDp7ql	2019/6/26 22:55					
hhhhhhaaaaaa2	h.a.	339	524	Spin-split band hybridization in graphene proximitized with α-RuCl3 nanosheets https://t.co/VEQxfqrDFS グラフェンとαCuClのシート重ねたらスピン散乱によって 量子振動が減衰するらしい こんな研究があるんやね	2019/6/26 23:04					
triwave33	triwave	807	726	強化学習タスクで、人間の生体信号を内発的な報酬（系から付与されない報酬）として扱うもの。  強化学習に限らず、自動運転のプランニング問題に対して安全を定義・評価するのは極めて難しい問題。生体情報を取り込めたら面白いがそんなにうまくいくの？しっかり読む  https://t.co/wZR0OMV9dr	2019/6/26 23:32					
kadamasaru	嘉田 勝	2,896	318	ほぉ。 https://t.co/m7SQeRIWC6	2019/6/26 23:41					
QU00h	は	540	466	この論文、ためしに実験で確かめてみようかな  https://t.co/vUEEohidgg	2019/6/27 0:37					
alterakey	Takahiro Yoshimura	817	907	おお、華麗だ… #メモ  From IP ID to Device ID and KASLR Bypass (Extended Version) https://t.co/KUdi5nxkPV https://t.co/KUdi5nxkPV	2019/6/27 0:37					
petrucci1993	TKC(物理)	62	110	https://t.co/Ez4jgbxdtz 修正版あげました	2019/6/27 0:51					
q9ac	將籠林檎 / ??	278	228	超伝導体を二つ持ってきてカシミール効果の温度依存性を調べる。二つが十分に近い領域(vdW regeme)だと超伝導相転移はカシミール効果に不連続性をもたらさないが、二つが離れていると(Casimir regeme)超伝導転移点でカシミールフォースが不連続になるらしい。 https://t.co/9n2fupxScD	2019/6/27 1:06					
GenerateTaiyaki	taiyaki	14	0	DeepVoxels (CVPR2019 oral https://t.co/2bBGcktVrs) 物体を様々な方向から撮影した2D画像から3Dの埋め込み表現を学習する手法.各方向からの画像特徴の三次元的・逐次的な統合+埋め込みを2Dに射影した特徴からの訓練データの再構成+敵対的ロス.物体を未知の方向から見た画像を高解像度で生成可能. https://t.co/VEGCybiazv	2019/6/27 1:48					
Q7Xf5U4TpGCMDjG	ほ(研究)	0	0	https://t.co/EubJn4Hyo0 … https://t.co/FFPh73L2j2 CVSports(CVPR2019)ゴルフのスウィングのシークエンスを８種類に分類し、それぞれの検出を行う研究。同時にゴルフのスウィングに関するデータセットGolfDBの提案。 https://t.co/G0TpYt2QZf	2019/6/27 2:12					
yasuokajihei	宮島正	380	132	(Google DeepMind) https://t.co/u5e1OsKHnQ 他者の心を類推し、理解する能力についての「心の理論」に基づいた振る舞いが出来るようなモデルを深層強化学習で実現．	2019/6/27 3:41					
Kashalpha	かしゃるふぁ	697	823	VanQver (Variational and Adiabatically Navigated Quantum Eigensolver)は名前がおしゃれなのでお気に入り https://t.co/yKPKWn4xj4	2019/6/27 6:16					
infoseeker18	verbatim	887	1,952	期限付助手時代、（古典的な）情報理論の演習を持たされたことがあって、基本的なことを一通り勉強したんだけど、全然面白いと思わなかった。しかし、Wittenの量子情報理論のショートイントロダクションを見てかなり印象が変わった。案外面白い。https://t.co/abDL0EtlwM	2019/6/27 8:13					
yu4u	Yusuke Uchida	4,447	950	AutoAugmentの物体検出版だよー＾＾ / “[1906.11172] Learning Data Augmentation Strategies for Object Detection” https://t.co/s5CA9SZ8bh	2019/6/27 9:57					
ueda_physics	Shu	1,116	1,148	PBH 由来の 511 keV ガンマ線の観測から PBH がダークマターに占める割合に 1 % 以下の制限がつく https://t.co/PxqWBWR6BD	2019/6/27 10:48					
karino2012	karino2@貴族階級	651	81	少し古いがPixelRNNの論文はなかなか示唆に富むな。 https://t.co/FIyaAWE11i	2019/6/27 11:08					
esXFdfOJxiGBFLx	人工知能 Deep Learning AI image medical machine learni	858	1,901	女子ワールドカップと機械学習に関するarxivです。 過去のデータやFIFAランキングデータから優勝予測を行うものです。 今現在、女子ワールドカップが行われているため読んでみました。 日本が低いのが気になりますね。 https://t.co/8nCfZqPPyK	2019/6/27 11:08					
hillbig	Daisuke Okanohara	15,910	254	一部しか観測できない環境下（POMDPs）では過去の観測列から現在の状態信念を計算する。この信念からの長期の予測タスクを解くことで時間的に一貫性がある信念が得られ、地図や自己位置も復元可能。この信念を元にした強化学習はデータ効率がよい https://t.co/zbkyaMlSbB https://t.co/HqyL8zkIBQ	2019/6/27 11:30					
tonets	大上雅史｜M Ohue	4,605	1,769	Dror、ディープもやってるのか（って前も言った気も） [1807.01297] Transferrable End-to-End Learning for Protein Interface Prediction https://t.co/0DfbvZJLgx	2019/6/27 12:29					
taketo1024	さのたけと	6,347	243	@kyow_Q また東京に来られる機会にでも是非??   量子群の表現とも関連があるようで分かりたいと思ってます??こちらの Section6 に概説があります?? https://t.co/YhNH9S4dE1	2019/6/27 12:36					
CharStream	カオナシ(T.MATSUMOTO)	349	412	@hyuki 原書の.pdf版ならこちら（https://t.co/NtqT2pmKIH）から無償でダウンロードできますから、英語版は簡単に入手できますよ。 #ベーシック圏論	2019/6/27 13:14					
q9ac	將籠林檎 / ??	278	228	Weyl型分散を持つ物質をメカニカルに変形したり非慣性系に乗せたり。曲がった時空の上のワイル粒子のダイナミクスみたいなのを考えるフレームワークらしい。 https://t.co/IUBxUfVk8E	2019/6/27 14:31					
q9ac	將籠林檎 / ??	278	228	量子カルノーエンジン。調和ポテンシャル下にある量子系が熱浴とひっついてる設定を考えて、マスター方程式を書き下す→古典的なカルノーサイクルに対応するプロセスを考えて、その中でエネルギーやコヒーレンスがどう消費されていくかを調べてる。 https://t.co/VjK19MdQKf	2019/6/27 14:46					
SMBKRHYT	?Hayato Shimabukuro?	923	628	moriwakiさんたちの21cm-OIII cross correlation論文出たんだ。  https://t.co/VGq06l1wfN	2019/6/27 15:47					
bbr_bbq	Isao Takaesu	1,111	80	FuzzingをMLに応用する研究動向の纏め。 A Review of Machine Learning Applications in Fuzzing https://t.co/hwLPeONvnV	2019/6/27 16:01					
bbr_bbq	Isao Takaesu	1,111	80	DNNの学習時に使用される勾配から訓練データを復元する手法「Deep Leakage」。訓練データに機微情報が含まれていた場合、情報漏洩となる。 Deep Leakage from Gradients https://t.co/q8zOw4z4OM	2019/6/27 16:11					
mjmiyama	観山正道	745	601	昨日の配信でも話題に出た山城くんの論文  https://t.co/00grdqaMhR	2019/6/27 16:29					
CQC_0702	緋井悠里	11	13	雷雲の下での地上のガンマ線バーストの観測 https://t.co/azjCIlOXph ちょっと待って。これ完全にシルヴァリオの総統閣下なんだけど。	2019/6/27 17:36					
NH_M_	Nm?m	1,131	427	Chigusa, Moroi, Shoji; Bounce Configuration from Gradient Flow https://t.co/7vWyIUpExL グラディエントフロー方程式を用いることで、真空崩壊確率の新しい計算手法を発見した。従来の方法ではパラメータスキャンをしなければならずかなり大変な計算だったが、この新手法ではより単純に計算できる	2019/6/27 17:52					
akihiro_akichan	akihiro_f	122	118	https://t.co/63t23UbL2r  ピザのトッピングを追加・除去できるGAN。両モデルともに対象物の画像とマスクを生成して元画像にかぶせる構造になっている。トッピングをのclassification lossとcycle consictency lossを使用。著者のうちMITはともかく、カタールの大学もピザへの熱い情熱をもっている模様	2019/6/27 19:12					
lukasberns	Lukas Berns	367	402	@QFTlover Complex Probability Theory → Quantum Theory https://t.co/ThbBB8JpUr  Quaternionic quantum theory に関しては https://t.co/ghLBeJ8ix8	2019/6/27 20:57					
QFTlover	T duality	243	188	@lukasberns なるほど、ありがとうございます。 ちなみにこのことを考えたきっかけはこの論文でした https://t.co/CuPi4BJ0mX	2019/6/27 21:34					
qard_t	T-QARD channel	516	0	勾配法ベース https://t.co/lKV8snqXax 勾配フリー https://t.co/RgOimdbFRM https://t.co/bWAgFyrJ9F	2019/6/27 21:47					
re_hako_moon	はこつき＠VR	43	49	https://t.co/ZzrPPfEiJ5 三次元形状を二次元平面を集めて再構成する。Latent Spape Representationと単位平面上のサンプル点を入力として、いくつかのMLPがそれぞれ対応する三次元点を出力。三次元表面と二次元平面のマッピングを学習するため、歪みの少ないUVマップの作成などにも使える。	2019/6/27 23:12					
QFTlover	T duality	243	188	後で読む。T\barT変形  Moving the CFT into the bulk with $Tbar T$ https://t.co/W9JEwNKY6G	2019/6/28 0:07					
shion_honda	Shion Honda	1,234	242	Graphonomy [Gong+, 2019, CVPR] データセットによってクラスの種類や粒度が異なるhuman parsingを統一的に学習する方法を提案。クラスをノード、クラスの隣接・階層関係をエッジとするグラフを考えて、GNNを転移学習した。ラベルを活用することでSOTAを達成。 https://t.co/vddA3j1DmW #NowReading https://t.co/K0pcXpByeX	2019/6/28 1:04					
KBiostats	KingBiostats	22	188	U統計量(MVU, minimum-variance unbiasedな推定量を考えるときによく出てくる)の集中不等式を沢山網羅。便利。 https://t.co/MDoYcP7O2Z	2019/6/28 1:16					
GenerateTaiyaki	taiyaki	14	0	DeepView (CVPR2019oral https://t.co/QfzXEBIwke) 複数視点画像からのシーンのmultiplane image (MPI) の生成でSOTA.損失関数に対する勾配を用いてMPIを直接最適化すると収束が遅い上過学習するが,勾配からMPIの更新量を推定するCNNを用いて更新すると過学習せず高速に収束. https://t.co/zSb0nBQG6R https://t.co/BqvSOYMW7p	2019/6/28 1:39					
q9ac	將籠林檎 / ??	278	228	プラズマ振動の輻射緩和を記述するための場の量子化の方法について．有限系も無限系も取り扱っている．ナノ粒子に閉じ込められたプラズマ振動の量子化は実用的にも基礎的にも重要な話だと思うし，いい学位論文な気がするー． https://t.co/hs9nRZjw9o	2019/6/28 4:58					
q9ac	將籠林檎 / ??	278	228	アクシオンがある電磁気学からはカシミールフォースが引力だけではなく斥力になる領域が存在することが導かれるらしい。シータ項わからん(´･ω･｀) https://t.co/iI9hvOHg53	2019/6/28 5:47					
q9ac	將籠林檎 / ??	278	228	ワイル点を実現するフォトニック構造中では、電磁場の異常分散のお陰で、2つの量子系が長距離相互作用できるようになる。 https://t.co/vLiBNEyibZ	2019/6/28 5:52					
doiken_	どいけん	201	214	時代の流れ的にうまくいけば 今の暗号化って量子コンピュータにやられるよね → 量子暗号化しようよ！ → ブロックチェーンやびくない？ → 量子ブロックチェーン みたいな感じの流れで たぶん15年は先になるとは思うんだけど もう論文出てて時代なんだなぁと思ったわけ https://t.co/iQQiIpoYxu	2019/6/28 8:14					
K00TSUKA	大塚一輝	671	541	50KのCOCOの入力画像を部位でセグメンテーションしマーキングを施したDensePose-COCOを出力.GTX1080で240x320画像を20-26フレーム,800x1100を4-5フレームでリアルタイム処理.Deeplab,Mask-RCNNからのアーキテクチャ. -「DensePose: Dense Human Pose Estimation In The Wild」 https://t.co/mlEivOCbBo	2019/6/28 8:32					
MasWag	Masaki Waga	508	628	これ、PPLで可逆計算みたいだって僕とかが言っていた仕事っぽい [1906.11422] Stepping OCaml https://t.co/VQ3gGPjIg7	2019/6/28 10:21					
necrophilism	きゃりーねくねく	605	449	熱力学不確定性関係は何度 unify されるんだ？  https://t.co/D8MIRBUXuy	2019/6/28 10:46					
tomonoritotani	戸谷友則 (TOTANI, Tomonori)	352	0	https://t.co/R33mIkIBEq 謎の高速電波バースト(FRB)、繰り返すものと一回きりのものがあり、これまで母銀河が判明していたのは繰り返すFRBの一例だけでした。今回初めて、一回きりの種族で母銀河が特定されました。しかもその銀河は、繰り返すFRBの母銀河とは全く異なるタイプでした	2019/6/28 12:41					
q9ac	將籠林檎 / ??	278	228	物質波(原子)がグラフェンの表面で反射される際の磁気光学効果の影響について。原子はグラフェンとの間にCasimir-Polder forceを感じるが、これの媒介役のグラフェン上の電磁場は外から磁場をかけると磁気光学効果を感じる。これがC-P force、ひいては物質波の反射をモジュる https://t.co/8n4OlPsq72	2019/6/28 12:59					
wayama_ryousuke	Ryousuke_Wayama	198	194	これ他のタスクにも応用できそうな気がする。　https://t.co/clh2DmAbIf	2019/6/28 13:09					
aki_room	aki_room	1,301	489	ingappabilitiesって言うのね  https://t.co/dsjZ2IkBxp A generalized boundary condition applied to Lieb-Schultz-Mattis type ingappabilities Y. Yao and M. Oshikawa	2019/6/28 13:12					
32mao	良き隣人	34	70	これマジ?  https://t.co/mP5JO0mnr7	2019/6/28 13:14					
ueda_physics	Shu	1,116	1,148	PBH からの重力波を 3 次まで計算したら LISA とかで見える確率と範囲が大きくなったので PBH DM の可能性を消せるかもしれない https://t.co/DquRDqM4na	2019/6/28 14:51					
sir24de3	A.N.	147	175	金属水素の存在を示す論文が公開されている。量子閉じ込めの確認も行なっているようだし、今度は本当かもしれない。 古くから予言されてはいたが、最近の硫化水素の超高圧下における高温超伝導の発見と繋げてみると、現在進行する科学史を感じられる。  https://t.co/xXBE849YYN	2019/6/28 16:38					
R_O_R_I_J_O	?ろりじょ?	1,212	909	これ面白そう https://t.co/BItvkLAqed	2019/6/28 16:41					
qard_t	T-QARD channel	516	0	今回の岡田さんの発表に関する論文はこちら。  The efficient quantum and simulated annealing of Potts models using a half-hot constraint  Shuntaro Okada, Masayuki Ohzeki, Kazuyuki Tanaka  https://t.co/TVrTaecNwR https://t.co/ThGEDdrDd2	2019/6/28 17:14					
jan_zde	Jan Zdenek丨ズデニェク・ヤン	2	33	ExtremeNet https://t.co/mwNPZQ8KUu CornerNetインスパイアのモデル。Cornerポイントの代わりに四方のextremeポイントと中心点を予測する。予測されたポイントで通常のbboxが決まって、更に全extremeポイント繋げるとマスクっぽいもっと細かいbboxを出力できる。 https://t.co/RZWfGqhyjp	2019/6/28 17:17					
re_hako_moon	はこつき＠VR	43	49	https://t.co/NUuhXULTcm 三次元点群のセマンティックセグメンテーションとインスタンスセグメンテーションを同時に行う。点ごとのEmbeddingがインスタンスごとに近くなるように学習し、CRFを使ってセグメンテーション結果をRefineする。大規模な点群に対してWindowを走査させることで対応。	2019/6/28 18:05					
life_wont_wait	Q	2,170	57	前に出すぎだ！戻れ！と言いたくなるな/Can Neutron-Star Mergers Explain the r-process Enrichment in Globular Clusters? https://t.co/WcfOjV9HtM	2019/6/28 18:47					
life_wont_wait	Q	2,170	57	そんなことホントにできるのかorできたんだという素直な驚き/A single fast radio burst localized to a massive galaxy at cosmological distance https://t.co/W2ovZa6AUl	2019/6/28 19:04					
qard_t	T-QARD channel	516	0	その二例とは  リクルートさんのアイテムリスト最適化 DENSOさんの工場内AGV運行最適化  です！  どちらもT-QARDの共同研究の成果です。数ある応用事例から取り上げられたことを光栄に思います。真にビジネス的価値を生む点が注目を集めています。  https://t.co/E6yvjjCI8N https://t.co/1zb4s8pylF https://t.co/ExaOJmhyoS https://t.co/4jXPRbMrcu	2019/6/28 19:09					
cucumislily	jurilynn	116	185	実行時間も比較的速そうで良いな。ソースソードあるのかしら。 https://t.co/XDhK7LUvtJ	2019/6/28 19:27					
yjmtsmt	(っ'-')? =????◯本	225	200	https://t.co/YBtuHypFKy Toroidal Diamond Anvil Cell で400 GPa over出したのすごそう	2019/6/28 19:32					
daikiyamanaka	daikiyamanaka	325	458	Voxblox++ きましたね https://t.co/76FNl8evru A volumetric object-level semantic mapping framework. https://t.co/60Iq4ypvvV	2019/6/28 20:03					
shion_honda	Shion Honda	1,234	242	AtomNet [Wallach+, 2015] 3Dグリッド上に配置したタンパク質と小分子化合物の組に、3D-CNNを適用して活性を予測した。負例サンプリングに工夫があり、正例と記述子(分子量など)は近いが構造が似ていないものを不活性と仮定した。DUDE、ChEMBLなどで評価した。 https://t.co/ewsAVZoU7Z #NowReading https://t.co/RsBCqLebdd	2019/6/28 20:17					
tocom242242	tocom	57	62	【マルチエージェント深層強化学習】Value-Decomposition Networkについて 1706.05296.pdf https://t.co/XW33BEO1Ju	2019/6/28 20:29					
esumii	Eijiro Sumii	3,461	1,120	https://t.co/8SXowpyJiS をよろしくお願いします！（想定お約束陽マ https://t.co/yzlln4vUE3	2019/6/28 21:04					
candidusflumen	Toshihico　Shiracawa	1,501	2,013	「Machine Learning as Statistical Data Assimilation」 機械学習も統計的データ同化も深い所では、統計力学の問題だと主張している文書 → https://t.co/FlqABJJnLp	2019/6/28 21:57					
rarara_brahmin	ブラフ	34	86	そしてワイはこいつを実装するんや…（まだよく分かってない）  https://t.co/PFTFfBxkXJ	2019/6/28 22:01					
SO880	ドラミギ	1,319	1,302	https://t.co/4t8rYR79Dc とりあえずφ(..)メモメモ	2019/6/28 22:11					
deepchiji	D.C.	5	25	ちょっと前のこの論文にも、ImageNetタスクの精度に反比例して AlexNet &gt; VGG &gt; ResNet の順にテクスチャ耐性が強くなる(物の表面ではなくちゃんと全体的な形まで考慮して判断できるようになる)傾向があると書かれてあり、考えさせられた https://t.co/yViI8QKr7l https://t.co/VwKh7xlBu2	2019/6/29 0:12					
Seiki_KOMIYA	Seiki KOMIYA	53	231	そういや機械学習で超伝導体探索するのはT野さんもやってるって言ってたな。これかな。計算はT倉先生にかなり手伝ってもらってるんだろうか。https://t.co/390iEPmD3Y	2019/6/29 1:50					
yasu00327	yasu	138	136	結局卒研は https://t.co/oFQhOLxIjY これもじって プリページングに適応させたらどうなるかやってみようと思う	2019/6/29 5:36					
ballforest	mat	3,466	4,111	論文 https://t.co/jV9oSingll	2019/6/29 10:19					
shunk031	しゅんけー	1,701	667	「Webを学ぶためにはまずSHAKAIとかSEKAIを学ぶ必要があるみたい」と研究室後輩が言ってたので、hardmaruさんのWorld Modelsを進める優しい研究室先輩になってしまった / [1803.10122] World Models https://t.co/DaUEsymFrh	2019/6/29 12:26					
691_7758337633	たけのこ赤軍@MSFロス	1,930	1,911	arXiv に上げたプレプリントです  https://t.co/rYurMkQ4Jk https://t.co/ZzqqNc9lCD	2019/6/29 13:23					
grahamian2317	Grahamian@データ分析と機械学習	1,413	139	LIME完全に理解したいから読む https://t.co/oUgLUBEWZX	2019/6/29 14:19					
fluctuation326	AD	0	8	Concentration sensingにおいてMIMO問題を取り扱った。そこで、最尤推定法において対数尤度のヘッシアン行列の固有スペクトラムの性質が、Vandermonde行列の性質を備えていることを発見した。 https://t.co/a1SUf9524E	2019/6/29 17:53					
tonagai	tomo	406	88	パンケーキの最適化論文のarXivはこちら。 Pancake making and surface coating: optimal control of a gravity-driven liquid film https://t.co/5FkDWgvdJc	2019/6/29 18:52					
deltam	deltam?????♂???	520	738	"論文中で言及されてたのでTAOCPの4巻だけ衝動買いした。難易度高いとされてる問題を解決したらしい。 ""[1307.2549] Hamiltonicity of the Cayley Digraph on the Symmetric Group Generated by σ = (1 2 ... n) and τ = (1 2)"" https://t.co/aNfQscJeyT"	2019/6/29 20:51					
2StagePey	?????♂?じんぺちん	205	148	去年にも最高のピザの焼き方って論文が出たっけwww https://t.co/1GmCRq6dpB https://t.co/Ytxd38erSm	2019/6/29 22:40					
kazmuzik	Kaz Muzik	736	996	Transformer-XL https://t.co/KQndN7mVDc によって、BERTを超えたという噂のXLNet https://t.co/zRXG66hZBd のPythonの実装 https://t.co/McKw6O1QJ3 https://t.co/jKse9gORtu	2019/6/29 23:39					
daegon137	hsdk bd(炯淳)	1,368	2,625	最近見た中で一番面白い論文だな。「Magic: The Gathering is Turing Complete」https://t.co/hCIUYqH5X3	2019/6/29 23:53					
691_7758337633	たけのこ赤軍@MSFロス	1,930	1,911	@NegeLon あれ読んでないんですか？？？絶対に読むべきですこの論文はマジで頭おかしい最高  https://t.co/xHWlN6MTEM	2019/6/30 1:28					
YamanamiBooks	やまなみ書房	141	152	Phys. Rev. Fluid に掲載された論文はこちら。 https://t.co/KQlDHY9RLy arXivにも上げられています。 https://t.co/S2eCKIVKG6  https://t.co/CCrQSNBaD5	2019/6/30 2:30					
amasawa_seiji	ぬん。	1,027	467	論文はこちら。 ⇒/[1901.06028] Pancake making and surface coating: optimal control of a gravity-driven liquid film https://t.co/LssnBOJTfp	2019/6/30 2:42					
q9ac	將籠林檎 / ??	278	228	ある断熱的な過程を考えたときそれに付随する幾何学的位相というものがあることはよく知られているが、その断熱過程をロスのある過程にしてみるという設定幾何学的位相を複素数にするとうまく説明できるらしい。 https://t.co/YT8yodCxCI	2019/6/30 7:13					
q9ac	將籠林檎 / ??	278	228	カシミール効果は、ミラーに挟まれた真空というセットアップで電磁場の揺らぎを考えることで出てくるミラー間の引力相互作用だが、この真空の部分を電解質(液体)に置き換えてみたらどうなる？という試行。非局所応答があったりイオンがあるおかげで縦モードの寄与がでてくる。 https://t.co/IWHVJCuNVt	2019/6/30 7:17					
q9ac	將籠林檎 / ??	278	228	量子論における時間の矢の問題。コヒーレント状態のエントロピーが時間とともに増大していくことを示している⇔時間には向きがある https://t.co/pVBTRfK5ha	2019/6/30 7:24					
q9ac	將籠林檎 / ??	278	228	時空上に螺旋転位がある系のディラック場とそれを回転させた時の振る舞いの解析。  時空上の螺旋転位(・ω・ )!そんなのあるんだなあ https://t.co/ZvfAQrH4X4	2019/6/30 7:32					
q9ac	將籠林檎 / ??	278	228	半無限のバルクに詰まったディラック場と電磁場の相互作用。トポロジカル物質と電磁場の相互作用の解析にお役立ちらしい。 https://t.co/T1SiAKct18	2019/6/30 7:37					
q9ac	將籠林檎 / ??	278	228	二枚のグラフェンがカーボンナノチューブで繋がってる「ワームホール」みたいな不思議な物質を考えている。ワームホールがあるとランダウ量子化みたいなことが起こるらしい。 https://t.co/Yzfj0MDNgb	2019/6/30 7:58					
TsuguoMogami	mogami290	12	10	置換対称性のある集合を出力とするNNをどう作るかは前々から問題だった。 https://t.co/y4OeDu7jpM を私の言葉でぶっちゃければ、多対1で自然に表現できる集合→内部表現の逆関数として、1対多の関数を定義すれば集合を出力できるということ。境界に現れる不連続と置換対称性はソルバに押し付ける。	2019/6/30 11:31					
toshiki_recruit	たかはしとしき@リクルート	200	390	お https://t.co/I88upzMK6f	2019/6/30 12:19					
u_vi58	まむ	609	349	一方で、もしブラックボックス関数の入力空間が1次元ならば、ICML2018にhttps://t.co/WwyfgI6tWsが出ていて、タイトなcumulative regret boundを達成するアルゴリズムが得られていることが分かって面白い。	2019/6/30 12:22					
subarusatosi	中嶋慧	3,333	23	量子開放系のBerry位相に興味がある人は、私のD論 https://t.co/OYjL51WnSg や https://t.co/SYFiIZQKwl が面白いと感じるかも知れません。	2019/6/30 12:45					
yu4u	Yusuke Uchida	4,447	950	同じようにNASを4 hoursでできると言っているこの論文、4 hoursとは言ったがGPU hoursとは言っていない（TPUv2）のが利根川っぽくて良い / Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours https://t.co/vBBADsvHB0 #cvsaisentan	2019/6/30 14:50					
syao_ming	しゃをみん	281	108	少し前のGANSynthみたいに、位相推定を正しくできれば、時間周波数域上で高音質の音声合成ができますよという話。STFTについての論考が助かる・・・しかし、時間域信号の直接生成もそこそこ成功しているのに(WaveNet)、未だに位相推定はうまくいかないんだなぁ・・・ https://t.co/JqYuaRoKhR	2019/6/30 14:55					
esXFdfOJxiGBFLx	人工知能 Deep Learning AI image medical machine learni	858	1,901	学習しやすい画像を生成する方法に関するGAN論文です。 これは今後、どのようなポイントが画像の認識に重要かがわかるかもしれません。 医療では癌などの診断根拠になるような画像特徴を導ける可能性があります。 https://t.co/PCUJuhlPBA	2019/6/30 16:58					
akihiro_akichan	akihiro_f	122	118	https://t.co/dUqP0WZqSi CVPR2019.複数の物体を教師なしで追跡できるRATを提案。検出結果から画像を再構成してロスをとる機構、追跡の被りを防ぐためにFCNで抽出した特徴量ベクトルから１つ物体を検出するたびにメモリの更新(消去)をする機構がポイントか。教師なしでこれは凄いのではなかろうか。	2019/6/30 17:11					
mesh1bot	ぼっと@27	110	115	https://t.co/sBTPR2SSbH  これ一回はちゃんと読むべきだと思うんだよなー英語に強くなりたい・・	2019/6/30 17:25					
KSKSKSKS2	katsugeneration	177	444	2D画像を既存の3DモデルにマッチングさせるVisual Localizationのタスクで、階層的なニューラルネットモデルを使用することにより、頑健性と計算速度を両立させた手法を提案。GPU使用で20FPS出すことができる https://t.co/TnHl0WGHeg	2019/6/30 17:36					
Ab_ten	Ab.	60	36	https://t.co/Ql87Nnxj4u mixup: Beyond Empirical Risk Minimization に GAN の学習を stabilize するって書かれとったー。ちゃんと読まねば。	2019/6/30 17:57					
KSKSKSKS2	katsugeneration	177	444	音声および映像を元にした質問応答のタスクで、マルチモーダルなアテンションを利用したシンプルなモデルで、既存手法をCIDErで20ポイント上回る手法を提案 https://t.co/5mtnOrlCGO	2019/6/30 18:45					
KSKSKSKS2	katsugeneration	177	444	ビデオについてのキャプションを生成するタスクにおいて、映像情報を時間方向の変化や物体検出やアクション認識の結果を利用してエンコードするEnriched VisualEncodingという新しい手法を提案。MSVDなどのベンチマークで既存手法を上回る結果を示した。 https://t.co/dmsfNFClJR	2019/6/30 19:55					
candidusflumen	Toshihico　Shiracawa	1,501	2,013	「A theory of quantum gravity based on quantum computation」 量子計算に基づき重力と量子力学を統一しようという話だ。今流行りの研究のように見えるが、元々2005年に初稿が上げられ、このバージョンは2018年のものだ。結構古い時期に量子情報と重力の関連に注目している https://t.co/WzNtoaNt83	2019/6/30 21:40					
candidusflumen	Toshihico　Shiracawa	1,501	2,013	「Quantum Computation as Gravity」 情報理論も重力の理論も幾何学で扱えるので、幾何学的観点から統一しようという試みのようだ → https://t.co/c2RKlieNOH	2019/6/30 21:55					
KSKSKSKS2	katsugeneration	177	444	画像とその説明文が与えられ、説明文内の単語と画像の対応関係を求めるPhrase Groundingのタスクにおいて、学習時に説明文と画像のペアしか与えられない弱教師あり学習の設定で、既存手法を大幅に上回る性能を示した https://t.co/KPlmlbf2Gt	2019/6/30 22:03					
hsntdo	Hoshino Tadao	286	100	タイトルからしてすごそうな論文みつけた（アブストだけ読んだ） https://t.co/E9UcztBbNA	2019/6/30 22:24					
asam9891	Masa.I	470	128	Adversarial Examples Are Not Bugs, They Are Features さっきの投稿の参考文献。robust な特徴は人の感覚と一致してる普遍的な特徴、non robust な特徴は人の感覚と必ずしも一致しないが特定のクラスの識別に重要な特徴。non robust な特徴が敵対事例の原因となっている https://t.co/78rcMx5FXZ	2019/6/30 22:34					
kosukemizz0310	kosuke.M	529	499	分位点回帰を用いたDistributional RLの改良。分位点ごとにモデルを近似することをやめ、分位点のembeddingをモデルに加えている。また、リスクを考慮したモデル化も行った。 https://t.co/WDhNDtGC60	2019/7/1 1:36					
KSKSKSKS2	katsugeneration	177	444	クエリ画像と同じものが写っている画像を検索するタスクで、ローカル特徴量によるリランキングを後付けできる手法を提案。グローバルな特徴量を出力する既存のCNNを再学習せずに、ベースモデルによる類似度検索と比較し、最大で5ポイントほどmAPやmP@10が向上することを示した https://t.co/2NDZg51F2w	2019/7/1 9:02					
icoxfog417	piqcy	8,884	127	"BERTを超えたと話題になったXLNetのコストについての話。論文中では""512 TPU v3 chips""で2.5 daysと言及されている。1TPUは4chipsで構成されるので128TPUを2.5days=$61,440ほどになるとの試算(660万ほど)。一発で上手くいったはずはないと思うので実態はさらに上と思われる  https://t.co/ZpnHUq3GwV https://t.co/GS0FirpDNV"	2019/7/1 9:37					
yu4u	Yusuke Uchida	4,447	950	FPGAﾖｲｼｮｰｯ / “[1906.11879] Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels” https://t.co/b6KjqFIYTL	2019/7/1 10:06					
L_H_Sullivan	ふー??じん??	368	319	Lattice and magnetic dynamics in polar chiral incommensurate antiferromagnet Ni2InSbO6 https://t.co/W1I7pQTN2A arkパイセンがファーストちゃうの？	2019/7/1 11:10					
L_H_Sullivan	ふー??じん??	368	319	Complex magnetic phase diagram of metamagnetic MnPtSi https://t.co/CXM6KGIjel 興味深い	2019/7/1 11:12					
hmkz_	はま	1,082	2,520	GA（正確に言うと進化計算）意外と強いですよ。最近流行りのOne-shot NAS（ニューラルネットの学習と同時に構造探索もする技術）も先の論文の発展形ですし。 https://t.co/Te7VPRkGK7	2019/7/1 13:01					
kawashima2201_	Q学習をかじった?Kawashima??	62	320	参加してる機械学習勉強会で「A free energy principle for a particular physics」って論文の解読会が2,3ヶ月以内に開かれると聞いて泣いて震えてる https://t.co/LDQHmvF3H0	2019/7/1 13:09					
Kenji_Sugisaki	杉﨑 研司 (Kenji Sugisaki)	360	127	Hole-Spin-Echo Envelope Modulations https://t.co/KM7LBvH6eR ESR関係でESEEMはなじみがあるけどそれのhole版。気になる。	2019/7/1 13:56					
ikkoham	濵村 一航 Ikko Hamamura??	633	501	最近出た良いレビューらしい https://t.co/IdZmLSFZ1l #qikansai26	2019/7/1 15:02					
st_red_strat	's.to;t@??はご飯の上に魚が乗ってるから神	287	990	@enthal_p ミクロな因果律を指導原理として仮定する、というのならまだ良いけど  そもそもラグランジアンがnon-localな相互作用しか含んでなくてもマクロにconsistentかどうかは全く自明ではない https://t.co/1dvYhnmaIq ミクロな因果律からマクロな因果律は一般には言えないし、逆を無批判に仮定するのもヤバい	2019/7/1 15:42					
levelfour_	Han Bao	1,135	531	@tmiya_ なるほど…UCBで考えるとそうなりそうですね。最近だとおそらくそういう問題意識があってgood arm identification（ある程度良い腕をすべて選ぶ）みたいな問題も模索されているんだろうなと思いました。 https://t.co/7Q4Grxv0mC	2019/7/1 16:08					
tmaehara	? ??	6,881	717	A History of Metaheuristics https://t.co/IDQ2wrHyFJ 面白かった．	2019/7/1 17:34					
Lepidoptera2015	あおすじあげはちょう	290	347	https://t.co/zojYyqGSQs 推薦におけるfairnessってどう定義すればいいの？って話	2019/7/1 17:37					
dojin_tw	安藤道人	3,896	901	先週、サイバーエージェントの安井翔太氏@housecat442によるゲスト講義を行った。  学部生向け講義だが、広告業界における機械学習や因果推論の活用がテーマで、私が一番質問してしまった。  安井氏とイェール大の成田氏らとの共著論文（Narita, Yasui, Yata 2019)はこちら https://t.co/IUYAFx27tN https://t.co/9YjPKS3JKb	2019/7/1 17:42					
hhayakawa	早川尚男	1,992	426	https://t.co/tJ3tI6tzMI　或いは@Qhapaq_49 のseminarで https://t.co/FUQNoPzUFz の紹介がありました。	2019/7/1 17:59					
atomicchildren	あとむ	258	491	arxivの結果。https://t.co/66RwIHIXp2	2019/7/1 18:06					
esXFdfOJxiGBFLx	人工知能 Deep Learning AI image medical machine learni	858	1,901	MaskR-CNNによる乳がんの検出&amp;Segmentationに関するarxivです。 マンモグラフィーをグレーから擬似カラーに変化し、カラーコントラストとして塊状パターンを認識しやすくした点にあります。シーケンスをもつMRIやモダリティ間の差を特徴にすることができる可能性があります。 https://t.co/zZKWdFqiG9	2019/7/1 19:25					
en_sof	?zero	159	134	さっそく https://t.co/tDgzgYiqyu	2019/7/1 20:38					
osciiart	OsciiArt◆SPNEXTcRxQ	1,905	1,166	ラベルありデータとラベルなしデータでクラスの分布が違うと半教師あり学習はやらない方がましになる。というデータはあったわ。 https://t.co/Q7e9PLHbH8 https://t.co/4suteBSRB0	2019/7/1 20:54					
diskshima	Daisuke Shimamoto	369	952	時々考え過ぎちゃって、不必要に複雑にしちゃうのかもしれませんね。 ・Forget ゲート ・Chronic initialization で MNIST で良い結果が出たそうです。  The unreasonable effectiveness of the forget gate https://t.co/s73rTYydhx	2019/7/1 21:23	https://arxiv.org/abs/1804.04849	The unreasonable effectiveness of the forget gate	Given the success of the gated recurrent unit, a natural question is whetherall the gates of the long short-term memory (LSTM) network are necessary.Previous research has shown that the forget gate is one of the most importantgates in the LSTM. Here we show that a forget-gate-only version of the LSTMwith chrono-initialized biases, not only provides computational savings butoutperforms the standard LSTM on multiple benchmark datasets and competes withsome of the best contemporary models. Our proposed network, the JANET, achievesaccuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming thestandard LSTM which yields accuracies of 98.5% and 91%.	Neural and Evolutionary Computing (cs.NE)	Machine Learning (cs.LG);Machine Learning (stat.ML)
necrophilism	きゃりーねくねく	605	449	全然知らなかったけど，AI Feynman https://t.co/UKvC1C6uU8 なるものが話題になってたのか．	2019/7/1 21:41	https://arxiv.org/abs/1905.11481	AI Feynman: a Physics-Inspired Method for Symbolic Regression	A core challenge for both physics and artificial intellicence (AI) issymbolic regression: finding a symbolic expression that matches data from anunknown function. Although this problem is likely to be NP-hard in principle,functions of practical interest often exhibit symmetries, separability,compositionality and other simplifying properties. In this spirit, we develop arecursive multidimensional symbolic regression algorithm that combines neuralnetwork fitting with a suite of physics-inspired techniques. We apply it to 100equations from the Feynman Lectures on Physics, and it discovers all of them,while previous publicly available software cracks only 71; for a more difficulttest set, we improve the state of the art success rate from 15% to 90%.	Computational Physics (physics.comp-ph)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG);High Energy Physics - Theory (hep-th)
re_hako_moon	はこつき＠VR	43	49	https://t.co/BACDU24k4Q 単眼カメラ画像から人間のメッシュを再構成する。一度画像からメッシュを構築後、Joint、Anchor、Vertexの順に徐々にメッシュの変形を推定していくことで密に細かい形状（衣服など）を含めた再構成を実現。	2019/7/1 21:53	https://arxiv.org/abs/1904.10506	Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation	This paper presents a novel framework to recover detailed human body shapesfrom a single image. It is a challenging task due to factors such as variationsin human shapes, body poses, and viewpoints. Prior methods typically attempt torecover the human body shape using a parametric based template that lacks thesurface details. As such the resulting body shape appears to be withoutclothing. In this paper, we propose a novel learning-based framework thatcombines the robustness of parametric model with the flexibility of free-form3D deformation. We use the deep neural networks to refine the 3D shape in aHierarchical Mesh Deformation (HMD) framework, utilizing the constraints frombody joints, silhouettes, and per-pixel shading information. We are able torestore detailed human body shapes beyond skinned models. Experimentsdemonstrate that our method has outperformed previous state-of-the-artapproaches, achieving better accuracy in terms of both 2D IoU number and 3Dmetric distance. The code is available in this https URL	Computer Vision and Pattern Recognition (cs.CV)	Image and Video Processing (eess.IV)
mlmasasing	Masa	72	213	XLNetの論文をメモしておく https://t.co/nHgneekTeM	2019/7/1 22:15	https://arxiv.org/abs/1906.08237	XLNet: Generalized Autoregressive Pretraining for Language Understanding	With the capability of modeling bidirectional contexts, denoisingautoencoding based pretraining like BERT achieves better performance thanpretraining approaches based on autoregressive language modeling. However,relying on corrupting the input with masks, BERT neglects dependency betweenthe masked positions and suffers from a pretrain-finetune discrepancy. In lightof these pros and cons, we propose XLNet, a generalized autoregressivepretraining method that (1) enables learning bidirectional contexts bymaximizing the expected likelihood over all permutations of the factorizationorder and (2) overcomes the limitations of BERT thanks to its autoregressiveformulation. Furthermore, XLNet integrates ideas from Transformer-XL, thestate-of-the-art autoregressive model, into pretraining. Empirically, XLNetoutperforms BERT on 20 tasks, often by a large margin, and achievesstate-of-the-art results on 18 tasks including question answering, naturallanguage inference, sentiment analysis, and document ranking.	Computation and Language (cs.CL)	Machine Learning (cs.LG)
KSuzukiii	すずきぃ@求職	348	306	やってることはわかったんだけど、一から作るのめんどいなぁ... / Data-Free Quantization through Weight Equalization and Bias Correction https://t.co/MApWU1ioSs	2019/7/1 22:56	https://arxiv.org/abs/1906.04721	Data-Free Quantization through Weight Equalization and Bias Correction	We introduce a data-free quantization method for deep neural networks thatdoes not require fine-tuning or hyperparameter selection. It achievesnear-original model performance on common computer vision architectures andtasks. 8-bit fixed-point quantization is essential for efficient inference inmodern deep learning hardware architectures. However, quantizing models to runin 8-bit is a non-trivial task, frequently leading to either significantperformance reduction or engineering time spent on training a network to beamenable to quantization. Our approach relies on equalizing the weight rangesin the network by making use of a scale-equivariance property of activationfunctions. In addition the method corrects biases in the error that areintroduced during quantization. This improves quantization accuracyperformance, and can be applied ubiquitously to almost any model with astraight-forward API call. For common architectures, such as the MobileNetfamily, we achieve state-of-the-art quantized model performance. We furthershow that the method also extends to other computer vision architectures andtasks such as semantic segmentation and object detection.	Machine Learning (cs.LG)	Computer Vision and Pattern Recognition (cs.CV);Machine Learning (stat.ML)
hayashiyus	Yusuke HAYASHI 林 祐輔	3,520	565	統計モデルに逆温度を導入する一般的な手順．  1. https://t.co/3i7VpTqtOP 2. https://t.co/1yt6OIqScb https://t.co/rcz8YxUPoN	2019/7/1 23:51	https://arxiv.org/abs/1004.2316, https://arxiv.org/abs/1208.6338	Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory, A Widely Applicable Bayesian Information Criterion	In regular statistical models, the leave-one-out cross-validation isasymptotically equivalent to the Akaike information criterion. However, sincemany learning machines are singular statistical models, the asymptotic behaviorof the cross-validation remains unknown. In previous studies, we establishedthe singular learning theory and proposed a widely applicable informationcriterion, the expectation value of which is asymptotically equal to theaverage Bayes generalization loss. In the present paper, we theoreticallycompare the Bayes cross-validation loss and the widely applicable informationcriterion and prove two theorems. First, the Bayes cross-validation loss isasymptotically equivalent to the widely applicable information criterion as arandom variable. Therefore, model selection and hyperparameter optimizationusing these two values are asymptotically equivalent. Second, the sum of theBayes generalization error and the Bayes cross-validation error isasymptotically equal to $2\lambda/n$, where $\lambda$ is the real log canonicalthreshold and $n$ is the number of training samples. Therefore the relationbetween the cross-validation error and the generalization error is determinedby the algebraic geometrical structure of a learning machine. We also clarifythat the deviance information criteria are different from the Bayescross-validation and the widely applicable information criterion., A statistical model or a learning machine is called regular if the map takinga parameter to a probability distribution is one-to-one and if its Fisherinformation matrix is always positive definite. If otherwise, it is calledsingular. In regular statistical models, the Bayes free energy, which isdefined by the minus logarithm of Bayes marginal likelihood, can beasymptotically approximated by the Schwarz Bayes information criterion (BIC),whereas in singular models such approximation does not hold.Recently, it was proved that the Bayes free energy of a singular model isasymptotically given by a generalized formula using a birational invariant, thereal log canonical threshold (RLCT), instead of half the number of parametersin BIC. Theoretical values of RLCTs in several statistical models are now beingdiscovered based on algebraic geometrical methodology. However, it has beendifficult to estimate the Bayes free energy using only training samples,because an RLCT depends on an unknown true distribution.In the present paper, we define a widely applicable Bayesian informationcriterion (WBIC) by the average log likelihood function over the posteriordistribution with the inverse temperature $1/\log n$, where $n$ is the numberof training samples. We mathematically prove that WBIC has the same asymptoticexpansion as the Bayes free energy, even if a statistical model is singular forand unrealizable by a statistical model. Since WBIC can be numericallycalculated without any information about a true distribution, it is ageneralized version of BIC onto singular statistical models.	Machine Learning (cs.LG), Machine Learning (cs.LG)	, Machine Learning (stat.ML)
fullflu	fullflu	85	137	多重代入法と機械学習のアンサンブル学習をどう結びつけるのか、ってのを研究している事例があれば知りたい。arxivとかにちらほらあるけど、あんまり調べてないのでこれが王道なのかはわからない https://t.co/Rut3mNrOPe	2019/7/2 5:23	https://arxiv.org/abs/1802.00154	Bootstrapping and Multiple Imputation Ensemble Approaches for Missing Data	Presence of missing values in a dataset can adversely affect the performanceof a classifier. Single and Multiple Imputation are normally performed to fillin the missing values. In this paper, we present several variants of combiningsingle and multiple imputation with bootstrapping to create ensembles that canmodel uncertainty and diversity in the data, and that are robust to highmissingness in the data. We present three ensemble strategies: bootstrapping onincomplete data followed by (i) single imputation and (ii) multiple imputation,and (iii) multiple imputation ensemble without bootstrapping. We perform anextensive evaluation of the performance of the these ensemble strategies on 8datasets by varying the missingness ratio. Our results show that bootstrappingfollowed by multiple imputation using expectation maximization is the mostrobust method even at high missingness ratio (up to 30%). For small missingnessratio (up to 10%) most of the ensemble methods perform quivalently but betterthan single imputation. Kappa-error plots suggest that accurate classifierswith reasonable diversity is the reason for this behaviour. A consistentobservation in all the datasets suggests that for small missingness (up to10%), bootstrapping on incomplete data without any imputation producesequivalent results to other ensemble methods.	Machine Learning (cs.LG)	
tmaehara	? ??	6,881	717	わたしはアルゴリズム系に出すときは conclusion カットします．たとえば https://t.co/Trc6I0St7y https://t.co/9na2Pv9CeT https://t.co/XvAQaLYL6w あたりは conclusion なし． https://t.co/EOenzmBYNB	2019/7/2 7:41	https://arxiv.org/abs/1707.04020, https://arxiv.org/abs/1902.08742	Stochastic Packing Integer Programs with Few Queries, Optimal Algorithm to Reconstruct a Tree from a Subtree Distance	We consider a stochastic variant of the packing-type integer linearprogramming problem, which contains random variables in the objective vector.We are allowed to reveal each entry of the objective vector by conducting aquery, and the task is to find a good solution by conducting a small number ofqueries. We propose a general framework of adaptive and non-adaptive algorithmsfor this problem, and provide a unified methodology for analyzing theperformance of those algorithms. We also demonstrate our framework by applyingit to a variety of stochastic combinatorial optimization problems such asmatching, matroid, and stable set problems., This paper addresses the problem of finding a representation of a subtreedistance, which is an extension of the tree metric. We show that a minimalrepresentation is uniquely determined by a given subtree distance, and give alinear time algorithm that finds such a representation. This algorithm achievesthe optimal time complexity.	Data Structures and Algorithms (cs.DS), Data Structures and Algorithms (cs.DS)	
akihiro_akichan	akihiro_f	122	118	https://t.co/cYf0mKYOMC 印象の残りやすさを調整できる機構GANalyzeを提案。生成器Gと印象度検査器Aを固定し、調整したいパラメータaと潜在変数zの埋め込み器Tを学習させ、zが固定なのでaを調整すれば同じ構図で印象に残りやすい画像を生成できる。 https://t.co/KbyiCTk1RS	2019/7/2 8:23	https://arxiv.org/abs/1906.10112	GANalyze: Toward Visual Definitions of Cognitive Image Properties	"We introduce a framework that uses Generative Adversarial Networks (GANs) tostudy cognitive properties like memorability, aesthetics, and emotionalvalence. These attributes are of interest because we do not have a concretevisual definition of what they entail. What does it look like for a dog to bemore or less memorable? GANs allow us to generate a manifold of natural-lookingimages with fine-grained differences in their visual attributes. By navigatingthis manifold in directions that increase memorability, we can visualize whatit looks like for a particular generated image to become more or lessmemorable. The resulting ``visual definitions"" surface image properties (like``object size"") that may underlie memorability. Through behavioral experiments,we verify that our method indeed discovers image manipulations that causallyaffect human memory performance. We further demonstrate that the same frameworkcan be used to analyze image aesthetics and emotional valence. Visit theGANalyze website at this http URL."	Computer Vision and Pattern Recognition (cs.CV)	
TalesOfOdajun	TalesOfOdajun	165	221	読む https://t.co/7MOrnhSH4o	2019/7/2 9:23	https://arxiv.org/abs/1903.10970	Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing	Apache Hive is an open-source relational database system for analyticbig-data workloads. In this paper we describe the key innovations on thejourney from batch tool to fully fledged enterprise data warehousing system. Wepresent a hybrid architecture that combines traditional MPP techniques withmore recent big data and cloud concepts to achieve the scale and performancerequired by today's analytic applications. We explore the system by detailingenhancements along four main axis: Transactions, optimizer, runtime, andfederation. We then provide experimental results to demonstrate the performanceof the system for typical workloads and conclude with a look at the communityroadmap.	Databases (cs.DB)	
icoxfog417	piqcy	8,884	127	失われた言語の解読(翻訳)を行う研究。失われている故に大規模なコーパスは使えないので、同族言語とのアライメントを手がかりに翻訳を行なっている。具体的には、既知の同族言語における文字の並び/単語の出現確率で制約をかけて生成を行なっている。  https://t.co/wyBQd6rOkR	2019/7/2 9:34	https://arxiv.org/abs/1906.06718	Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear B	In this paper we propose a novel neural approach for automatic deciphermentof lost languages. To compensate for the lack of strong supervision signal, ourmodel design is informed by patterns in language change documented inhistorical linguistics. The model utilizes an expressive sequence-to-sequencemodel to capture character-level correspondences between cognates. Toeffectively train the model in an unsupervised manner, we innovate the trainingprocedure by formalizing it as a minimum-cost flow problem. When applied to thedecipherment of Ugaritic, we achieve a 5.5% absolute improvement overstate-of-the-art results. We also report the first automatic results indeciphering Linear B, a syllabic language related to ancient Greek, where ourmodel correctly translates 67.3% of cognates.	Computation and Language (cs.CL)	
saeeeeru	さえない / Saeru Yamamuro	1,646	482	"『ザ・フォーミュラ』の著者であるバラバシの研究グループが""サッカー選手を評価する専門家の能力""について分析した論文。 https://t.co/RX99b4iXKv"	2019/7/2 9:36	https://arxiv.org/abs/1712.02224	Human Perception of Performance	Humans are routinely asked to evaluate the performance of other individuals,separating success from failure and affecting outcomes from science toeducation and sports. Yet, in many contexts, the metrics driving the humanevaluation process remain unclear. Here we analyse a massive dataset capturingplayers' evaluations by human judges to explore human perception of performancein soccer, the world's most popular sport. We use machine learning to design anartificial judge which accurately reproduces human evaluation, allowing us todemonstrate how human observers are biased towards diverse contextual features.By investigating the structure of the artificial judge, we uncover the aspectsof the players' behavior which attract the attention of human judges,demonstrating that human evaluation is based on a noticeability heuristic whereonly feature values far from the norm are considered to rate an individual'sperformance.	Physics and Society (physics.soc-ph)	Artificial Intelligence (cs.AI);Data Analysis, Statistics and Probability (physics.data-an);Applications (stat.AP)
Ryuhei_Mori	Mori	100	50	グラフの彩色数が O(1.9140^n)時間の量子アルゴリズムで計算できるというお話。 https://t.co/flRkOBKJyY 古典アルゴリズムでは branching が包除原理に負けることが多いけど、量子アルゴリズムでは逆転できる。 量子アルゴリズムなんも知らなくても、古典アルゴリズムが好きなら参戦できるので是非。	2019/7/2 10:00	https://arxiv.org/abs/1907.00529	Exponential-time quantum algorithms for graph coloring problems	The fastest known classical algorithm deciding the $k$-colorability of$n$-vertex graph requires running time $\Omega(2^n)$ for $k\ge 5$. In thiswork, we present an exponential-space quantum algorithm computing the chromaticnumber with running time $O(1.9140^n)$ using quantum random access memory(QRAM). Our approach is based on Ambainis et al's quantum dynamic programmingwith applications of Grover's search to branching algorithms. We also present apolynomial-space quantum algorithm not using QRAM for the graph $20$-coloringproblem with running time $O(1.9575^n)$. In the polynomial-space quantumalgorithm, we essentially show $(4-\epsilon)^n$-time classical algorithms thatcan be improved quadratically by Grover's search.	Data Structures and Algorithms (cs.DS)	Quantum Physics (quant-ph)
qard_t	T-QARD channel	516	0	論文はこちら https://t.co/MButwL5k8L https://t.co/mT86T0CzSC	2019/7/2 10:38	https://arxiv.org/abs/1907.00707	Quantum-Assisted Genetic Algorithm	Genetic algorithms, which mimic evolutionary processes to solve optimizationproblems, can be enhanced by using powerful semi-local search algorithms asmutation operators. Here, we introduce reverse quantum annealing, a class ofquantum evolutions that can be used for performing families of quasi-local orquasi-nonlocal search starting from a classical state, as novel sources ofmutations. Reverse annealing enables the development of genetic algorithms thatuse quantum fluctuation for mutations and classical mechanisms for thecrossovers -- we refer to these as Quantum-Assisted Genetic Algorithms (QAGAs).We describe a QAGA and present experimental results using a D-Wave 2000Qquantum annealing processor. On a set of spin-glass inputs, standard (forward)quantum annealing finds good solutions very quickly but struggles to findglobal optima. In contrast, our QAGA proves effective at finding global optimafor these inputs. This successful interplay of non-local classical and quantumfluctuations could provide a promising step toward practical applications ofNoisy Intermediate-Scale Quantum (NISQ) devices for heuristic discreteoptimization.	Quantum Physics (quant-ph)	Neural and Evolutionary Computing (cs.NE)
whisponchan	NoriakiOshita	1,671	719	@AskOkra ソースドメインとターゲットドメインの分布のposterior ratioというのを定義して、その値を訓練することによって転移学習するのは、定式化も綺麗なのでいいと思います。 https://t.co/Su7L8sD3to こちらはチョットテクニカルですが最近の傾向を知るのに良いと思います。 https://t.co/KsQfgagGLs	2019/7/2 10:52	https://arxiv.org/abs/1506.02784	Estimating Posterior Ratio for Classification: Transfer Learning from Probabilistic Perspective	Transfer learning assumes classifiers of similar tasks share certainparameter structures. Unfortunately, modern classifiers uses sophisticatedfeature representations with huge parameter spaces which lead to costlytransfer. Under the impression that changes from one classifier to anothershould be ``simple'', an efficient transfer learning criteria that only learnsthe ``differences'' is proposed in this paper. We train a \emph{posteriorratio} which turns out to minimizes the upper-bound of the target learningrisk. The model of posterior ratio does not have to share the same parameterspace with the source classifier at all so it can be easily modelled andefficiently trained. The resulting classifier therefore is obtained by simplymultiplying the existing probabilistic-classifier with the learned posteriorratio.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
norita113	nrt@ブルベ冬	710	827	え？まじ？ / [1907.00015] Dark matter heating of gas accreting onto Sgr A$^*$ - https://t.co/NMEv55YkZh	2019/7/2 10:56	https://arxiv.org/abs/1907.00015	Dark matter heating of gas accreting onto Sgr A$^*$	We study effects of heating by dark matter (DM) annihilation on black holegas accretion. We observe that, for reasonable assumptions about DM densitiesin spikes around supermassive black holes, as well as DM masses andannihilation cross-sections within the standard WIMP model, heating by DMannihilation may have an appreciable effect on the accretion onto Sgr A$^*$ inthe Galactic center. Motivated by this observation we study the effects of suchheating on Bondi accretion, i.e. spherically symmetric, steady-state Newtonianaccretion onto a black hole. We consider different adiabatic indices for thegas, and different power-law exponents for the DM density profile. We find thattypical transonic solutions with heating have a significantly reduced accretionrate. However, for many plausible parameters, transonic solutions do not exist,suggesting a breakdown of the underlying assumptions of steady-state Bondiaccretion. Our findings indicate that heating by DM annihilation may play animportant role in the accretion onto supermassive black holes at the center ofgalaxies, and may help explain the low accretion rate observed for Sgr A$^*$.	High Energy Astrophysical Phenomena (astro-ph.HE)	General Relativity and Quantum Cosmology (gr-qc)
esXFdfOJxiGBFLx	人工知能 Deep Learning AI image medical machine learni	858	1,901	皮膚領域のsegmentationに関するarXivです。 少し珍しいのは皮膚を撮影する装置に組み込めるようにパラメータを削減したGANを提案している点である。ボトルネックをなくし、位置とchannel attentionを追加することでパラメータを削減しつつ、既存より性能を出している。 https://t.co/E8NMLSODVk	2019/7/2 10:58	https://arxiv.org/abs/1907.00856	MobileGAN: Skin Lesion Segmentation Using a Lightweight Generative Adversarial Network	Skin lesion segmentation in dermoscopic images is a challenge due to theirblurry and irregular boundaries. Most of the segmentation approaches based ondeep learning are time and memory consuming due to the hundreds of millions ofparameters. Consequently, it is difficult to apply them to real dermatoscopedevices with limited GPU and memory resources. In this paper, we propose alightweight and efficient Generative Adversarial Networks (GAN) model, calledMobileGAN for skin lesion segmentation. More precisely, the MobileGAN combines1D non-bottleneck factorization networks with position and channel attentionmodules in a GAN model. The proposed model is evaluated on the test dataset ofthe ISBI 2017 challenges and the validation dataset of ISIC 2018 challenges.Although the proposed network has only 2.35 millions of parameters, it is stillcomparable with the state-of-the-art. The experimental results show that ourMobileGAN obtains comparable performance with an accuracy of 97.61%.	Image and Video Processing (eess.IV)	Computer Vision and Pattern Recognition (cs.CV)
Ryuhei_Mori	Mori	100	50	短い。https://t.co/D6RMM3HDeX	2019/7/2 11:07	https://arxiv.org/abs/1907.00847	Induced subgraphs of hypercubes and a proof of the Sensitivity Conjecture	In this paper, we show that every $(2^{n-1}+1)$-vertex induced subgraph ofthe $n$-dimensional cube graph has maximum degree at least $\sqrt{n}$. Thisresult is best possible, and improves a logarithmic lower bound shown by Chung,Füredi, Graham and Seymour in 1988. As a direct consequence, we prove thatthe sensitivity and degree of a boolean function are polynomially related,solving an outstanding foundational problem in theoretical computer science,the Sensitivity Conjecture of Nisan and Szegedy.	Combinatorics (math.CO)	Computational Complexity (cs.CC)
ueda_physics	Shu	1,116	1,148	Higgs inflation の一種でアクシオンの質量などを計算してみた, scalar-to-tensor ratio が 1e-12 くらいになるのが特徴 https://t.co/wmdeGiDvrt	2019/7/2 11:22	https://arxiv.org/abs/1906.11837	Axion dark matter from Higgs inflation with an intermediate $H_*$	In order to accommodate the QCD axion as the dark matter (DM) in a model inwhich the Peccei-Quinn (PQ) symmetry is broken before the end of inflation, arelatively low scale of inflation has to be invoked in order to avoid boundsfrom DM isocurvature fluctuations. We construct a simple model in which theStandard Model Higgs field is non-minimally coupled to gravity and acts as theinflaton, leading to a scale of inflation $H_* \sim 10^8\,$GeV. When the PQsymmetry is incorporated in the model and the energy scale at which thesymmetry breaks is much larger than the scale of inflation, we find that inthis scenario the required axion mass for which the axion constitutes all DM is$m_0 \lesssim 0.05{\rm \,\mu eV}$ for a quartic Higgs self-coupling$\lambda_\phi = 0.1$, which correspond to the PQ breaking scale $v_\sigma\gtrsim 10^{14}\,$GeV and tensor-to-scalar ratio $r \sim 10^{-12}$. Futureexperiments sensitive to the relevant QCD axion mass scale can therefore shedlight on the physics of the Universe before the end of inflation.	Cosmology and Nongalactic Astrophysics (astro-ph.CO)	General Relativity and Quantum Cosmology (gr-qc);High Energy Physics - Phenomenology (hep-ph)
ueda_physics	Shu	1,116	1,148	Configuration entropy の最小値とダークエネルギーの状態方程式・パラメータの関係から, 後者に制約を付けられるかもしれない https://t.co/gKwPLE5ToQ	2019/7/2 11:22	https://arxiv.org/abs/1907.00331	Can we constrain the dark energy equation of state parameters using configuration entropy?	We propose a new scheme for constraining the dark energy equation of stateparameter/parameters based on the study of the evolution of the configurationentropy. We analyze a set of one parameter and two parameter dynamical darkenergy models and find that the derivative of the configuration entropy in allthe dynamical dark energy models exhibit a minimum. The magnitude of theminimum of the entropy rate is decided by both the form of the equation ofstate as well as the parameters associated with it. The location of the minimumof the entropy rate is less sensitive to the equation of state and dependsmainly on its parameters. We determine the best fit equations for the locationand magnitude of the minimum of the entropy rate in terms of theparameter/parameters of the dark energy equation of state. These relationswould allow us to constrain the dark energy equation of stateparameter/parameters for any given parametrization provided the evolution ofthe configuration entropy in the Universe is known from observations.	Cosmology and Nongalactic Astrophysics (astro-ph.CO)	
osciiart	OsciiArt◆SPNEXTcRxQ	1,905	1,166	元々軽量なモデルを作るためにdistillationは提案されているが同じサイズで高性能のモデルを使うために利用することはBorn Again Network (BAN) で提案されている。ただしpseudo labeling がhard targetを使うに対しBANは原則soft targetを使う等ところどころ異なる。 https://t.co/KU9IjCAANN	2019/7/2 12:15	https://arxiv.org/abs/1805.04770	Born Again Neural Networks	Knowledge distillation (KD) consists of transferring knowledge from onemachine learning model (the teacher}) to another (the student). Commonly, theteacher is a high-capacity model with formidable performance, while the studentis more compact. By transferring knowledge, one hopes to benefit from thestudent's compactness. %we desire a compact model with performance close to theteacher's. We study KD from a new perspective: rather than compressing models,we train students parameterized identically to their teachers. Surprisingly,these {Born-Again Networks (BANs), outperform their teachers significantly,both on computer vision and language modeling tasks. Our experiments with BANsbased on DenseNets demonstrate state-of-the-art performance on the CIFAR-10(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additionalexperiments explore two distillation objectives: (i) Confidence-Weighted byTeacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).Both methods elucidate the essential components of KD, demonstrating a role ofthe teacher outputs on both predicted and non-predicted classes. We presentexperiments with students of various capacities, focusing on the under-exploredcase where students overpower teachers. Our experiments show significantadvantages from transferring knowledge between DenseNets and ResNets in eitherdirection.	Machine Learning (stat.ML)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG)
Kenji_Sugisaki	杉﨑 研司 (Kenji Sugisaki)	360	127	Computational Chemistry on Quantum Computers: Ground state estimation https://t.co/Zd6oEaMG5I あとで読む	2019/7/2 13:16	https://arxiv.org/abs/1907.00362	Computational Chemistry on Quantum Computers: Ground state estimation	We present computational chemistry data for small molecules ($CO$, $HCl$,$F_2$, $NH_4^+$, $CH_4$, $NH_{3}$, $H_3O^+$, $H{_2}O$, $BeH_{2}$, $LiH$,$OH^-$, $HF$, $HeH^+$, $H_2$), obtained by implementing the Unitary CoupledCluster method with Single and Double excitations (UCCSD) on a quantum computersimulator. We have used the Variational Quantum Eigensolver (VQE) algorithm toextract the ground state energies of these molecules. This energy datarepresents the expected ground state energy that a quantum computer willproduce for the given molecules, on the STO-3G basis. Since there is a lot ofinterest in the implementation of UCCSD on quantum computers, we hope that ourwork will serve as a benchmark for future experimental implementations.	Quantum Physics (quant-ph)	Materials Science (cond-mat.mtrl-sci)
hillbig	Daisuke Okanohara	15,910	254	画像に対する敵対的摂動で人間に差がわからない変化で予測ラベルが変わるのは、ノイズや過学習のせいではなく、人間には認識できない汎化可能な特徴を捉えているためであり、実際得られた特徴を使った分類は他のデータセットにも汎化する。https://t.co/1YdGJ02BSe	2019/7/2 13:19	https://arxiv.org/abs/1905.02175	Adversarial Examples Are Not Bugs, They Are Features	Adversarial examples have attracted significant attention in machinelearning, but the reasons for their existence and pervasiveness remain unclear.We demonstrate that adversarial examples can be directly attributed to thepresence of non-robust features: features derived from patterns in the datadistribution that are highly predictive, yet brittle and incomprehensible tohumans. After capturing these features within a theoretical framework, weestablish their widespread existence in standard datasets. Finally, we presenta simple setting where we can rigorously tie the phenomena we observe inpractice to a misalignment between the (human-specified) notion of robustnessand the inherent geometry of the data.	Machine Learning (stat.ML)	Cryptography and Security (cs.CR);Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG)
esXFdfOJxiGBFLx	人工知能 Deep Learning AI image medical machine learni	858	1,901	Adversarial Exampleに対してモデルではなく、データセットで対策を行うという考えのarXivです。Adversarial Exampleは専門ではなかったけどこれは面白いと思った。ロバストと非ロバストは面白いな。 https://t.co/iHBDbYzxTo	2019/7/2 15:22	https://arxiv.org/abs/1905.02175	Adversarial Examples Are Not Bugs, They Are Features	Adversarial examples have attracted significant attention in machinelearning, but the reasons for their existence and pervasiveness remain unclear.We demonstrate that adversarial examples can be directly attributed to thepresence of non-robust features: features derived from patterns in the datadistribution that are highly predictive, yet brittle and incomprehensible tohumans. After capturing these features within a theoretical framework, weestablish their widespread existence in standard datasets. Finally, we presenta simple setting where we can rigorously tie the phenomena we observe inpractice to a misalignment between the (human-specified) notion of robustnessand the inherent geometry of the data.	Machine Learning (stat.ML)	Cryptography and Security (cs.CR);Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG)
akashi_akatsuki	明石暁	426	593	Recent Trends in Deep Learning Based Natural Language Processing https://t.co/U9lvS8LhWg  2017年のだけど読むか…	2019/7/2 15:56	https://arxiv.org/abs/1708.02709	Recent Trends in Deep Learning Based Natural Language Processing	Deep learning methods employ multiple processing layers to learn hierarchicalrepresentations of data and have produced state-of-the-art results in manydomains. Recently, a variety of model designs and methods have blossomed in thecontext of natural language processing (NLP). In this paper, we reviewsignificant deep learning related models and methods that have been employedfor numerous NLP tasks and provide a walk-through of their evolution. We alsosummarize, compare and contrast the various models and put forward a detailedunderstanding of the past, present and future of deep learning in NLP.	Computation and Language (cs.CL)	
hobbymath2020	hobbymath	81	55	新しいプレプリントをアップしました。 https://t.co/fZ4WciPrEI  カルバックライブラーダイバージェンスの下限が、分布の平均値と分散で決まることを示したものです。 ベルヌーイ分布に対しては、等号が成立します。	2019/7/2 16:27	https://arxiv.org/abs/1907.00288	A New Lower Bound for Kullback-Leibler Divergence Based on Hammersley-Chapman-Robbins Bound	In this paper, we derive a useful lower bound for the Kullback-Leiblerdivergence (KL-divergence) based on the Hammersley-Chapman-Robbins bound(HCRB). The HCRB states that the variance of an estimator is bounded from belowby the Chi-square divergence and the expectation value of the estimator. Byusing the relation between the KL-divergence and the Chi-square divergence, weshow that the lower bound for the KL-divergence which only depends on theexpectation value and the variance of a function we choose. We show that theequality holds for the Bernoulli distributions and show that the inequalityconverges to the Cramér-Rao bound when two distributions are very close.Furthermore, we describe application examples and examples of numericalcalculation.	Statistics Theory (math.ST)	Information Theory (cs.IT);Machine Learning (stat.ML)
atushiTAKEDA	武田敦志	77	36	@chokudai 競プロ界隈での著作権は把握していないのですが、情報処理学会だと、著作者の権利（第５条）として規定してます。 https://t.co/Ud1tD1Hx2K  arxivだと、arxvi側の権利を規定しています。 https://t.co/eSiS1ctkWO  こんな感じで、著作権を持たない側の権利を明記した方がいいかなと思います。	2019/7/2 18:32					
Q7Xf5U4TpGCMDjG	ほ(研究)	0	0	https://t.co/6dgO3MVqOE  https://t.co/QT5Zqs9neq アイスホッケーでの行動認識を行う研究 Keypoint+Optical Flowを用いることで従来手法よりも精度が向上している。 keypointとして関節点の他にアイスホッケーのスティックの始点、終点を加えている。 https://t.co/siSjNhTKB0	2019/7/2 18:51	https://arxiv.org/abs/1812.09533	Temporal Hockey Action Recognition via Pose and Optical Flows	Recognizing actions in ice hockey using computer vision poses challenges dueto bulky equipment and inadequate image quality. A novel two-stream frameworkhas been designed to improve action recognition accuracy for hockey using threemain components. First, pose is estimated via the Part Affinity Fields model toextract meaningful cues from the player. Second, optical flow (usingLiteFlowNet) is used to extract temporal features. Third, pose and optical flowstreams are fused and passed to fully-connected layers to estimate the hockeyplayer's action. A novel publicly available dataset named HARPET (Hockey ActionRecognition Pose Estimation, Temporal) was created, composed of sequences ofannotated actions and pose of hockey players including their hockey sticks asan extension of human body pose. Three contributions are recognized. (1) Thenovel two-stream architecture achieves 85% action recognition accuracy, withthe inclusion of optical flows increasing accuracy by about 10%. (2) The uniquelocalization of hand-held objects (e.g., hockey sticks) as part of poseincreases accuracy by about 13%. (3) For pose estimation, a bigger and moregeneral dataset, MSCOCO, is successfully used for transfer learning to asmaller and more specific dataset, HARPET, achieving a PCKh of 87%.	Computer Vision and Pattern Recognition (cs.CV)	
Pechod_dynol	sepia	618	644	https://t.co/dYymFxo5rh Scheme representation for first-order logic   これのこと？	2019/7/2 19:11	https://arxiv.org/abs/1402.2600	Scheme representation for first-order logic	"Although contemporary model theory has been called ""algebraic geometry minusfields"", the formal methods of the two fields are radically different. Thisdissertation aims to shrink that gap by presenting a theory of logical schemes,geometric entities which relate to first-order logical theories in much thesame way that algebraic schemes relate to commutative rings.The construction relies on a Grothendieck-style representation theorem whichassociates every coherent or classical first-order theory with an affinescheme: a topological groupoid (the spectrum of the theory) together with asheaf of (local) syntactic categories. The groupoid is constructed from thesemantics of the theory (models and isomorphisms) and topologized using aStone-type construction. The sheaf of categories can be regarded as a logicaltheory varying over the spectrum, and its global sections recover the theory upto semantic equivalence. These affine pieces can be glued together to give moregeneral logical schemes and these are studied using methods from algebraicgeometry. The final chapter also presents some connections between schemes andother areas of logic such as model theory, type theory and topos theory."	Logic (math.LO)	Algebraic Geometry (math.AG);Category Theory (math.CT)
shiku0304	しく	85	57	平坦な空間じゃないとカーネルが正定値にならないことに気付かなかった僕が馬鹿でした。一週間分の仕事が無駄になりましたが、妙にスッキリしました。 https://t.co/BlDa1OmHbI	2019/7/2 20:37	https://arxiv.org/abs/1411.0296	Geodesic Exponential Kernels: When Curvature and Linearity Conflict	We consider kernel methods on general geodesic metric spaces and provide bothnegative and positive results. First we show that the common Gaussian kernelcan only be generalized to a positive definite kernel on a geodesic metricspace if the space is flat. As a result, for data on a Riemannian manifold, thegeodesic Gaussian kernel is only positive definite if the Riemannian manifoldis Euclidean. This implies that any attempt to design geodesic Gaussian kernelson curved Riemannian manifolds is futile. However, we show that for spaces withconditionally negative definite distances the geodesic Laplacian kernel can begeneralized while retaining positive definiteness. This implies that geodesicLaplacian kernels can be generalized to some curved spaces, including spheresand hyperbolic spaces. Our theoretical results are verified empirically.	Machine Learning (cs.LG)	Computer Vision and Pattern Recognition (cs.CV)
daigo_hirooka	Daigo	97	560	いい表現を得るためにはinductive biasが重要ってICML2019のbest paperでも言ってましたね。 https://t.co/mF8iF9JPdK https://t.co/IE3a1UACOt	2019/7/2 20:42	https://arxiv.org/abs/1811.12359	Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations	The key idea behind the unsupervised learning of disentangled representationsis that real-world data is generated by a few explanatory factors of variationwhich can be recovered by unsupervised learning algorithms. In this paper, weprovide a sober look at recent progress in the field and challenge some commonassumptions. We first theoretically show that the unsupervised learning ofdisentangled representations is fundamentally impossible without inductivebiases on both the models and the data. Then, we train more than 12000 modelscovering most prominent methods and evaluation metrics in a reproduciblelarge-scale experimental study on seven different data sets. We observe thatwhile the different methods successfully enforce properties ``encouraged'' bythe corresponding losses, well-disentangled models seemingly cannot beidentified without supervision. Furthermore, increased disentanglement does notseem to lead to a decreased sample complexity of learning for downstream tasks.Our results suggest that future work on disentanglement learning should beexplicit about the role of inductive biases and (implicit) supervision,investigate concrete benefits of enforcing disentanglement of the learnedrepresentations, and consider a reproducible experimental setup coveringseveral data sets.	Machine Learning (cs.LG)	Artificial Intelligence (cs.AI);Machine Learning (stat.ML)
allowfirm	allowfirm	12	37	@shion_honda はじめまして。式10.41のハッチング箇所は「LSTM: A Search Space Odyssey」( https://t.co/p0y5PsKuxV )ですと、p.2の「g」に対応すると思いますが、たしかに図1でもgは「usually tanh」と記載されていました。式10.42の方がシグモイドですから、その相方はtanhが一般的なように思われます。	2019/7/2 21:52	https://arxiv.org/abs/1503.04069	LSTM: A Search Space Odyssey	Several variants of the Long Short-Term Memory (LSTM) architecture forrecurrent neural networks have been proposed since its inception in 1995. Inrecent years, these networks have become the state-of-the-art models for avariety of machine learning problems. This has led to a renewed interest inunderstanding the role and utility of various computational components oftypical LSTM variants. In this paper, we present the first large-scale analysisof eight LSTM variants on three representative tasks: speech recognition,handwriting recognition, and polyphonic music modeling. The hyperparameters ofall LSTM variants for each task were optimized separately using random search,and their importance was assessed using the powerful fANOVA framework. Intotal, we summarize the results of 5400 experimental runs ($\approx 15$ yearsof CPU time), which makes our study the largest of its kind on LSTM networks.Our results show that none of the variants can improve upon the standard LSTMarchitecture significantly, and demonstrate the forget gate and the outputactivation function to be its most critical components. We further observe thatthe studied hyperparameters are virtually independent and derive guidelines fortheir efficient adjustment.	Neural and Evolutionary Computing (cs.NE)	Machine Learning (cs.LG)
MomonariKudo	工藤 桃成 Momonari Kudo	237	186	"論文""Algorithmic study of superspecial hyperelliptic curves over finite fields"" (with Shushi Harashita)をarXivに公開しました。 https://t.co/9cDdmqX05F 一般種数の超楕円曲線のうちsuperspecialなものを数え上げるアルゴリズムと、超楕円曲線の自己同型群を計算するアルゴリズムを提案。"	2019/7/2 22:08	https://arxiv.org/abs/1907.00894	Algorithmic study of superspecial hyperelliptic curves over finite fields	This paper presents algorithmic approaches to study superspecialhyperelliptic curves. The algorithms proposed in this paper are: an algorithmto enumerate superspecial hyperelliptic curves of genus $g$ over finite fields$\mathbb{F}_q$, and an algorithm to compute the automorphism group of a (notnecessarily superspecial) hyperelliptic curve over finite fields. The firstalgorithm works for any $(g,q)$ such that $q$ and $2g+2$ are coprime and$q>2g+1$. As an application, we enumerate superspecial hyperelliptic curves ofgenus $g=4$ over $\mathbb{F}_{p}$ for $11 \leq p \leq 23$ and over$\mathbb{F}_{p^2}$ for $11 \leq p \leq 19$ with our implementation on acomputer algebra system Magma. Moreover, we found maximal hyperelliptic curvesand minimal hyperelliptic curves over $\mathbb{F}_{p^2}$ from among enumeratedsuperspecial ones. The second algorithm computes an automorphism as a concreteelement in (a quotient of) a linear group in the general linear group of degree$2$.	Algebraic Geometry (math.AG)	Number Theory (math.NT)
re_hako_moon	はこつき＠VR	43	49	https://t.co/NKkjbUZ5e1 教師なし学習で三次元特徴点の検出器を学習。点群データセット中のモデルをランダムに剛体変換し、その剛体変換前後で変わらず検出できるような特徴点を検出する。一度局所点群から特徴点を計算し、改めて特徴点同士の相対位置を考慮して最終的な特徴点と信頼度を出力する。	2019/7/2 22:12	https://arxiv.org/abs/1904.00229	USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds	In this paper, we propose the USIP detector: an Unsupervised Stable InterestPoint detector that can detect highly repeatable and accurately localizedkeypoints from 3D point clouds under arbitrary transformations without the needfor any ground truth training data. Our USIP detector consists of a featureproposal network that learns stable keypoints from input 3D point clouds andtheir respective transformed pairs from randomly generated transformations. Weprovide degeneracy analysis of our USIP detector and suggest solutions toprevent it. We encourage high repeatability and accurate localization of thekeypoints with a probabilistic chamfer loss that minimizes the distancesbetween the detected keypoints from the training point cloud pairs. Extensiveexperimental results of repeatability tests on several simulated and real-world3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIPdetector significantly outperforms existing hand-crafted and deeplearning-based 3D keypoint detectors. Our code is available at the projectwebsite. this https URL	Computer Vision and Pattern Recognition (cs.CV)	
akihiro_akichan	akihiro_f	122	118	https://t.co/hWzbQImOKV 画像中の重要人物を抽出するPOINTを提案。画像中の人物を全て抜き出した後にそれぞれの顔、周囲、位置の３つをまとめてCNNで特徴量化し、関係性を計算するRelation Moduleがコアか。Self-AttentionのようにQKVで関係性を取得するが３項の積でなくQV＋KVの定式化になっている。 https://t.co/5aezltGCqX	2019/7/2 22:22	https://arxiv.org/abs/1904.03632	Learning to Learn Relation for Important People Detection in Still Images	Humans can easily recognize the importance of people in social event images,and they always focus on the most important individuals. However, learning tolearn the relation between people in an image, and inferring the most importantperson based on this relation, remains undeveloped. In this work, we propose adeep imPOrtance relatIon NeTwork (POINT) that combines both relation modelingand feature learning. In particular, we infer two types of interaction modules:the person-person interaction module that learns the interaction between peopleand the event-person interaction module that learns to describe how a person isinvolved in the event occurring in an image. We then estimate the importancerelations among people from both interactions and encode the relation featurefrom the importance relations. In this way, POINT automatically learns severaltypes of relation features in parallel, and we aggregate these relationfeatures and the person's feature to form the importance feature for importantpeople classification. Extensive experimental results show that our method iseffective for important people detection and verify the efficacy of learning tolearn relations for important people detection.	Computer Vision and Pattern Recognition (cs.CV)	
atsushieeeee	Atsushi Tabata	74	71	#ImageNet 単体だと、テクスチャの要因が強くなるが、style transferしたデータセットを合わせて学習することで、物体の特徴として形状も重視させることが可能に。よりロバストなモデルが作れる。https://t.co/SZKw1oea6P	2019/7/2 22:30	https://arxiv.org/abs/1811.12231	ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness	"Convolutional Neural Networks (CNNs) are commonly thought to recogniseobjects by learning increasingly complex representations of object shapes. Somerecent studies suggest a more important role of image textures. We here putthese conflicting hypotheses to a quantitative test by evaluating CNNs andhuman observers on images with a texture-shape cue conflict. We show thatImageNet-trained CNNs are strongly biased towards recognising textures ratherthan shapes, which is in stark contrast to human behavioural evidence andreveals fundamentally different classification strategies. We then demonstratethat the same standard architecture (ResNet-50) that learns a texture-basedrepresentation on ImageNet is able to learn a shape-based representationinstead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.This provides a much better fit for human behavioural performance in ourwell-controlled psychophysical lab setting (nine experiments totalling 48,560psychophysical trials across 97 observers) and comes with a number ofunexpected emergent benefits such as improved object detection performance andpreviously unseen robustness towards a wide range of image distortions,highlighting advantages of a shape-based representation."	Computer Vision and Pattern Recognition (cs.CV)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG);Neurons and Cognition (q-bio.NC);Machine Learning (stat.ML)
shunk031	しゅんけー	1,701	667	DropConnect、お前もベイジアンニューラルネットだったんだな… https://t.co/cEEyDfcE1u	2019/7/2 23:10	https://arxiv.org/abs/1906.04569	DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks	Deep neural networks (DNNs) have achieved state-of-the-art performances inmany important domains, including medical diagnosis, security, and autonomousdriving. In these domains where safety is highly critical, an erroneousdecision can result in serious consequences. While a perfect predictionaccuracy is not always achievable, recent work on Bayesian deep networks showsthat it is possible to know when DNNs are more likely to make mistakes. Knowingwhat DNNs do not know is desirable to increase the safety of deep learningtechnology in sensitive applications. Bayesian neural networks attempt toaddress this challenge. However, traditional approaches are computationallyintractable and do not scale well to large, complex neural networkarchitectures. In this paper, we develop a theoretical framework to approximateBayesian inference for DNNs by imposing a Bernoulli distribution on the modelweights. This method, called MC-DropConnect, gives us a tool to represent themodel uncertainty with little change in the overall model structure orcomputational cost. We extensively validate the proposed algorithm on multiplenetwork architectures and datasets for classification and semantic segmentationtasks. We also propose new metrics to quantify the uncertainty estimates. Thisenables an objective comparison between MC-DropConnect and prior approaches.Our empirical results demonstrate that the proposed framework yieldssignificant improvement in both prediction accuracy and uncertainty estimationquality compared to the state of the art.	Machine Learning (cs.LG)	Artificial Intelligence (cs.AI);Computer Vision and Pattern Recognition (cs.CV);Machine Learning (stat.ML)
KSKSKSKS2	katsugeneration	177	444	画像からキャプションを生成するタスクで、画像から複数のPOSタグのシーケンスを予測し、その情報をもとにキャプションを生成することにより、多様で精度の高いキャプションを既存手法より高速に生成する手法を提案した。 https://t.co/8wyBRnB3wt	2019/7/2 23:36	https://arxiv.org/abs/1805.12589	Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech	Image captioning is an ambiguous problem, with many suitable captions for animage. To address ambiguity, beam search is the de facto method for samplingmultiple captions. However, beam search is computationally expensive and knownto produce generic captions. To address this concern, some variationalauto-encoder (VAE) and generative adversarial net (GAN) based methods have beenproposed. Though diverse, GAN and VAE are less accurate. In this paper, wefirst predict a meaningful summary of the image, then generate the captionbased on that summary. We use part-of-speech as summaries, since our summaryshould drive caption generation. We achieve the trifecta: (1) High accuracy forthe diverse captions as evaluated by standard captioning metrics and userstudies; (2) Faster computation of diverse captions compared to beam search anddiverse beam search; and (3) High diversity as evaluated by counting novelsentences, distinct n-grams and mutual overlap (i.e., mBleu-4) scores.	Computer Vision and Pattern Recognition (cs.CV)	
shion_honda	Shion Honda	1,234	242	Triangulation Learning Network [Qin+, 2019, CVPR] 2Dステレオ画像を活用して3Dで物体検出を行うという研究。深さ情報を使わず、2枚の画像の差分から三角測量のように物体のアンカーの位置を推定するTLNetを提案。channel reweightingも提案。KITTIでSOTA。 https://t.co/75UuE3wqrb #NowReading https://t.co/aUe2WwgAuj	2019/7/3 0:44	https://arxiv.org/abs/1906.01193	Triangulation Learning Network: from Monocular to Stereo 3D Object Detection	In this paper, we study the problem of 3D object detection from stereoimages, in which the key challenge is how to effectively utilize stereoinformation. Different from previous methods using pixel-level depth maps, wepropose employing 3D anchors to explicitly construct object-levelcorrespondences between the regions of interest in stereo images, from whichthe deep neural network learns to detect and triangulate the targeted object in3D space. We also introduce a cost-efficient channel reweighting strategy thatenhances representational features and weakens noisy signals to facilitate thelearning process. All of these are flexibly integrated into a solid baselinedetector that uses monocular images. We demonstrate that both the monocularbaseline and the stereo triangulation learning network outperform the priorstate-of-the-arts in 3D object detection and localization on the challengingKITTI dataset.	Computer Vision and Pattern Recognition (cs.CV)	
daikinish	Daiki Nishiguchi	1,562	833	自分の論文が2つも、各々から図を引用されつつレビュー論文に出て来ると嬉しい…！ https://t.co/dy0GMmaJvV	2019/7/3 1:41	https://arxiv.org/abs/1907.00360	Self-Propelled Rods: Insights and Perspectives for Active Matter	A wide range of experimental systems including gliding, swarming and swimmingbacteria, in-vitro motility assays as well as shaken granular media arecommonly described as self-propelled rods. Large ensembles of those entitiesdisplay a large variety of self-organized, collective phenomena, includingformation of moving polar clusters, polar and nematic dynamic bands,mobility-induced phase separation, topological defects and mesoscaleturbulence, among others. Here, we give a brief survey of experimentalobservations and review the theoretical description of self-propelled rods. Ourfocus is on the emergent pattern formation of ensembles of dry self-propelledrods governed by short-ranged, contact mediated interactions and their wetcounterparts that are also subject to long-ranged hydrodynamic flows.Altogether, self-propelled rods provide an overarching theme covering manyaspects of active matter containing well-explored limiting cases. Theircollective behavior not only bridges the well-studied regimes of polarself-propelled particles and active nematics, and includes active phaseseparation, but also reveals a rich variety of new patterns.	Statistical Mechanics (cond-mat.stat-mech)	Soft Condensed Matter (cond-mat.soft);Biological Physics (physics.bio-ph)
MikasorFlow	A$AP MIKA$A	42	117	Deterministic generation of a two-dimensional cluster state for universal quantum computing  これです  https://t.co/6PNE0gEafM	2019/7/3 2:29	https://arxiv.org/abs/1906.08709	Deterministic generation of a two-dimensional cluster state for universal quantum computing	Measurement-based quantum computation offers exponential computationalspeed-up via simple measurements on a large entangled cluster state. We proposeand demonstrate a scalable scheme for the generation of photonic cluster statessuitable for universal measurement-based quantum computation. We exploittemporal multiplexing of squeezed light modes, delay loops, and beam-splittertransformations to deterministically generate a cylindrical cluster state witha two-dimensional (2D) topological structure as required for universal quantuminformation processing. The generated state consists of more than 30000entangled modes arranged in a cylindrical lattice with 24 modes on thecircumference, defining the input register, and a length of 1250 modes,defining the computation depth. Our demonstrated source of 2D cluster statescan be combined with quantum error correction based on theGottesman-Kitaev-Preskill qubit encoding to enable fault-tolerant quantumcomputation.	Quantum Physics (quant-ph)	Optics (physics.optics)
norarenmei1	おやー	36	428	数学の話でたので、久しぶりにポアンカレ予想証明論文にてをつけてる https://t.co/oaK9TElJ6m	2019/7/3 2:39					
kimar_iidx	未来に虹を架けよう	293	282	https://t.co/7LQ58MbWBB よくわからないまま読んでるんだけど引用が最近のばっかりでホットなんだなあと思っている	2019/7/3 4:19	https://arxiv.org/abs/1810.07258	Late-Time Observations of Type Ia Supernova SN 2014J with the Hubble Space Telescope Wide Field Camera 3	Recent works have studied the late-time light curves of Type Ia supernovae(SNe Ia) when these were older than 500 days past B-band maximum light. Ofthese, SN 2014J, which exploded in the nearby galaxy M82, was studied with theAdvanced Camera for Surveys onboard the Hubble Space Telescope (HST) by Yang etal. Here, I report complementary photometry of SN 2014J taken with the HST WideField Camera 3 when it was ~360-1300 days old. My F555W measurements areconsistent with the F606W measurements of Yang et al., but the F438Wmeasurements are ~1 mag fainter than their F475W measurements. I corroboratetheir finding that even though SN 2014J has spatially resolved light echoes,its photometry is not contaminated by an unresolved echo. Finally, I comparethe F438W and F555W light curves of SN 2014J to those of the other late-timeSNe Ia observed to date and show that more intrinsically luminous SNe haveslower light-curve decline rates. This is consistent with the correlationclaimed by Graur et al., which was based on a comparison of pseudo-bolometriclight curves. By conducting a direct comparison of the late-time light curvesin the same filters, I remove any systematic uncertainties introduced by theassumptions that go into constructing the pseudo-bolometric light curves, thusstrengthening the Graur et al. claim.	High Energy Astrophysical Phenomena (astro-ph.HE)	Cosmology and Nongalactic Astrophysics (astro-ph.CO)
iBotamon	iBotamon???5 CVPR???	606	1,861	この論文は既製のEmbeddingを使わずにゼロベースでコーパスから構築している。OOV問題に対処しようとした？ https://t.co/0jvldDZtF8	2019/7/3 6:28	https://arxiv.org/abs/1708.09254	Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural Networks - Preliminary Results	Radiology reports are an important means of communication betweenradiologists and other physicians. These reports express a radiologist'sinterpretation of a medical imaging examination and are critical inestablishing a diagnosis and formulating a treatment plan. In this paper, wepropose a Bi-directional convolutional neural network (Bi-CNN) model for theinterpretation and classification of mammograms based on breast density andchest radiographic radiology reports based on the basis of chest pathology. Theproposed approach helps to organize databases of radiology reports, retrievethem expeditiously, and evaluate the radiology report that could be used in anauditing system to decrease incorrect diagnoses. Our study revealed that theproposed Bi-CNN outperforms the random forest and the support vector machinemethods.	Computer Vision and Pattern Recognition (cs.CV)	
iBotamon	iBotamon???5 CVPR???	606	1,861	これも8万件のレポートからword2vecをゼロベースで構築している。このNeural-Attention Modelは上手く理解できていないけれどself-attentionっぽい？ https://t.co/Ij4vrTJ4PL	2019/7/3 7:38	https://arxiv.org/abs/1708.06828	Classification of Radiology Reports Using Neural Attention Models	"The electronic health record (EHR) contains a large amount ofmulti-dimensional and unstructured clinical data of significant operational andresearch value. Distinguished from previous studies, our approach embraces adouble-annotated dataset and strays away from obscure ""black-box"" models tocomprehensive deep learning models. In this paper, we present a novel neuralattention mechanism that not only classifies clinically important findings.Specifically, convolutional neural networks (CNN) with attention analysis areused to classify radiology head computed tomography reports based on fivecategories that radiologists would account for in assessing acute andcommunicable findings in daily practice. The experiments show that our CNNattention models outperform non-neural models, especially when trained on alarger dataset. Our attention analysis demonstrates the intuition behind theclassifier's decision by generating a heatmap that highlights attended termsused by the CNN model; this is valuable when potential downstream medicaldecisions are to be performed by human experts or the classifier information isto be used in cohort construction such as for epidemiological studies."	Computation and Language (cs.CL)	Artificial Intelligence (cs.AI);Information Retrieval (cs.IR)
barinoriron	てんにょ	2,095	301	またも FRB の母銀河特定。星形成率がとても低い銀河とのことで、発生源は old stellar population ではないかとのこと。  A fast radio burst localised to a massive galaxy　https://t.co/S7ikI0gg7O	2019/7/3 10:10	https://arxiv.org/abs/1907.01542	A fast radio burst localised to a massive galaxy	Intense, millisecond-duration bursts of radio waves have been detected frombeyond the Milky Way [1]. Their extragalactic origins are evidenced by theirlarge dispersion measures, which are greater than expected for propagationthrough the Milky Way interstellar medium alone, and imply contributions fromthe intergalactic medium and potentially host galaxies [2]. Although severaltheories exist for the sources of these fast radio bursts, their intensities,durations and temporal structures suggest coherent emission from highlymagnetised plasma [3,4]. Two sources have been observed to repeat [5,6], andone repeater (FRB 121102) has been localised to the largest star-forming regionof a dwarf galaxy at a cosmological redshift of 0.19 [7, 8]. However, the hostgalaxies and distances of the so far non-repeating fast radio bursts are yet tobe identified. Unlike repeating sources, these events must be observed with aninterferometer with sufficient spatial resolution for arcsecond localisation atthe time of discovery. Here we report the localisation of a fast radio burst(FRB 190523) to a few-arcsecond region containing a single massive galaxy at aredshift of 0.66. This galaxy is in stark contrast to the host of FRB 121102,being a thousand times more massive, with a greater than hundred times lowerspecific star-formation rate. The properties of this galaxy highlight thepossibility of a channel for FRB production associated with older stellarpopulations.	High Energy Astrophysical Phenomena (astro-ph.HE)	Instrumentation and Methods for Astrophysics (astro-ph.IM)
monizuka	GPO: makoto onizuka	642	533	Graph U-Nets: graph pooling においてグラフそのものを縮退させるという大技を導入． https://t.co/JAwyqlGK6P	2019/7/3 11:17	https://arxiv.org/abs/1905.05178	Graph U-Nets	We consider the problem of representation learning for graph data.Convolutional neural networks can naturally operate on images, but havesignificant challenges in dealing with graph data. Given images are specialcases of graphs with nodes lie on 2D lattices, graph embedding tasks have anatural correspondence with image pixel-wise prediction tasks such assegmentation. While encoder-decoder architectures like U-Nets have beensuccessfully applied on many image pixel-wise prediction tasks, similar methodsare lacking for graph data. This is due to the fact that pooling andup-sampling operations are not natural on graph data. To address thesechallenges, we propose novel graph pooling (gPool) and unpooling (gUnpool)operations in this work. The gPool layer adaptively selects some nodes to forma smaller graph based on their scalar projection values on a trainableprojection vector. We further propose the gUnpool layer as the inverseoperation of the gPool layer. The gUnpool layer restores the graph into itsoriginal structure using the position information of nodes selected in thecorresponding gPool layer. Based on our proposed gPool and gUnpool layers, wedevelop an encoder-decoder model on graph, known as the graph U-Nets. Ourexperimental results on node classification and graph classification tasksdemonstrate that our methods achieve consistently better performance thanprevious models.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
masahiro_sakai	Masahiro Sakai	1,787	1,333	Principled Deep Neural Network Training through Linear Programming https://t.co/MciqNOXhEi NNの学習(経験損失の最小化)を、2進近似の上で木分解を使って指数的サイズのLPに帰着。直接実用的には使えなさそうで、理論的な洞察に繋がると良いと思うのだけど、自分はあまり理解できてない………	2019/7/3 11:49	https://arxiv.org/abs/1810.03218	Principled Deep Neural Network Training through Linear Programming	Deep Learning has received significant attention due to its impressiveperformance in many state-of-the-art learning tasks. Unfortunately, while verypowerful, Deep Learning is not well understood theoretically and in particularonly recently results for the complexity of training deep neural networks havebeen obtained. In this work we show that large classes of deep neural networkswith various architectures (e.g., DNNs, CNNs, Binary Neural Networks, andResNets), activation functions (e.g., ReLUs and leaky ReLUs), and lossfunctions (e.g., Hinge loss, Euclidean loss, etc) can be trained to nearoptimality with desired target accuracy using linear programming in time thatis exponential in the input data and parameter space dimension and polynomialin the size of the data set; improvements of the dependence in the inputdimension are known to be unlikely assuming $P\neq NP$, and improving thedependence on the parameter space dimension remains open. In particular, weobtain polynomial time algorithms for training for a given fixed networkarchitecture. Our work applies more broadly to empirical risk minimizationproblems which allows us to generalize various previous results and obtain newcomplexity results for previously unstudied architectures in the properlearning setting.	Machine Learning (cs.LG)	Optimization and Control (math.OC);Machine Learning (stat.ML)
Soliton111	Soliton	78	96	half-BPS op.の4点関数において、特別なR-charge polarizationかつ各BPS状態が非常に重い極限をとれば、4点関数の計算は八角形ダイアグラムの二乗に比例した形でかけるらしい。 で、可積分系テクニックをつかえば、all loopの計算が可能とのこと。  https://t.co/TsADi5yrat https://t.co/WKRsKv4Deu	2019/7/3 12:01	https://arxiv.org/abs/1903.05038, https://arxiv.org/abs/1905.11467	Determinant formula for the octagon form factor in $\mathcal{N}$=4 SYM, The Octagon as a Determinant	We compute to all loop orders correlation function of four heavy BPSoperators in $\mathcal{N}$= 4 SYM with special polarisations consideredrecently by Frank Coronado. Our main result is an expression for the octagonform factor as determinant of a semi-infinite matrix. We find that at weakcoupling the entries of this matrix are linear combinations of ladder functionswith simple rational coefficients and give the full perturbative expansion ofthe octagon., The computation of a certain class of four-point functions of heavily chargedBPS operators boils down to the computation of a special form factor - theoctagon. In this paper, which is an extended version of the short note [1], wederive a non-perturbative formula for the square of the octagon as thedeterminant of a semi-infinite skew-symmetric matrix. We show thatperturbatively in the weak coupling limit the octagon is given by a determinantconstructed from the polylogarithms evaluating ladder Feynman graphs. We alsogive a simple operator representation of the octagon in terms of a vacuumexpectation value of massless free bosons or fermions living in the rapidityplane.	High Energy Physics - Theory (hep-th), High Energy Physics - Theory (hep-th)	
yu4u	Yusuke Uchida	4,447	950	なんかすごいでかい（語彙）カメラ画像からのOCRのためのデータセット (ICDAR 2019) https://t.co/ouadk3eKGj / Brno Mobile OCR Dataset https://t.co/QpvuoNRLKB https://t.co/XYQJkfZgSn	2019/7/3 12:07	https://arxiv.org/abs/1907.01307	Brno Mobile OCR Dataset	We introduce the Brno Mobile OCR Dataset (B-MOD) for document OpticalCharacter Recognition from low-quality images captured by handheld mobiledevices. While OCR of high-quality scanned documents is a mature field wheremany commercial tools are available, and large datasets of text in the wildexist, no existing datasets can be used to develop and test document OCRmethods robust to non-uniform lighting, image blur, strong noise, built-indenoising, sharpening, compression and other artifacts present in manyphotographs from mobile devices.This dataset contains 2 113 unique pages from random scientific papers, whichwere photographed by multiple people using 23 different mobile devices. Theresulting 19 728 photographs of various visual quality are accompanied byprecise positions and text annotations of 500k text lines. We further providean evaluation methodology, including an evaluation server and a testset withnon-public annotations.We provide a state-of-the-art text recognition baseline build onconvolutional and recurrent neural networks trained with Connectionist TemporalClassification loss. This baseline achieves 2 %, 22 % and 73 % word error rateson easy, medium and hard parts of the dataset, respectively, confirming thatthe dataset is challenging.The presented dataset will enable future development and evaluation ofdocument analysis for low-quality images. It is primarily intended forline-level text recognition, and can be further used for line localization,layout analysis, image restoration and text binarization.	Computer Vision and Pattern Recognition (cs.CV)	
esXFdfOJxiGBFLx	人工知能 Deep Learning AI image medical machine learni	858	1,901	マルチGANによるパッチベースでの高解像度医用画像の生成に関するarXivです。 生成に関してはパッチにして計算コストの大きい3DCTや胸部X戦に対応させて、マルチにすることで性能を向上させつつってことだと思うけど、その画像をどう扱うべきかは医師は困りそうではある。 https://t.co/cGgBUpB9gr	2019/7/3 12:33	https://arxiv.org/abs/1907.01376	Multi-scale GANs for Memory-efficient Generation of High Resolution Medical Images	Currently generative adversarial networks (GANs) are rarely applied tomedical images of large sizes, especially 3D volumes, due to their largecomputational demand. We propose a novel multi-scale patch-based GAN approachto generate large high resolution 2D and 3D images. Our key idea is to firstlearn a low-resolution version of the image and then generate patches ofsuccessively growing resolutions conditioned on previous scales. In a domaintranslation use-case scenario, 3D thorax CTs of size 512x512x512 and thoraxX-rays of size 2048x2048 are generated and we show that, due to the constantGPU memory demand of our method, arbitrarily large images of high resolutioncan be generated. Moreover, compared to common patch-based approaches, ourmulti-resolution scheme enables better image quality and prevents patchartifacts.	Image and Video Processing (eess.IV)	Computer Vision and Pattern Recognition (cs.CV)
hiropon_matsu	HIROPON	88	412	"Bengio Prigogineなんかも引かれている。 ""Learning the Arrow of Time."" https://t.co/YaJ8KQ9jOG"	2019/7/3 13:05	https://arxiv.org/abs/1907.01285	Learning the Arrow of Time	We humans seem to have an innate understanding of the asymmetric progressionof time, which we use to efficiently and safely perceive and manipulate ourenvironment. Drawing inspiration from that, we address the problem of learningan arrow of time in a Markov (Decision) Process. We illustrate how a learnedarrow of time can capture meaningful information about the environment, whichin turn can be used to measure reachability, detect side-effects and to obtainan intrinsic reward signal. We show empirical results on a selection ofdiscrete and continuous environments, and demonstrate for a class of stochasticprocesses that the learned arrow of time agrees reasonably well with a knownnotion of an arrow of time given by the celebrated Jordan-Kinderlehrer-Ottoresult.	Machine Learning (cs.LG)	Artificial Intelligence (cs.AI)
Kenji_Sugisaki	杉﨑 研司 (Kenji Sugisaki)	360	127	Symmetry Configuration Mapping for Compact Representation of Quantum Chemistry on Quantum Computers https://t.co/KDRvoIXPCC あとで読む。	2019/7/3 14:19	https://arxiv.org/abs/1907.01493	Symmetry Configuration Mapping for Compact Representation of Quantum Chemistry on Quantum Computers	Near-term quantum computers may be able to significantly speed up complexcomputational tasks, but algorithms that make efficient use of quantumresources are needed. Quantum chemistry is widely regarded as a candidate forthe first demonstration of quantum advantage with near-term quantum computers.In the present work, we demonstrate how taking advantage of the symmetries of amolecule leads to a reduction in the number of qubits required. This reductionin qubits also leads to a reduction in the number of variational parametersneeded to reach chemical accuracy. Furthermore, we show how a simplemodification of the hardware-efficient ansatz for the variational quantumeigensolver yields a factor of 3 reduction in the number of parameters with noloss in accuracy for most problems in quantum chemistry.	Quantum Physics (quant-ph)	
norihitoishida	Norihito Ishida	256	509	https://t.co/IsfZTzv1VF L2正則化はバッチ正規化と一緒にやっても意味なし　覚えました	2019/7/3 14:26	https://arxiv.org/abs/1706.05350	L2 Regularization versus Batch and Weight Normalization	Batch Normalization is a commonly used trick to improve the training of deepneural networks. These neural networks use L2 regularization, also calledweight decay, ostensibly to prevent overfitting. However, we show that L2regularization has no regularizing effect when combined with normalization.Instead, regularization has an influence on the scale of weights, and therebyon the effective learning rate. We investigate this dependence, both in theory,and experimentally. We show that popular optimization methods such as ADAM onlypartially eliminate the influence of normalization on the learning rate. Thisleads to a discussion on other ways to mitigate this issue.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
takasan_san_san	Kazuaki Takasan / 高三 和晃	1,622	2,005	arXivはこちら High-frequency Expansion for Floquet Prethermal Phases with Emergent Symmetries: Application to Time Crystals and Floquet Engineering https://t.co/IUURrEbvM5	2019/7/3 15:39	https://arxiv.org/abs/1902.01126	High-frequency Expansion for Floquet Prethermal Phases with Emergent Symmetries: Application to Time Crystals and Floquet Engineering	Prethermalization, where quasi-steady states are realized in the intermediatelong time regime (prethermal regime), in periodically driven (Floquet) systemsis an important phenomenon since it provides a platform of nontrivial Floquetmany-body physics. In this Letter, we consider Floquet systems with dual energyscales: the Hamiltonian consists of two different terms whose amplitude iseither comparable or much smaller than the frequency. As a result, when thelarger-amplitude drive induces a \mathbb{Z}_N symmetry operation, we obtain theeffective static Hamiltonian respecting a new emergent \mathbb{Z}_N symmetry inhigh frequency expansions, which describes the dynamics of such Floquet systemsin the prethermal regime. As an application of our formulation, we considerprethermal discrete time crystals, in which our formalism gives a general wayto analyze them in the prethermal regime in terms of the static effectiveHamiltonian. We also provide an application to Floquet engineering, with whichwe can perform simultaneous control of phases and symmetries of the systems.This enables us to control symmetry protected topological phases even when theoriginal system does not respect the symmetry.	Mesoscale and Nanoscale Physics (cond-mat.mes-hall)	Other Condensed Matter (cond-mat.other);Statistical Mechanics (cond-mat.stat-mech)
emerson_et_al	えまーそん	224	199	久々に単著出してみました https://t.co/m6sltV5S0m	2019/7/3 17:26	https://arxiv.org/abs/1907.00993	Dynamical Emergence of Scalaron in Higgs Inflation	We point out that a light scalaron dynamically emerges if scalar fields havea sizable non-minimal coupling to the Ricci scalar as in the Higgs inflationmodel. We support this claim in two ways. One is based on the renormalizationgroup equation; the non-minimal coupling inevitably induces a Ricci scalarquadratic term due to the renormalization group running. The other is based onscattering amplitudes; a scalar four-point amplitude develops a pole aftersumming over a certain class of diagrams, which we identify as the scalaron.Our result implies that the Higgs inflation is actually a two-fieldinflationary model. Another implication is that the Higgs inflation does notsuffer from the unitarity issue since the scalaron pushes up the cut-off scaleto the Planck scale.	High Energy Physics - Phenomenology (hep-ph)	Cosmology and Nongalactic Astrophysics (astro-ph.CO);General Relativity and Quantum Cosmology (gr-qc);High Energy Physics - Theory (hep-th)
NH_M_	Nm?m	1,131	427	Lohitsiri, Tong. Hypercharge Quantisation and Fermat's Last Theorem https://t.co/FWLFdrxm6y 素粒子標準模型とフェルマーの最終定理の関係性が見つかったようです。ハイパーチャージ量子化、ゲージアノマリー相殺、そしてフェルマーの最終定理によって、標準模型の電荷を導出できることがわかった	2019/7/3 18:06	https://arxiv.org/abs/1907.00514	Hypercharge Quantisation and Fermat's Last Theorem	What values of the Standard Model hypercharges result in a mathematicallyconsistent quantum field theory? We show that the constraints imposed by thelack of gauge anomalies can be recast as the equation x^3 + y^3 = z^3. Ifhypercharge is quantised, then x, y and z must be integers. The trivial (andonly) solutions, with x=0 or y=0, reproduce the hypercharge assignments seen inNature. This argument does not rely on the mixed gauge-gravitational anomaly,which is automatically vanishing if hypercharge is quantised and the gaugeanomalies vanish.	High Energy Physics - Theory (hep-th)	High Energy Physics - Phenomenology (hep-ph)
hiromichinomata	H. NOMATA	627	979	予測外したときの責任重大感 https://t.co/SoJBAw3bCg	2019/7/3 18:29	https://arxiv.org/abs/1906.11893	HalalNet: A Deep Neural Network that Classifies the Halalness Slaughtered Chicken from their Images	Halal requirement in food is important for millions of Muslims worldwideespecially for meat and chicken products, insuring that slaughter houses adhereto this requirement is a challenging task to do manually. In this paper amethod is proposed that uses a camera that takes images of slaughtered chickenon the conveyor in a slaughter house, the images are then analyzed by a deepneural network to classify if the image is of a halal slaughtered chicken ornot. However, traditional deep learning models require large amounts of data totrain on, which in this case these amounts of data were challenging to collectespecially the images of non-halal slaughtered chicken, hence this paper showshow the use of one shot learning [1] and transfer learning [2] can reach highaccuracy on the few amounts of data that were available. The architecture usedis based on the Siamese neural networks architecture which ranks the similaritybetween two inputs [3] while using the Xception network [4] as the twinnetworks. We call it HalalNet. This work was done as part of SYCUT (syriahcompliant slaughtering system) which is a monitoring system that monitors thehalalness of the slaughtered chicken in a slaughter house. The data used totrain and validate HalalNet was collected from the Azain slaughtering site(Semenyih, Selangor, Malaysia) containing images of both halal and non-halalslaughtered chicken.	Computer Vision and Pattern Recognition (cs.CV)	Image and Video Processing (eess.IV)
phasetr	相転移P	3,355	1,313	@yaschan__ https://t.co/EjqSvfadpv もあります。	2019/7/3 19:26					
Dave50425992	Dave@AQUA	128	516	@TaneoKoyama タンパク質の相性？ 生物的なことはわかんないけど、求まる解はハミルトニアンが1番小さい状態(要はエネルギー的に1番安定な状態)だよね。  他のNP問題の解法どうぞ。 https://t.co/eN4Cf7KuBq	2019/7/3 19:33	https://arxiv.org/abs/1302.5843	Ising formulations of many NP problems	We provide Ising formulations for many NP-complete and NP-hard problems,including all of Karp's 21 NP-complete problems. This collects and extendsmappings to the Ising model from partitioning, covering and satisfiability. Ineach case, the required number of spins is at most cubic in the size of theproblem. This work may be useful in designing adiabatic quantum optimizationalgorithms.	Statistical Mechanics (cond-mat.stat-mech)	Computational Complexity (cs.CC);Data Structures and Algorithms (cs.DS);Quantum Physics (quant-ph)
tenityu	Takayuki Tanabe?	69	119	メモリアロケーターごとの比較ベンチみたいなものってどこで手に入りますかね．．．_mm_malloc の性能が知りたい．今年の SIGMOD ポスターでこれ (https://t.co/PPfECiiQBB ) の発展版は見たんですけど，_mm_malloc が入ってなくて．	2019/7/3 19:38	https://arxiv.org/abs/1905.01135	On the Impact of Memory Allocation on High-Performance Query Processing	Somewhat surprisingly, the behavior of analytical query engines is cruciallyaffected by the dynamic memory allocator used. Memory allocators highlyinfluence performance, scalability, memory efficiency and memory fairness toother processes. In this work, we provide the first comprehensive experimentalanalysis on the impact of memory allocation for high-performance query engines.We test five state-of-the-art dynamic memory allocators and discuss theirstrengths and weaknesses within our DBMS. The right allocator can increase theperformance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.	Databases (cs.DB)	Performance (cs.PF)
shion_honda	Shion Honda	1,234	242	Deep Drug-Target Binding Affinity Prediction [Ozturk+, 2018, Bioinfo.] タンパク質と化合物の文字列表現をそれぞれCNNでencodeし、それらをDNNに入力して両者の結合を予測するDeepDTAを提案。DavisとKIBAで評価し、CNNの表現学習による精度を確認した。 https://t.co/qTi58Aj5fv #NowReading https://t.co/kVxWkurbrq	2019/7/3 21:21	https://arxiv.org/abs/1801.10193	DeepDTA: Deep Drug-Target Binding Affinity Prediction	The identification of novel drug-target (DT) interactions is a substantialpart of the drug discovery process. Most of the computational methods that havebeen proposed to predict DT interactions have focused on binary classification,where the goal is to determine whether a DT pair interacts or not. However,protein-ligand interactions assume a continuum of binding strength values, alsocalled binding affinity and predicting this value still remains a challenge.The increase in the affinity data available in DT knowledge-bases allows theuse of advanced learning techniques such as deep learning architectures in theprediction of binding affinities. In this study, we propose a deep-learningbased model that uses only sequence information of both targets and drugs topredict DT interaction binding affinities. The few studies that focus on DTbinding affinity prediction use either 3D structures of protein-ligandcomplexes or 2D features of compounds. One novel approach used in this work isthe modeling of protein sequences and compound 1D representations withconvolutional neural networks (CNNs). The results show that the proposed deeplearning based model that uses the 1D representations of targets and drugs isan effective approach for drug target binding affinity prediction. The model inwhich high-level representations of a drug and a target are constructed viaCNNs achieved the best Concordance Index (CI) performance in one of our largerbenchmark data sets, outperforming the KronRLS algorithm and SimBoost, astate-of-the-art method for DT binding affinity prediction.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
mocobt	mocobt??	225	454	分子構造の学習でGCNは使われていたり。 https://t.co/l78WSOUKkN #xpaperchallenge	2019/7/3 21:22	https://arxiv.org/abs/1805.11973	MolGAN: An implicit generative model for small molecular graphs	Deep generative models for graph-structured data offer a new angle on theproblem of chemical synthesis: by optimizing differentiable models thatdirectly generate molecular graphs, it is possible to side-step expensivesearch procedures in the discrete and vast space of chemical structures. Weintroduce MolGAN, an implicit, likelihood-free generative model for smallmolecular graphs that circumvents the need for expensive graph matchingprocedures or node ordering heuristics of previous likelihood-based methods.Our method adapts generative adversarial networks (GANs) to operate directly ongraph-structured data. We combine our approach with a reinforcement learningobjective to encourage the generation of molecules with specific desiredchemical properties. In experiments on the QM9 chemical database, wedemonstrate that our model is capable of generating close to 100% validcompounds. MolGAN compares favorably both to recent proposals that usestring-based (SMILES) representations of molecules and to a likelihood-basedmethod that directly generates graphs, albeit being susceptible to modecollapse.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
akihiro_akichan	akihiro_f	122	118	https://t.co/SwjxcFA9jQ 料理画像から材料と作り方を生成する研究。ResNetで特徴量を抽出して材料を推定、Transformer Decoderで再帰的に作り方を構成、という作りになっている。ドメイン絞れば、材料のレコメンドくらいには使えそう。料理写真から材料推定→カロリー計算とかできないかな https://t.co/TorBPMRQCS	2019/7/3 21:25	https://arxiv.org/abs/1812.06164	Inverse Cooking: Recipe Generation from Food Images	People enjoy food photography because they appreciate food. Behind each mealthere is a story described in a complex recipe and, unfortunately, by simplylooking at a food image we do not have access to its preparation process.Therefore, in this paper we introduce an inverse cooking system that recreatescooking recipes given food images. Our system predicts ingredients as sets bymeans of a novel architecture, modeling their dependencies without imposing anyorder, and then generates cooking instructions by attending to both image andits inferred ingredients simultaneously. We extensively evaluate the wholesystem on the large-scale Recipe1M dataset and show that (1) we improveperformance w.r.t. previous baselines for ingredient prediction; (2) we areable to obtain high quality recipes by leveraging both image and ingredients;(3) our system is able to produce more compelling recipes than retrieval-basedapproaches according to human judgment. We make code and models publiclyavailable.	Computer Vision and Pattern Recognition (cs.CV)	
s_aiueo32	さしすせ	325	430	GraphSAGEと同じ著者なのか - https://t.co/m6InY1gbBT #xpaperchallenge	2019/7/3 21:32	https://arxiv.org/abs/1806.01445	Embedding Logical Queries on Knowledge Graphs	"Learning low-dimensional embeddings of knowledge graphs is a powerfulapproach used to predict unobserved or missing edges between entities. However,an open challenge in this area is developing techniques that can go beyondsimple edge prediction and handle more complex logical queries, which mightinvolve multiple unobserved edges, entities, and variables. For instance, givenan incomplete biological knowledge graph, we might want to predict ""em whatdrugs are likely to target proteins involved with both diseases X and Y?"" -- aquery that requires reasoning about all possible proteins that {\em might}interact with diseases X and Y. Here we introduce a framework to efficientlymake predictions about conjunctive logical queries -- a flexible but tractablesubset of first-order logic -- on incomplete knowledge graphs. In our approach,we embed graph nodes in a low-dimensional space and represent logical operatorsas learned geometric operations (e.g., translation, rotation) in this embeddingspace. By performing logical operations within a low-dimensional embeddingspace, our approach achieves a time complexity that is linear in the number ofquery variables, compared to the exponential complexity required by a naiveenumeration-based approach. We demonstrate the utility of this framework in twoapplication studies on real-world datasets with millions of relations:predicting logical relationships in a network of drug-gene-disease interactionsand in a graph-based representation of social interactions derived from apopular web forum."	Social and Information Networks (cs.SI)	Machine Learning (cs.LG);Machine Learning (stat.ML)
yoshi_and_aki	Yoshi-aki Shimada	624	831	ISCA2019のMartonoshiグループの「Full-Stack, Real-System Quantum Computer Studies」なかなか面白い。Rigetti（超伝導）, IBM（超伝導）, UMD（イオントラップ）の量子コンピュータを複数のプログラムで性能比較。UMDマシンが良いっぽいなぁ。https://t.co/AsuLErJ0Xg	2019/7/3 22:13	https://arxiv.org/abs/1905.11349	Full-Stack, Real-System Quantum Computer Studies: Architectural Comparisons and Design Insights	In recent years, Quantum Computing (QC) has progressed to the point wheresmall working prototypes are available for use. Termed Noisy Intermediate-ScaleQuantum (NISQ) computers, these prototypes are too small for large benchmarksor even for Quantum Error Correction, but they do have sufficient resources torun small benchmarks, particularly if compiled with optimizations to make useof scarce qubits and limited operation counts and coherence times. QC has notyet, however, settled on a particular preferred device implementationtechnology, and indeed different NISQ prototypes implement qubits with verydifferent physical approaches and therefore widely-varying device and machinecharacteristics.Our work performs a full-stack, benchmark-driven hardware-software analysisof QC systems. We evaluate QC architectural possibilities, software-visiblegates, and software optimizations to tackle fundamental design questions aboutgate set choices, communication topology, the factors affecting benchmarkperformance and compiler optimizations. In order to answer key cross-technologyand cross-platform design questions, our work has built the first top-to-bottomtoolflow to target different qubit device technologies, includingsuperconducting and trapped ion qubits which are the current QC front-runners.We use our toolflow, TriQ, to conduct {\em real-system} measurements on 7running QC prototypes from 3 different groups, IBM, Rigetti, and University ofMaryland. From these real-system experiences at QC's hardware-softwareinterface, we make observations about native and software-visible gates fordifferent QC technologies, communication topologies, and the value ofnoise-aware compilation even on lower-noise platforms. This is the largestcross-platform real-system QC study performed thus far; its results have thepotential to inform both QC device and compiler design going forward.	Quantum Physics (quant-ph)	
tomonoritotani	戸谷友則 (TOTANI, Tomonori)	352	0	https://t.co/VTFF77PRiE 先日ツイートした、単発の高速電波バーストとして始めて FRB 180924 に母銀河が同定されたというニュースに続き、別チームが FRB 190523 の母銀河を同定。こちらも、大きめで星形成をあまりしていない銀河ということで、古い星種族からの FRB を支持。	2019/7/3 23:04	https://arxiv.org/abs/1907.01542	A fast radio burst localised to a massive galaxy	Intense, millisecond-duration bursts of radio waves have been detected frombeyond the Milky Way [1]. Their extragalactic origins are evidenced by theirlarge dispersion measures, which are greater than expected for propagationthrough the Milky Way interstellar medium alone, and imply contributions fromthe intergalactic medium and potentially host galaxies [2]. Although severaltheories exist for the sources of these fast radio bursts, their intensities,durations and temporal structures suggest coherent emission from highlymagnetised plasma [3,4]. Two sources have been observed to repeat [5,6], andone repeater (FRB 121102) has been localised to the largest star-forming regionof a dwarf galaxy at a cosmological redshift of 0.19 [7, 8]. However, the hostgalaxies and distances of the so far non-repeating fast radio bursts are yet tobe identified. Unlike repeating sources, these events must be observed with aninterferometer with sufficient spatial resolution for arcsecond localisation atthe time of discovery. Here we report the localisation of a fast radio burst(FRB 190523) to a few-arcsecond region containing a single massive galaxy at aredshift of 0.66. This galaxy is in stark contrast to the host of FRB 121102,being a thousand times more massive, with a greater than hundred times lowerspecific star-formation rate. The properties of this galaxy highlight thepossibility of a channel for FRB production associated with older stellarpopulations.	High Energy Astrophysical Phenomena (astro-ph.HE)	Instrumentation and Methods for Astrophysics (astro-ph.IM)
Soliton111	Soliton	78	96	fishnet theoryのholographic dual(?)できてたのかw 作用は近接相互作用をもつ点粒子の集まりのものでかけて、ある意味でAdS5上のstringを離散化したものとみなせるみたいですね。 順当ではある。  https://t.co/om99PMTzke https://t.co/mfsaUU2Zmv	2019/7/3 23:15	https://arxiv.org/abs/1903.10508, https://arxiv.org/abs/1907.01001	The Holographic Fishchain, Quantum Fishchain in $AdS_5$	We present the first-principle derivation of a weak-strong duality betweenthe fishnet theory in four dimensions and a discretized string-like modelliving in five dimensions. At strong coupling, the dual description becomesclassical and we demonstrate explicitly the classical integrability of themodel. We test our results by reproducing the strong coupling limit of the$4$-point correlator computed before non-perturbatively from the conformalpartial wave expansion. Due to the extreme simplicity of our model, it couldprovide an ideal playground for holography with no super-symmetry. Furthermore,since the fishnet model and ${\cal N}=4$ SYM theory are continuously linked ourconsideration could shed light on the derivation of AdS/CFT for the latter., In our previous paper we derived the holographic dual of the planar fishnetCFT in four dimensions. The dual model becomes classical in the stronglycoupled regime of the CFT and takes the form of an integrable chain ofparticles in five dimensions. Here we study the theory at the quantum level. Byapplying the canonical quantization procedure with constraints, we show thatthe model describes a quantum chain of particles propagating in $AdS_5$. Weprove the duality at the full quantum level in the ${\mathfrak u}(1)$ sectorand reproduce exactly the spectrum for the cases when it is known analytically.	High Energy Physics - Theory (hep-th), High Energy Physics - Theory (hep-th)	Mathematical Physics (math-ph), Mathematical Physics (math-ph)
nyker_goto	ニューヨーカーGOTO	798	735	2009年時点で https://t.co/Pbodg8v9bU にて bottoun 先生があげていた shuffle が良い理由も2015年に https://t.co/aCYn2Z3FHI で少し明らかになったりしているので SGD にはまだ改善の余地があると信じている。	2019/7/4 0:59	https://arxiv.org/abs/1510.08560	Why Random Reshuffling Beats Stochastic Gradient Descent	We analyze the convergence rate of the random reshuffling (RR) method, whichis a randomized first-order incremental algorithm for minimizing a finite sumof convex component functions. RR proceeds in cycles, picking a uniformlyrandom order (permutation) and processing the component functions one at a timeaccording to this order, i.e., at each cycle, each component function issampled without replacement from the collection. Though RR has been numericallyobserved to outperform its with-replacement counterpart stochastic gradientdescent (SGD), characterization of its convergence rate has been a longstanding open question. In this paper, we answer this question by showing thatwhen the component functions are quadratics or smooth and the sum function isstrongly convex, RR with iterate averaging and a diminishing stepsize$\alpha_k=\Theta(1/k^s)$ for $s\in (1/2,1)$ converges at rate$\Theta(1/k^{2s})$ with probability one in the suboptimality of the objectivevalue, thus improving upon the $\Omega(1/k)$ rate of SGD. Our analysis draws onthe theory of Polyak-Ruppert averaging and relies on decoupling the dependentcycle gradient error into an independent term over cycles and another termdominated by $\alpha_k^2$. This allows us to apply law of large numbers to anappropriately weighted version of the cycle gradient errors, where the weightsdepend on the stepsize. We also provide high probability convergence rateestimates that shows decay rate of different terms and allows us to propose amodification of RR with convergence rate ${\cal O}(\frac{1}{k^2})$.	Optimization and Control (math.OC)	
cygnusgm	アルミニ	366	898	The Physics of baking good Pizza https://t.co/iS9z7wweNn 　　　↑ ギガジンで解説している元論文  ちょっと前にはオムレツを流体力学で焼こうとしてた論文があったな? https://t.co/SD4JbrT4at	2019/7/4 1:26	https://arxiv.org/abs/1806.08790	The Physics of baking good Pizza	Physical principles are involved in almost any aspect of cooking. Here weanalyze the specific process of baking pizzas, deriving in simple terms thebaking times for two different situations: For a brick oven in a pizzeria and amodern metallic oven at home. Our study is based on basic thermodynamicprinciples relevant to the cooking process and is accessible to undergraduatestudents. We start with a historical overview of the development and art ofpizza baking, illustrate the underlying physics by some simple common examples,and then apply them in detail to the example of baking pizza.	Popular Physics (physics.pop-ph)	
udoooom	うどん	1,871	1,938	ESPNetの論文中にはTitanXで112FPSと書かれてるが，1080Tiだと116fps出るし凄まじい しかも精度も良い... https://t.co/cOSZAyefIJ	2019/7/4 1:26	https://arxiv.org/abs/1803.06815	ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation	We introduce a fast and efficient convolutional neural network, ESPNet, forsemantic segmentation of high resolution images under resource constraints.ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP),which is efficient in terms of computation, memory, and power. ESPNet is 22times faster (on a standard GPU) and 180 times smaller than thestate-of-the-art semantic segmentation network PSPNet, while its category-wiseaccuracy is only 8% less. We evaluated ESPNet on a variety of semanticsegmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsywhole slide image dataset. Under the same constraints on memory andcomputation, ESPNet outperforms all the current efficient CNN networks such asMobileNet, ShuffleNet, and ENet on both standard metrics and our newlyintroduced performance metrics that measure efficiency on edge devices. Ournetwork can process high resolution images at a rate of 112 and 9 frames persecond on a standard GPU and edge device, respectively.	Computer Vision and Pattern Recognition (cs.CV)	
subarusatosi	中嶋慧	3,333	23	素粒子論にフェルマーの最終定理が関係してたら面白いなと思った事はあるけど、本当に関係しているのか… https://t.co/c8qMC1bbD3	2019/7/4 3:17	https://arxiv.org/abs/1907.00514	Hypercharge Quantisation and Fermat's Last Theorem	What values of the Standard Model hypercharges result in a mathematicallyconsistent quantum field theory? We show that the constraints imposed by thelack of gauge anomalies can be recast as the equation x^3 + y^3 = z^3. Ifhypercharge is quantised, then x, y and z must be integers. The trivial (andonly) solutions, with x=0 or y=0, reproduce the hypercharge assignments seen inNature. This argument does not rely on the mixed gauge-gravitational anomaly,which is automatically vanishing if hypercharge is quantised and the gaugeanomalies vanish.	High Energy Physics - Theory (hep-th)	High Energy Physics - Phenomenology (hep-ph)
tonagai	tomo	406	88	AI ファインマン - 測定データ列から、一体どんな関数になっているかを推定する。プランクの法則とか見つけられる？ 論文：AI Feynman: a Physics-Inspired Method for Symbolic Regression https://t.co/iilSfGQvsL サイト：Feynman Symbolic Regression Database https://t.co/2iS7ZgQCDD	2019/7/4 6:52	https://arxiv.org/abs/1905.11481	AI Feynman: a Physics-Inspired Method for Symbolic Regression	A core challenge for both physics and artificial intellicence (AI) issymbolic regression: finding a symbolic expression that matches data from anunknown function. Although this problem is likely to be NP-hard in principle,functions of practical interest often exhibit symmetries, separability,compositionality and other simplifying properties. In this spirit, we develop arecursive multidimensional symbolic regression algorithm that combines neuralnetwork fitting with a suite of physics-inspired techniques. We apply it to 100equations from the Feynman Lectures on Physics, and it discovers all of them,while previous publicly available software cracks only 71; for a more difficulttest set, we improve the state of the art success rate from 15% to 90%.	Computational Physics (physics.comp-ph)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG);High Energy Physics - Theory (hep-th)
1789aorhow	まきゃ	325	241	https://t.co/a5oEYFz2HM 出社したらこれコピーする	2019/7/4 7:54	https://arxiv.org/abs/1907.00514	Hypercharge Quantisation and Fermat's Last Theorem	What values of the Standard Model hypercharges result in a mathematicallyconsistent quantum field theory? We show that the constraints imposed by thelack of gauge anomalies can be recast as the equation x^3 + y^3 = z^3. Ifhypercharge is quantised, then x, y and z must be integers. The trivial (andonly) solutions, with x=0 or y=0, reproduce the hypercharge assignments seen inNature. This argument does not rely on the mixed gauge-gravitational anomaly,which is automatically vanishing if hypercharge is quantised and the gaugeanomalies vanish.	High Energy Physics - Theory (hep-th)	High Energy Physics - Phenomenology (hep-ph)
haruhiko_nishi	Haruhiko Nishi	69	177	https://t.co/4yktLkbaTX @sasaism 今度教えて下さい。	2019/7/4 7:58	https://arxiv.org/abs/1803.02194	Bidding Machine: Learning to Bid for Directly Optimizing Profits in Display Advertising	Real-time bidding (RTB) based display advertising has become one of the keytechnological advances in computational advertising. RTB enables advertisers tobuy individual ad impressions via an auction in real-time and facilitates theevaluation and the bidding of individual impressions across multipleadvertisers. In RTB, the advertisers face three main challenges when optimizingtheir bidding strategies, namely (i) estimating the utility (e.g., conversions,clicks) of the ad impression, (ii) forecasting the market value (thus the cost)of the given ad impression, and (iii) deciding the optimal bid for the givenauction based on the first two. Previous solutions assume the first two aresolved before addressing the bid optimization problem. However, thesechallenges are strongly correlated and dealing with any individual problemindependently may not be globally optimal. In this paper, we propose BiddingMachine, a comprehensive learning to bid framework, which consists of threeoptimizers dealing with each challenge above, and as a whole, jointly optimizesthese three parts. We show that such a joint optimization would largelyincrease the campaign effectiveness and the profit. From the learningperspective, we show that the bidding machine can be updated smoothly with bothoffline periodical batch or online sequential training schemes. Our extensiveoffline empirical study and online A/B testing verify the high effectiveness ofthe proposed bidding machine.	Computer Science and Game Theory (cs.GT)	Computers and Society (cs.CY);Information Retrieval (cs.IR);Machine Learning (cs.LG)
syuntoku14	しゅんとく	609	673	https://t.co/wCUPhNRqMj  これ、safe rlの中でもかなりシンプルかつ強そうなのですごい(小学生)	2019/7/4 8:30	https://arxiv.org/abs/1801.08757	Safe Exploration in Continuous Action Spaces	We address the problem of deploying a reinforcement learning (RL) agent on aphysical system such as a datacenter cooling unit or robot, where criticalconstraints must never be violated. We show how to exploit the typically smoothdynamics of these systems and enable RL algorithms to never violate constraintsduring learning. Our technique is to directly add to the policy a safety layerthat analytically solves an action correction formulation per each state. Thenovelty of obtaining an elegant closed-form solution is attained due to alinearized model, learned on past trajectories consisting of arbitrary actions.This is to mimic the real-world circumstances where data logs were generatedwith a behavior policy that is implausible to describe mathematically; suchcases render the known safety-aware off-policy methods inapplicable. Wedemonstrate the efficacy of our approach on new representative physics-basedenvironments, and prevail where reward shaping fails by maintaining zeroconstraint violations.	Artificial Intelligence (cs.AI)	
yu4u	Yusuke Uchida	4,447	950	tf-liteの話かな。The Minimum-Cost Flow Algorithmとかでてくるし、やはり人類は全てのCS分野に精通すべきと思ってしまう / “[1907.01989] On-Device Neural Net Inference with Mobile GPUs” https://t.co/y94j8xJrTK	2019/7/4 9:54	https://arxiv.org/abs/1907.01989	On-Device Neural Net Inference with Mobile GPUs	On-device inference of machine learning models for mobile phones is desirabledue to its lower latency and increased privacy. Running such acompute-intensive task solely on the mobile CPU, however, can be difficult dueto limited computing power, thermal constraints, and energy consumption. Appdevelopers and researchers have begun exploiting hardware accelerators toovercome these challenges. Recently, device manufacturers are adding neuralprocessing units into high-end phones for on-device inference, but theseaccount for only a small fraction of hand-held devices. In this paper, wepresent how we leverage the mobile GPU, a ubiquitous hardware accelerator onvirtually every phone, to run inference of deep neural networks in real-timefor both Android and iOS devices. By describing our architecture, we alsodiscuss how to design networks that are mobile GPU-friendly. Ourstate-of-the-art mobile GPU inference engine is integrated into the open-sourceproject TensorFlow Lite and publicly available at this https URL.	Distributed, Parallel, and Cluster Computing (cs.DC)	Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)
_tkato_	tkato	763	308	これおもしろいよ https://t.co/UXXrW86qKt	2019/7/4 10:14	https://arxiv.org/abs/1907.01989	On-Device Neural Net Inference with Mobile GPUs	On-device inference of machine learning models for mobile phones is desirabledue to its lower latency and increased privacy. Running such acompute-intensive task solely on the mobile CPU, however, can be difficult dueto limited computing power, thermal constraints, and energy consumption. Appdevelopers and researchers have begun exploiting hardware accelerators toovercome these challenges. Recently, device manufacturers are adding neuralprocessing units into high-end phones for on-device inference, but theseaccount for only a small fraction of hand-held devices. In this paper, wepresent how we leverage the mobile GPU, a ubiquitous hardware accelerator onvirtually every phone, to run inference of deep neural networks in real-timefor both Android and iOS devices. By describing our architecture, we alsodiscuss how to design networks that are mobile GPU-friendly. Ourstate-of-the-art mobile GPU inference engine is integrated into the open-sourceproject TensorFlow Lite and publicly available at this https URL.	Distributed, Parallel, and Cluster Computing (cs.DC)	Computer Vision and Pattern Recognition (cs.CV);Machine Learning (cs.LG);Machine Learning (stat.ML)
fkm	fkm	891	168	MLの手法を使ってソート、やってみた人いるんだ https://t.co/L2yd7QRHJg	2019/7/4 10:22	https://arxiv.org/abs/1805.04272	An $O(N)$ Sorting Algorithm: Machine Learning Sort	We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,which shows a huge potential sorting big data. This sorting algorithm can beapplied to parallel sorting and is suitable for GPU or TPU acceleration.Furthermore, we discuss the application of this algorithm to sparse hash table.	Machine Learning (cs.LG)	Data Structures and Algorithms (cs.DS);Machine Learning (stat.ML)
hobbymath2020	hobbymath	81	55	プレプリント  “A New Lower Bound for Kullback-Leibler Divergence Based on Hammersley-Chapman-Robbins Bound”  https://t.co/fZ4WciPrEI  に対するサンプルコードを公開しました。 https://t.co/tQR2pB709O	2019/7/4 12:04	https://arxiv.org/abs/1907.00288	A New Lower Bound for Kullback-Leibler Divergence Based on Hammersley-Chapman-Robbins Bound	In this paper, we derive a useful lower bound for the Kullback-Leiblerdivergence (KL-divergence) based on the Hammersley-Chapman-Robbins bound(HCRB). The HCRB states that the variance of an estimator is bounded from belowby the Chi-square divergence and the expectation value of the estimator. Byusing the relation between the KL-divergence and the Chi-square divergence, weshow that the lower bound for the KL-divergence which only depends on theexpectation value and the variance of a function we choose. We show that theequality holds for the Bernoulli distributions and show that the inequalityconverges to the Cramér-Rao bound when two distributions are very close.Furthermore, we describe application examples and examples of numericalcalculation.	Statistics Theory (math.ST)	Information Theory (cs.IT);Machine Learning (stat.ML)
TsuguoMogami	mogami290	12	10	DL without Weight Transport: https://t.co/V13lBlig9f やっぱり眠っている間にランダム発火を使って行列から転置行列にweightを転写できるとう話だ。私の１３年前に書いたこれ https://t.co/gc3FgmrwOt と同じアイデア。やられた。	2019/7/4 13:13	https://arxiv.org/abs/1904.05391	Deep Learning without Weight Transport	Current algorithms for deep learning probably cannot run in the brain becausethey rely on weight transport, where forward-path neurons transmit theirsynaptic weights to a feedback path, in a way that is likely impossiblebiologically. An algorithm called feedback alignment achieves deep learningwithout weight transport by using random feedback weights, but it performspoorly on hard visual-recognition tasks. Here we describe two mechanisms - aneural circuit called a weight mirror and a modification of an algorithmproposed by Kolen and Pollack in 1994 - both of which let the feedback pathlearn appropriate synaptic weights quickly and accurately even in largenetworks, without weight transport or complex wiring.Tested on the ImageNetvisual-recognition task, these mechanisms outperform both feedback alignmentand the newer sign-symmetry method, and nearly match backprop, the standardalgorithm of deep learning, which uses weight transport.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
benkyouaho	Io NGS	153	178	quark hadron dualityについて https://t.co/fdzO5VC9GC  あとで読む	2019/7/4 14:01	https://arxiv.org/abs/hep-ph/0009131	[hep-ph/0009131] Quark-Hadron Duality	"I review the notion of the quark-hadron duality from the modern perspective.Both, the theoretical foundation and practical applications are discussed. Theproper theoretical framework in which the problem can be formulated and treatedis Wilson's operator product expansion (OPE). Two models developed for thedescription of duality violations are considered in some detail: one isinstanton-based, another resonance-based. The mechanisms they represent arecomplementary. Although both models are rather primitive (their largest virtueis their simplicity) they hopefully capture important features of thephenomenon. Being open for improvements, they can be used ""as is"" fororientation in the studies of duality violations in the processes of practicalinterest."	High Energy Physics - Phenomenology (hep-ph)	
KUNImt_Sun	陸〇	276	357	めっちゃ現場感あるやつみつけた 例えばSEMとかで十分条件ベースではパスモデルを作らない場合に識別条件引っかかるときの最後段階的フィルタリングみたいな形ですぐ使えるやつだなこれは https://t.co/jEBoDMj3MI	2019/7/4 17:40	https://arxiv.org/abs/1907.01654	Adjustment Criteria for Recovering Causal Effects from Missing Data	Confounding bias, missing data, and selection bias are three common obstaclesto valid causal inference in the data sciences. Covariate adjustment is themost pervasive technique for recovering casual effects from confounding bias.In this paper, we introduce a covariate adjustment formulation for controllingconfounding bias in the presence of missing-not-at-random data and develop anecessary and sufficient condition for recovering causal effects using theadjustment. We also introduce an adjustment formulation for controlling bothconfounding and selection biases in the presence of missing data and develop anecessary and sufficient condition for valid adjustment. Furthermore, wepresent an algorithm that lists all valid adjustment sets and an algorithm thatfinds a valid adjustment set containing the minimum number of variables, whichare useful for researchers interested in selecting adjustment sets with desiredproperties.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
jaguring1	小猫遊りょう（たかにゃし・りょう）	2,601	191	Hypercharge Quantisation and Fermat's Last Theorem https://t.co/JQG4yiJYMM フェルマーの最終定理が道具として使われた事例。一見すると、あんな役に立たなさそうな定理でさえ、他分野で使われていく。これが数学の面白さの一側面だと思っている。	2019/7/4 17:43	https://arxiv.org/abs/1907.00514	Hypercharge Quantisation and Fermat's Last Theorem	What values of the Standard Model hypercharges result in a mathematicallyconsistent quantum field theory? We show that the constraints imposed by thelack of gauge anomalies can be recast as the equation x^3 + y^3 = z^3. Ifhypercharge is quantised, then x, y and z must be integers. The trivial (andonly) solutions, with x=0 or y=0, reproduce the hypercharge assignments seen inNature. This argument does not rely on the mixed gauge-gravitational anomaly,which is automatically vanishing if hypercharge is quantised and the gaugeanomalies vanish.	High Energy Physics - Theory (hep-th)	High Energy Physics - Phenomenology (hep-ph)
subarusatosi	中嶋慧	3,333	23	“Hypercharge Quantisation and Fermat's Last Theorem” https://t.co/c8qMC1bbD3  ハイパーチャージのパラメーターは5つがあるが、(1)より3つに落ちる。(2)は3パラメーターの3次式となる。 ハイパーチャージの量子化を仮定すると、フェルマーの最終定理(n = 3)より、ハイパーチャージが決まる。 https://t.co/uu5gwGvglK	2019/7/4 18:01	https://arxiv.org/abs/1907.00514	Hypercharge Quantisation and Fermat's Last Theorem	What values of the Standard Model hypercharges result in a mathematicallyconsistent quantum field theory? We show that the constraints imposed by thelack of gauge anomalies can be recast as the equation x^3 + y^3 = z^3. Ifhypercharge is quantised, then x, y and z must be integers. The trivial (andonly) solutions, with x=0 or y=0, reproduce the hypercharge assignments seen inNature. This argument does not rely on the mixed gauge-gravitational anomaly,which is automatically vanishing if hypercharge is quantised and the gaugeanomalies vanish.	High Energy Physics - Theory (hep-th)	High Energy Physics - Phenomenology (hep-ph)
enakalle	梅谷 武	419	51	Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear B https://t.co/hfoCTLNZLx  ＜言語学はこういうことがやりやすい分野なので、若い方はこの方向を目指すという選択肢も検討すべきかもしれない。	2019/7/4 18:31	https://arxiv.org/abs/1906.06718	Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear B	In this paper we propose a novel neural approach for automatic deciphermentof lost languages. To compensate for the lack of strong supervision signal, ourmodel design is informed by patterns in language change documented inhistorical linguistics. The model utilizes an expressive sequence-to-sequencemodel to capture character-level correspondences between cognates. Toeffectively train the model in an unsupervised manner, we innovate the trainingprocedure by formalizing it as a minimum-cost flow problem. When applied to thedecipherment of Ugaritic, we achieve a 5.5% absolute improvement overstate-of-the-art results. We also report the first automatic results indeciphering Linear B, a syllabic language related to ancient Greek, where ourmodel correctly translates 67.3% of cognates.	Computation and Language (cs.CL)	
mimura8322	Yukihiro MIMURA	42	121	“Hypercharge Quantisation and Fermat's Last Theorem” (https://t.co/XyQxxHlhpu)  / ハイパーチャージがU(1)の既約表現なら、表現の準同型写像が正しく定義されるためには電荷（の比）は有理数である。よって、著者はチャージの量子化の条件をもっと磨くべきである。（ほら日本語ならまだ字数余る）	2019/7/4 19:54	https://arxiv.org/abs/1907.00514	Hypercharge Quantisation and Fermat's Last Theorem	What values of the Standard Model hypercharges result in a mathematicallyconsistent quantum field theory? We show that the constraints imposed by thelack of gauge anomalies can be recast as the equation x^3 + y^3 = z^3. Ifhypercharge is quantised, then x, y and z must be integers. The trivial (andonly) solutions, with x=0 or y=0, reproduce the hypercharge assignments seen inNature. This argument does not rely on the mixed gauge-gravitational anomaly,which is automatically vanishing if hypercharge is quantised and the gaugeanomalies vanish.	High Energy Physics - Theory (hep-th)	High Energy Physics - Phenomenology (hep-ph)
re_hako_moon	はこつき＠VR	43	49	https://t.co/iaNhd5K2Uy 弱教師あり学習による三次元点群のセグメンテーション手法．一貫したセグメンテーション結果となるよう，Consistency Scoreを導入してオンラインで整合するように最適化する．	2019/7/4 20:27	https://arxiv.org/abs/1903.10297	CoSegNet: Deep Co-Segmentation of 3D Shapes with Group Consistency Loss	We introduce CoSegNet, a deep neural network architecture for co-segmentationof a set of 3D shapes represented as point clouds. CoSegNet takes as input aset of unsegmented shapes, proposes per-shape parts, and then jointly optimizesthe part labelings across the set subjected to a novel group consistency lossexpressed via matrix rank estimates. The proposals are refined in eachiteration by an auxiliary network that acts as a weak regularizing prior,pre-trained to denoise noisy, unlabeled parts from a large collection ofsegmented 3D shapes, where the part compositions within the same objectcategory can be highly inconsistent. The output is a consistent part labelingfor the input set, with each shape segmented into up to K (a user-specifiedhyperparameter) parts. The overall pipeline is thus weakly supervised,producing consistent segmentations tailored to the test set, without consistentground-truth segmentations. We show qualitative and quantitative results fromCoSegNet and evaluate it via ablation studies and comparisons tostate-of-the-art co-segmentation methods.	Computer Vision and Pattern Recognition (cs.CV)	Graphics (cs.GR)
Lepidoptera2015	あおすじあげはちょう	290	347	https://t.co/Kf1BB6p5IY deepの学習をfrank wolfeベースのアルゴリズムにすることで、stepsizeがclosed formで求まり、パラメータ一つ決めればlrの微調整頑張らなくてよく、SGDと同程度の性能が出るよという主張、ICLR19	2019/7/4 22:23	https://arxiv.org/abs/1811.07591	Deep Frank-Wolfe For Neural Network Optimization	Learning a deep neural network requires solving a challenging optimizationproblem: it is a high-dimensional, non-convex and non-smooth minimizationproblem with a large number of terms. The current practice in neural networkoptimization is to rely on the stochastic gradient descent (SGD) algorithm orits adaptive variants. However, SGD requires a hand-designed schedule for thelearning rate. In addition, its adaptive variants tend to produce solutionsthat generalize less well on unseen data than SGD with a hand-designedschedule. We present an optimization method that offers empirically the best ofboth worlds: our algorithm yields good generalization performance whilerequiring only one hyper-parameter. Our approach is based on a compositeproximal framework, which exploits the compositional nature of deep neuralnetworks and can leverage powerful convex optimization algorithms by design.Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computesan optimal step-size in closed-form at each time-step. We further show that thedescent direction is given by a simple backward pass in the network, yieldingthe same computational cost per iteration as SGD. We present experiments on theCIFAR and SNLI data sets, where we demonstrate the significant superiority ofour method over Adam, Adagrad, as well as the recently proposed BPGrad andAMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designedlearning rate schedule, and show that it provides similar generalization whileconverging faster. The code is publicly available atthis https URL.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
hsntdo	Hoshino Tadao	286	100	やっぱり既にあるようだ https://t.co/kG2sBHooYj	2019/7/4 23:56	https://arxiv.org/abs/1610.01271	Generalized Random Forests	We propose generalized random forests, a method for non-parametricstatistical estimation based on random forests (Breiman, 2001) that can be usedto fit any quantity of interest identified as the solution to a set of localmoment equations. Following the literature on local maximum likelihoodestimation, our method considers a weighted set of nearby training examples;however, instead of using classical kernel weighting functions that are proneto a strong curse of dimensionality, we use an adaptive weighting functionderived from a forest designed to express heterogeneity in the specifiedquantity of interest. We propose a flexible, computationally efficientalgorithm for growing generalized random forests, develop a large sample theoryfor our method showing that our estimates are consistent and asymptoticallyGaussian, and provide an estimator for their asymptotic variance that enablesvalid confidence intervals. We use our approach to develop new methods forthree statistical tasks: non-parametric quantile regression, conditionalaverage partial effect estimation, and heterogeneous treatment effectestimation via instrumental variables. A software implementation, grf for R andC++, is available from CRAN.	Methodology (stat.ME)	Econometrics (econ.EM);Machine Learning (stat.ML)
TomiyaAkio	3刷出ます「ディープラーニングと物理学」 A.Tomiya	1,262	504	@astrophys_tan えぇ、それですね https://t.co/FMIPxvKSst  色々と文献読んでて、引いてない奴も多いんですが、説明を聞いた中ではそれが分かり良かった気がします。	2019/7/5 0:21	https://arxiv.org/abs/1110.4732	Maxwell's Demon and Data Compression	In an asymmetric Szilard engine model of Maxwell's demon, we show theequivalence between information theoretical and thermodynamic entropies whenthe demon erases information optimally. The work gain by the engine can beexactly canceled out by the work necessary to reset demon's memory afteroptimal data compression a la Shannon before the erasure.	Classical Physics (physics.class-ph)	Statistical Mechanics (cond-mat.stat-mech);History and Philosophy of Physics (physics.hist-ph)
pacifinapacific	ぱしふぃん	266	543	https://t.co/MCFPySL4fO Person Re-identificationでGeneratorによる画像生成と識別を同一ネットワークで扱う仕組みを提案。生成された画像を効果的に用いることで精度向上に寄与(CVPR 2019 oral)	2019/7/5 1:11	https://arxiv.org/abs/1904.07223	Joint Discriminative and Generative Learning for Person Re-identification	Person re-identification (re-id) remains challenging due to significantintra-class variations across different cameras. Recently, there has been agrowing interest in using generative models to augment training data andenhance the invariance to input changes. The generative pipelines in existingmethods, however, stay relatively separate from the discriminative re-idlearning stages. Accordingly, re-id models are often trained in astraightforward manner on the generated data. In this paper, we seek to improvelearned re-id embeddings by better leveraging the generated data. To this end,we propose a joint learning framework that couples re-id learning and datageneration end-to-end. Our model involves a generative module that separatelyencodes each person into an appearance code and a structure code, and adiscriminative module that shares the appearance encoder with the generativemodule. By switching the appearance or structure codes, the generative moduleis able to generate high-quality cross-id composed images, which are online fedback to the appearance encoder and used to improve the discriminative module.The proposed joint learning framework renders significant improvement over thebaseline without using generated data, leading to the state-of-the-artperformance on several benchmark datasets.	Computer Vision and Pattern Recognition (cs.CV)	
mocobt	mocobt??	225	454	なんかHMMってぐらふっぽいしGNNと組み合わせた方法とかあるのかなと調べてみたら，先月発表された手法があった． やっぱ温故知新というか，昔の理論って大事な感じある．  [Qu et al. arXiv 2019] https://t.co/e5BX1rYl9x	2019/7/5 2:38	https://arxiv.org/abs/1905.06214	GMNN: Graph Markov Neural Networks	This paper studies semi-supervised object classification in relational data,which is a fundamental problem in relational data modeling. The problem hasbeen extensively studied in the literature of both statistical relationallearning (e.g. relational Markov networks) and graph neural networks (e.g.graph convolutional networks). Statistical relational learning methods caneffectively model the dependency of object labels through conditional randomfields for collective classification, whereas graph neural networks learneffective object representations for classification through end-to-endtraining. In this paper, we propose the Graph Markov Neural Network (GMNN) thatcombines the advantages of both worlds. A GMNN models the joint distribution ofobject labels with a conditional random field, which can be effectively trainedwith the variational EM algorithm. In the E-step, one graph neural networklearns effective object representations for approximating the posteriordistributions of object labels. In the M-step, another graph neural network isused to model the local label dependency. Experiments on object classification,link classification, and unsupervised node representation learning show thatGMNN achieves state-of-the-art results.	Machine Learning (cs.LG)	Social and Information Networks (cs.SI);Machine Learning (stat.ML)
MasakiTaniguch4	Masaki Taniguchi	20	16	https://t.co/G3dBMgwAHD のCor Dが以前考えたことに似ている… Gromovノルムを使うとは…	2019/7/5 4:47	https://arxiv.org/abs/1804.03777	Equivariant hyperbolization of $3$-manifolds via homology cobordisms	The main result of this paper is that any $3$-dimensional manifold with afinite group action is equivariantly, invertibly homology cobordant to ahyperbolic manifold; this result holds with suitable twisted coefficients aswell. The following two consequences motivated this work. First, there arehyperbolic equivariant corks (as defined in previous work of the authors) for awide class of finite groups. Second, any finite group that acts on a homology$3$-sphere also acts on a hyperbolic homology $3$-sphere. The theorem has otherapplications, including establishing the existence of an infinite number ofhyperbolic homology spheres with a free $Z_p$ action that does not extend toany contractible manifold. A non-equivariant version yields an infinite numberof hyperbolic integer homology spheres that bound integer homology balls but donot bound contractible manifolds. In passing, it is shown that the invertiblehomology cobordism relation on $3$-manifolds is antisymmetric.	Geometric Topology (math.GT)	Differential Geometry (math.DG)
hackernewsj	Hacker News記事題日本語翻訳	605	4	GraphQLへの移行：実用的評価 https://t.co/nrwQkSPqTz	2019/7/5 7:18	https://arxiv.org/abs/1906.07535	Migrating to GraphQL: A Practical Assessment	GraphQL is a novel query language proposed by Facebook to implement Web-basedAPIs. In this paper, we present a practical study on migrating API clients tothis new technology. First, we conduct a grey literature review to gain anin-depth understanding on the benefits and key characteristics normallyassociated to GraphQL by practitioners. After that, we assess such benefits inpractice, by migrating seven systems to use GraphQL, instead of standardREST-based APIs. As our key result, we show that GraphQL can reduce the size ofthe JSON documents returned by REST APIs in 94% (in number of fields) and in99% (in number of bytes), both median results.	Software Engineering (cs.SE)	
hillbig	Daisuke Okanohara	15,910	254	様々な形状の点群の生成モデルとしてPointFlowを提案。形状を表す潜在変数を生成し、それから連続正規化フローのダイナミクスを作り、それに基づき事前分布の点群を目的の形状に変化させる。変分法を使った最尤推定で直接学習可能 https://t.co/VxHqZ7zKL7 https://t.co/KffYbQsKPf	2019/7/5 10:02	https://arxiv.org/abs/1906.12320	PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows	As 3D point clouds become the representation of choice for multiple visionand graphics applications, the ability to synthesize or reconstructhigh-resolution, high-fidelity point clouds becomes crucial. Despite the recentsuccess of deep learning models in discriminative tasks of point clouds,generating point clouds remains challenging. This paper proposes a principledprobabilistic framework to generate 3D point clouds by modeling them as adistribution of distributions. Specifically, we learn a two-level hierarchy ofdistributions where the first level is the distribution of shapes and thesecond level is the distribution of points given a shape. This formulationallows us to both sample shapes and sample an arbitrary number of points from ashape. Our generative model, named PointFlow, learns each level of thedistribution with a continuous normalizing flow. The invertibility ofnormalizing flows enables the computation of the likelihood during training andallows us to train our model in the variational inference framework.Empirically, we demonstrate that PointFlow achieves state-of-the-artperformance in point cloud generation. We additionally show that our model canfaithfully reconstruct point clouds and learn useful representations in anunsupervised manner. The code will be available atthis https URL.	Computer Vision and Pattern Recognition (cs.CV)	Machine Learning (cs.LG)
yu4u	Yusuke Uchida	4,447	950	攻めてる / “[1907.02124] Non-structured DNN Weight Pruning Considered Harmful” https://t.co/llPs9QQClG	2019/7/5 10:24	https://arxiv.org/abs/1907.02124	Non-structured DNN Weight Pruning Considered Harmful	"Large deep neural network (DNN) models pose the key challenge to energyefficiency due to the significantly higher energy consumption of off-chip DRAMaccesses than arithmetic or SRAM operations. It motivates the intensiveresearch on model compression with two main approaches. Weight pruningleverages the redundancy in the number of weights and can be performed in anon-structured, which has higher flexibility and pruning rate but incurs indexaccesses due to irregular weights, or structured manner, which preserves thefull matrix structure with lower pruning rate. Weight quantization leveragesthe redundancy in the number of bits in weights. Compared to pruning,quantization is much more hardware-friendly, and has become a ""must-do"" stepfor FPGA and ASIC implementations. This paper provides a definitive answer tothe question for the first time. First, we build ADMM-NN-S by extending andenhancing ADMM-NN, a recently proposed joint weight pruning and quantizationframework. Second, we develop a methodology for fair and fundamental comparisonof non-structured and structured pruning in terms of both storage andcomputation efficiency. Our results show that ADMM-NN-S consistentlyoutperforms the prior art: (i) it achieves 348x, 36x, and 8x overall weightpruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zeroaccuracy loss; (ii) we demonstrate the first fully binarized (for all layers)DNNs can be lossless in accuracy in many cases. These results provide a strongbaseline and credibility of our study. Based on the proposed comparisonframework, with the same accuracy and quantization, the results show thatnon-structrued pruning is not competitive in terms of both storage andcomputation efficiency. Thus, we conclude that non-structured pruning isconsidered harmful. We urge the community not to continue the DNN inferenceacceleration for non-structured sparsity."	Machine Learning (cs.LG)	Computer Vision and Pattern Recognition (cs.CV);Neural and Evolutionary Computing (cs.NE);Machine Learning (stat.ML)
masashiotani	マサシ	91	92	落とされたが、とりあえずarxivには出した。 https://t.co/drG5Ce7QoZ	2019/7/5 10:55	https://arxiv.org/abs/1907.02235	Compact buncher cavity for muons accelerated by a radio-frequency quadrupole	A buncher cavity has been developed for the muons accelerated by aradio-frequency quadrupole linac (RFQ). The buncher cavity is designed for$\beta=v/c=0.04$ at an operational frequency of 324 MHz. It employs adouble-gap structure operated in the TEM mode for the required effectivevoltage with compact dimensions, in order to account for the limited space ofthe experiment. The measured resonant frequency and unloaded quality factor are323.95 MHz and $3.06\times10^3$, respectively. The buncher cavity wassuccessfully operated for longitudinal bunch size measurement of the muonsaccelerated by the RFQ.	Accelerator Physics (physics.acc-ph)	High Energy Physics - Experiment (hep-ex)
Vengineer	ソースコード解析職人	1,664	80	論文は、これみたい。 「Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications」 https://t.co/GLpQDxD0Jo https://t.co/gPHmPPWEzW	2019/7/5 11:32	https://arxiv.org/abs/1811.09886	Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications	The application of deep learning techniques resulted in remarkableimprovement of machine learning models. In this paper provides detailedcharacterizations of deep learning models used in many Facebook social networkservices. We present computational characteristics of our models, describe highperformance optimizations targeting existing systems, point out theirlimitations and make suggestions for the future general-purpose/acceleratedinference hardware. Also, we highlight the need for better co-design ofalgorithms, numerics and computing platforms to address the challenges ofworkloads often run in data centers.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
udoooom	うどん	1,871	1,938	Long-Term Feature Banks，今更知ったが発想シンプルで時間特徴をうまい感じに使ってるの凄いなと今更思った  https://t.co/GA7OxgDxmF	2019/7/5 12:25	https://arxiv.org/abs/1812.05038	Long-Term Feature Banks for Detailed Video Understanding	To understand the world, we humans constantly need to relate the present tothe past, and put events in context. In this paper, we enable existing videomodels to do the same. We propose a long-term feature bank---supportiveinformation extracted over the entire span of a video---to augmentstate-of-the-art video models that otherwise would only view short clips of 2-5seconds. Our experiments demonstrate that augmenting 3D convolutional networkswith a long-term feature bank yields state-of-the-art results on threechallenging video datasets: AVA, EPIC-Kitchens, and Charades.	Computer Vision and Pattern Recognition (cs.CV)	
hhhhhhaaaaaa2	h.a.	339	524	Dirac magnons in a honeycomb lattice quantum XY magnet CoTiO3 https://t.co/DXLcOaTGZf Dirac magnonなんてのがあるんか	2019/7/5 14:02	https://arxiv.org/abs/1907.02061	Dirac magnons in a honeycomb lattice quantum XY magnet CoTiO3	The discovery of massless Dirac electrons in graphene and topologicalDirac-Weyl materials has prompted a broad search for bosonic analogues of suchDirac particles. Recent experiments have found evidence for Dirac magnons abovean Ising-like ferromagnetic ground state in a two-dimensional (2D) kagomelattice magnet and in the van der Waals layered honeycomb crystal CrI$_3$, andin a 3D Heisenberg magnet Cu$_3$TeO$_6$. Here we report on our inelasticneutron scattering investigation on large single crystals of a stackedhoneycomb lattice magnet CoTiO$_3$, which is part of a broad family of ilmenitematerials. The magnetically ordered ground state of CoTiO$_3$ featuresferromagnetic layers of Co$^{2+}$, stacked antiferromagnetically along the$c$-axis. We discover that the magnon dispersion relation exhibits strongeasy-plane exchange anisotropy and hosts a clear gapless Dirac cone along theedge of the 3D Brillouin zone. Our results establish CoTiO$_3$ as a modelpseudospin-$1/2$ material to study interacting Dirac bosons in a 3D quantum XYmagnet.	Strongly Correlated Electrons (cond-mat.str-el)	
norihitoishida	Norihito Ishida	263	521	Dance Dance Convolution（CNN+conditional LSTMでDDRの自動譜面作成） https://t.co/vSItn8u5rM  DeepXさんの実装 https://t.co/X6WRvvTGM1	2019/7/5 14:49	https://arxiv.org/abs/1703.06891	Dance Dance Convolution	Dance Dance Revolution (DDR) is a popular rhythm-based video game. Playersperform steps on a dance platform in synchronization with music as directed byon-screen step charts. While many step charts are available in standardizedpacks, players may grow tired of existing charts, or wish to dance to a songfor which no chart exists. We introduce the task of learning to choreograph.Given a raw audio track, the goal is to produce a new step chart. This taskdecomposes naturally into two subtasks: deciding when to place steps anddeciding which steps to select. For the step placement task, we combinerecurrent and convolutional neural networks to ingest spectrograms of low-levelaudio features to predict steps, conditioned on chart difficulty. For stepselection, we present a conditional LSTM generative model that substantiallyoutperforms n-gram and fixed-window approaches.	Machine Learning (cs.LG)	Multimedia (cs.MM);Neural and Evolutionary Computing (cs.NE);Sound (cs.SD);Machine Learning (stat.ML)
Naka_m	nakam	390	444	そういえば、弊研の機械学習論文が出ていた： https://t.co/fes6CCwBr8	2019/7/5 17:12	https://arxiv.org/abs/1907.00208	Deep Gamblers: Learning to Abstain with Portfolio Theory	We deal with the \textit{selective classification} problem(supervised-learning problem with a rejection option), where we want to achievethe best performance at a certain level of coverage of the data. We transformthe original $m$-class classification problem to $(m+1)$-class where the$(m+1)$-th class represents the model abstaining from making a prediction dueto uncertainty. Inspired by portfolio theory, we propose a loss function forthe selective classification problem based on the doubling rate of gambling. Weshow that minimizing this loss function has a natural interpretation asmaximizing the return of a \textit{horse race}, where a player aims to balancebetween betting on an outcome (making a prediction) when confident andreserving one's winnings (abstaining) when not confident. This loss functionallows us to train neural networks and characterize the uncertainty ofprediction in an end-to-end fashion. In comparison with previous methods, ourmethod requires almost no modification to the model inference algorithm orneural architecture. Experimentally, we show that our method can identify bothuncertain and outlier data points, and achieves strong results on SVHN andCIFAR10 at various coverages of the data.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
r9y9	山本りゅういち / Ryuichi Yamamoto	835	559	https://t.co/pXjrC83jDA fairseqのpaper出てるの最近まで知らなかったけど、extensibleに設計されてるのが良いよなあ。DeepVoice3の実装したときにコード参考にしたのがもう二年前で、それから大分進化してる。Pytorchベースだしapexでmix precision 学習/推論もサポートしてる。良さみがあるなあ	2019/7/5 18:58	https://arxiv.org/abs/1904.01038	fairseq: A Fast, Extensible Toolkit for Sequence Modeling	fairseq is an open-source sequence modeling toolkit that allows researchersand developers to train custom models for translation, summarization, languagemodeling, and other text generation tasks. The toolkit is based on PyTorch andsupports distributed training across multiple GPUs and machines. We alsosupport fast mixed-precision training and inference on modern GPUs. A demovideo can be found at this https URL	Computation and Language (cs.CL)	
powowowbtc	PoW! WoW! WoW! (Mining, Bitcoin, Blockchain)	433	84	2. 半減期を廃止し、難易度調整が閾値を超えた場合に採掘報酬を変動させる 3. 負の利子率を導入して総供給量を減少させる  論文のダウンロードはこちらから https://t.co/OG0jekguSV	2019/7/5 19:19	https://arxiv.org/abs/1801.06771	How to Make a Digital Currency on a Blockchain Stable	Bitcoin and other similar digital currencies on blockchains are not idealmeans for payment, because their prices tend to go up in the long term (thuspeople are incentivized to hoard those currencies), and to fluctuate widely inthe short term (thus people would want to avoid risks of losing values). Thereason why those blockchain currencies based on proof of work are unstable maybe found in their designs that the supplies of currencies do not respond totheir positive and negative demand shocks, as the authors have formulated inour past work. Continuing from our past work, this paper proposes minimalchanges to the design of blockchain currencies so that their market prices areautomatically stabilized, absorbing both positive and negative demand shocks ofthe currencies by autonomously controlling their supplies. Those changes are:1) limiting re-adjustment of proof-of-work targets, 2) making mining rewardsvariable according to the observed over-threshold changes of block intervals,and 3) enforcing negative interests to remove old coins in circulation. We havemade basic design checks and evaluations of these measures through simplesimulations. In addition to stabilization of prices, the proposed measures mayhave effects of making those currencies preferred means for payment bydisincentivizing hoarding, and improving sustainability of the currency systemsby making rewards to miners perpetual.	Computers and Society (cs.CY)	
shiku0304	しく	86	60	ガウス過程みたいに、エルミート行列の逆行列作るのがボトルネックになる系の方法は、量子ビット数が増えていけば、かなり進展しそうな気がします。 https://t.co/rMDwwgfw1O	2019/7/5 19:20	https://arxiv.org/abs/1806.11463	Bayesian Deep Learning on a Quantum Computer	Bayesian methods in machine learning, such as Gaussian processes, have greatadvantages com-pared to other techniques. In particular, they provide estimatesof the uncertainty associated with a prediction. Extending the Bayesianapproach to deep architectures has remained a major challenge. Recent resultsconnected deep feedforward neural networks with Gaussian processes, allowingtraining without backpropagation. This connection enables us to leverage aquantum algorithm designed for Gaussian processes and develop a new algorithmfor Bayesian deep learning on quantum computers. The properties of the kernelmatrix in the Gaussian process ensure the efficient execution of the corecomponent of the protocol, quantum matrix inversion, providing an at leastpolynomial speedup over classical algorithms. Furthermore, we demonstrate theexecution of the algorithm on contemporary quantum computers and analyze itsrobustness with respect to realistic noise models.	Quantum Physics (quant-ph)	Artificial Intelligence (cs.AI);Machine Learning (stat.ML)
guicho271828	(phd '(masataro . asai))	783	782	思いついたら取られてた https://t.co/ajoKTbtx2B	2019/7/5 19:59	https://arxiv.org/abs/1504.04658	Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network	Identification and extraction of singing voice from within musical mixturesis a key challenge in source separation and machine audition. Recently, deepneural networks (DNN) have been used to estimate 'ideal' binary masks forcarefully controlled cocktail party speech separation problems. However, it isnot yet known whether these methods are capable of generalizing to thediscrimination of voice and non-voice in the context of musical mixtures. Here,we trained a convolutional DNN (of around a billion parameters) to provideprobabilistic estimates of the ideal binary mask for separation of vocal soundsfrom real-world musical mixtures. We contrast our DNN results with moretraditional linear methods. Our approach may be useful for automatic removal ofvocal sounds from musical mixtures for 'karaoke' type applications.	Sound (cs.SD)	Machine Learning (cs.LG);Neural and Evolutionary Computing (cs.NE)
dmbrkp_	ダムブレークP	56	267	某氏が好きそうな話題 https://t.co/ZCQScg1Oza	2019/7/5 20:27	https://arxiv.org/abs/1907.02332	Light-Control of Localised Photo-Bio-Convection	Microorganismal motility is often characterised by complex responses toenvironmental physico-chemical stimuli. Although the biological basis of theseresponses is often not well understood, their exploitation already promisesnovel avenues to directly control the motion of living active matter at boththe individual and collective level. Here we leverage the phototactic abilityof the model microalga {\it Chlamydomonas reinhardtii} to precisely control thetiming and position of localised cell photo-accumulation, leading to thecontrolled development of isolated bioconvective plumes. This novel form ofphoto-bio-convection allows a precise, fast and reconfigurable control of thespatio-temporal dynamics of the instability and the ensuing globalrecirculation, which can be activated and stopped in real time. A simplecontinuum model accounts for the phototactic response of the suspension anddemonstrates how the spatio-temporal dynamics of the illumination field can beused as a simple external switch to produce efficient bio-mixing.	Biological Physics (physics.bio-ph)	Fluid Dynamics (physics.flu-dyn);Cell Behavior (q-bio.CB)
q9ac	將籠林檎 / ??	277	228	非慣性系にあるアハロノフボームリング中に閉じ込められたディラック粒子。  複雑だあ(´･ω･｀)  https://t.co/w0qsdDLE2w	2019/7/5 22:00	https://arxiv.org/abs/1907.00054	Topological and noninertial effects in an Aharonov-Bohm ring	In this paper, we study the influence of topological and noninertial effectson a Dirac particle confined in an Aharonov-Bohm (AB) ring. Next, we determinethe Dirac spinor and the energy spectrum for the relativistic bound states. Weobserve that this spectrum depends of the quantum number $n$, magnetic flux$\Phi$ of the ring, angular velocity $\omega$ of the rotating frame, and of theparameter $\eta$ associated to topology of the cosmic string spacetime. Weobserved also that this spectrum is a periodic function and grows of values infunction of $n$, $\Phi$, $\omega$ and $\eta$. In the nonrelativistic limit, weobtain the equation of motion for a particle confined in an AB ring under theinfluence of topological and noninertial effects, where this topologicaleffects are generated now by a conic space. However, unlike of the relativisticcase, the spectrum this equation depends linearly of the parameter $\omega$ anddecreases of values with the increase of such parameter. In special, weverified that in the absence of the topological and noninertial effects($\eta=1$ and $\omega=0$) we recuperate the spectrum of a particle confined inan AB ring ($\Phi\neq 0$) or in an usual 1D quantum ring ($\Phi=0$).	High Energy Physics - Theory (hep-th)	Quantum Physics (quant-ph)
q9ac	將籠林檎 / ??	277	228	スクイーズド光があれば超強結合なしで超放射相転移が起こる。  https://t.co/1DVBgQNI36	2019/7/5 22:02	https://arxiv.org/abs/1907.00522	Squeezed light induced symmetry breaking superradiant phase transition	We theoretically investigate the quantum phase transition in the collectivesystems of qubits in a high-quality cavity, which is driven by a squeezedlight. We show that the squeezed light induced symmetry breaking can result inquantum phase transition without the ultrastrong coupling requirement. Usingthe standard mean field theory, we derive the condition of the quantum phasetransition. Surprisingly, we show that there exists a tricritical point wherethe first- and second-order phase transitions meet. With specific atom-cavitycoupling strengths, both the first- and second-order phase transition can becontrolled by the squeezed light, leading to an optical switching from thenormal phase to the superradiant phase by just increasing the squeezed lightintensity. The signature of these phase transitions can be observed bydetecting the phase space Wigner function distribution with different profilescontrolled by the squeezed light intensity. Such superradiant phase transitioncan be implemented in various quantum systems, including atoms, quantum dotsand ions in optical cavities as well as the circuit quantum electrodynamicssystem.	Quantum Physics (quant-ph)	
q9ac	將籠林檎 / ??	277	228	デターミニスティックな光学系の上を光が伝搬する様子を説明するのにミュラー行列を使って計算する方法があるが、これでは伝搬にかかる位相を取り入れた議論ができない。そのポイントを改善したピュアなオプティクスの論文だなあ  https://t.co/WGPjbHzva3	2019/7/5 22:23	https://arxiv.org/abs/1907.00580	Two theorems on the outer product of input and output Stokes vectors for deterministic optical systems	$2\times2$ complex Jones matrix transforms two dimensional complex Jonesvectors into complex Jones vectors and accounts for phase introduced bydeterministic optical systems. On the other hand, Mueller-Jones matrixtransforms four parameter real Stokes vectors into four parameter real Stokesvectors that contain no information about phase. Previously, a $4\times4$complex matrix ($\mathbf{Z}$ matrix) was introduced. $\mathbf{Z}$ matrix isanalogous to the Jones matrix and it is also akin to the Mueller-Jones matrixby the relation $\mathbf{M}=\mathbf{Z}\mathbf{Z^*}$. It was shown that$\mathbf{Z}$ matrix transforms Stokes vectors (Stokes matrices) into complexvectors (complex matrices) that contain relevant phases besides the otherinformation. In this note it is shown that, for deterministic optical systems,there exist two relations between outer product of experimentally measured realinput-output Stokes vectors and complex vectors (matrices) that represent thepolarization state and phase of totally polarized output light.	Optics (physics.optics)	
nebusokuririri	かまたまる	4,775	546	これが最新のパブリッシュしたペーパーです。よろしければ引用してください。 https://t.co/iQlpKIXqTn	2019/7/5 22:24	https://arxiv.org/abs/1707.07702	A Smooth Exit from Eternal Inflation?	The usual theory of inflation breaks down in eternal inflation. We derive adual description of eternal inflation in terms of a deformed Euclidean CFTlocated at the threshold of eternal inflation. The partition function gives theamplitude of different geometries of the threshold surface in the no-boundarystate. Its local and global behavior in dual toy models shows that theamplitude is low for surfaces which are not nearly conformal to the roundthree-sphere and essentially zero for surfaces with negative curvature. Basedon this we conjecture that the exit from eternal inflation does not produce aninfinite fractal-like multiverse, but is finite and reasonably smooth.	High Energy Physics - Theory (hep-th)	Cosmology and Nongalactic Astrophysics (astro-ph.CO);General Relativity and Quantum Cosmology (gr-qc)
q9ac	將籠林檎 / ??	277	228	超絶縁体っていうのがジョセフソン接合素子のアレイを準備すると実現しうるらしい。超絶縁体相では抵抗が無限大になるらしいです。  https://t.co/E0EH7Y9EJ4	2019/7/5 22:36	https://arxiv.org/abs/1906.12265	Electrostatics of a superinsulator	"In 1978, to explain quark confinement in hadrons, 't Hooft coined the notionof ""superinsulator"", a hypothetical ground state endowed with infinite electricresistance and representing thus an extreme opposite to a superconductor. Incondensed matter, the superinsulating ground state was first predicted forJosephson junction arrays (JJA) and was rediscovered as a phase emerging at theinsulating side of the superconductor-insulator transition (SIT) insuperconducting films due to the duality of the phase-amplitude uncertaintyprinciple. Superinsulators, dissipationless Bose condensates of vortices retainan infinite resistance at finite temperatures and, as such, have been gainingan intense research attention. Here we investigate the response of asuperinsulator to a dc electric field and show that small fields, E<Ec1 arecompletely suppressed inside the material, a dual, electric version of theMeissner effect. Intermediate fields, Ec1<E<Ec2, penetrate the superinsulatoras electric filaments, realizing Polyakov's electric strings in compact QED, adual, electric version of the mixed state of superconductors, and, finally,large fields, E>Ec2, break down superinsulation completely. We report transportmeasurements in NbTiN films revealing both thresholds and their linear scalingwith the system size. We demonstrate that the asymptotically free behaviour ofquarks within mesons maps onto the metal-like behaviour of small films. Ourfindings open the route to measurements of Polyakov's string tension as afunction of the system's parameters, enabling the exploration of strongcoupling gauge theory concepts via desktop experiments."	Superconductivity (cond-mat.supr-con)	
q9ac	將籠林檎 / ??	277	228	量子熱力学のイントロダクション。古典熱力学とその量子論的フォーミュレーションからスタートして、最後には量子情報のプロセッシングの解析を取り扱っている。大学院生向けのテキストらしい。  https://t.co/IMBlulgPNa	2019/7/5 22:40	https://arxiv.org/abs/1907.01596	Quantum Thermodynamics: An introduction to the thermodynamics of quantum information	This book provides an introduction to the emerging field of quantumthermodynamics, with particular focus on its relation to quantum informationand its implications for quantum computers and next generation quantumtechnologies. The text, aimed at graduate level physics students with a workingknowledge of quantum mechanics and statistical physics, provides a briefoverview of the development of classical thermodynamics and its quantumformulation in Chapter 1. Chapter 2 then explores typical thermodynamicsettings, such as cycles and work extraction protocols, when the workingmaterial is genuinely quantum. Finally, Chapter 3 explores the thermodynamicsof quantum information processing and introduces the reader to some morestate-of-the-art topics in this exciting and rapidly developing research field.	Quantum Physics (quant-ph)	Mesoscale and Nanoscale Physics (cond-mat.mes-hall);Quantum Gases (cond-mat.quant-gas);Statistical Mechanics (cond-mat.stat-mech)
q9ac	將籠林檎 / ??	277	228	電磁場中のDirac fermionと創発的ゲージ場中の中性fermionが双対な関係になっているらしい  https://t.co/P3CvjLfPcM	2019/7/5 22:46	https://arxiv.org/abs/1907.01501	Fermion-fermion duality in 3+1 dimensions	Dualities play a central role in both quantum field theories and condensedmatter systems. Recently, a web of dualities has been discovered in 2+1dimensions. Here, we propose in particular a generalization of the Son'sfermion-fermion duality to 3+1 dimensions. We show that the action of chargedDirac fermions coupled to an external electromagnetic field is dual to anaction of neutral fermions minimally coupled to an emergent vector gauge field.This dual action contains also a further tensor (Kalb-Ramond) gauge fieldcoupled to the emergent and electromagnetic vector potentials. We firstlydemonstrate the duality in the massive case. We then show the duality in thecase of massless fermions starting from a lattice model and employing theslave-rotor approach already used in the 2+1-dimensional duality [Burkov, Phys.Rev. B 99, 035124 (2019)]. We finally apply this result to 3D Dirac semimetalsin the low-energy regime. Besides the implications in topological phases ofmatter, our results shed light on the possible existence of a novel web ofdualities in 3+1-dimensional (non-supersymmetric) quantum field theories.	Mesoscale and Nanoscale Physics (cond-mat.mes-hall)	High Energy Physics - Theory (hep-th)
math_phys	N(eutral).W(-boson).	1,267	340	"https://t.co/Yfm23nLVnx SSBを伴う低エネルギー有効場の理論が、機械学習と関わりあるよ、という主張。gauge理論と時系列モデルが関係していて、損失関数を""gauge不変""にする事で収束を早められる、とか述べられている。詳細は読んでみないと分からない。"	2019/7/5 22:50	https://arxiv.org/abs/1907.02163	A Quantum Field Theory of Representation Learning	Continuous symmetries and their breaking play a prominent role incontemporary physics. Effective low-energy field theories around symmetrybreaking states explain diverse phenomena such as superconductivity, magnetism,and the mass of nucleons. We show that such field theories can also be a usefultool in machine learning, in particular for loss functions with continuoussymmetries that are spontaneously broken by random initializations. In thispaper, we illuminate our earlier published work (Bamler & Mandt, 2018) on thistopic more from the perspective of theoretical physics. We show that theanalogies between superconductivity and symmetry breaking in temporalrepresentation learning are rather deep, allowing us to formulate a gaugetheory of `charged' embedding vectors in time series models. We show thatmaking the loss function gauge invariant speeds up convergence in such models.	Machine Learning (stat.ML)	Statistical Mechanics (cond-mat.stat-mech);Machine Learning (cs.LG)
q9ac	將籠林檎 / ??	277	228	非線形媒質中の電磁場の揺らぎに関して、熱的な寄与を取り扱うのに、時空の揺らぎ→光円錐の揺らぎを考える  https://t.co/vcdkZsmFs0	2019/7/5 23:01	https://arxiv.org/abs/1907.00706	Lightcone fluctuations in a nonlinear medium due to thermal fluctuations	We study the flight time fluctuations of a probe light propagating in a slabof nonlinear optical material with an effective fluctuating refractive indexcaused by thermal fluctuations of background photons at a temperature $T$,which are analogous to the lightcone fluctuations due to fluctuating spacetimegeometry when gravity is quantized. A smoothly varying second ordersusceptibility is introduced, which results in that background field modeswhose wavelengths are of the order of the thickness of the slab give the maincontribution. We show that, in the low-temperature limit, the contribution ofthermal fluctuations to the flight time fluctuations is proportional to $T^4$,which is a small correction compared with the contributions from vacuumfluctuations, while in the high-temperature limit, the contribution of thermalfluctuations increases linearly with $T$, which dominates over that of vacuumfluctuations. Numerical estimation shows that, in realistic situations, thecontributions from thermal fluctuations are still small compared with that fromvacuum fluctuations even at room temperature.	General Relativity and Quantum Cosmology (gr-qc)	High Energy Physics - Theory (hep-th);Quantum Physics (quant-ph)
q9ac	將籠林檎 / ??	277	228	ナノ粒子が熱輸送をエンハンスするという話。電磁場のグリーン関数を使った計算。  https://t.co/vwvUGzrIyS	2019/7/5 23:09	https://arxiv.org/abs/1907.00828	Modal Approach to the Theory of Energy Transfer Mediated by a Metallic Nanosphere	Theoretically, the presence of a metallic nanoparticle enhances theintermolecular energy transfer. We calculate this enhancement factor with amodal approach pertaining analytical results in the case of a nanosphere. Wecalculate the Green's function of the system relaying on the spectralproperties of the electrostatic operator, fully known for spherical geometry.In contrast to other treatments, the present calculations are straightforwardfor any molecular orientation giving modal information about the response ofthe system. Numerical calculations and further discussions are also provided.	Mesoscale and Nanoscale Physics (cond-mat.mes-hall)	Classical Physics (physics.class-ph)
noan6251	ホテルバルティック（クローン）@築25年	849	846	これはAnnals of Statisticsの論文でFirst authorはE. Cand?sです（論文は別にfollowしてません）． https://t.co/h9zBSkLWsM	2019/7/5 23:16	https://arxiv.org/abs/0803.3136	Rejoinder: The Dantzig selector: Statistical estimation when $p$ is much larger than $n$	Rejoinder to ``The Dantzig selector: Statistical estimation when $p$ is muchlarger than $n$'' [math/0506081]	Statistics Theory (math.ST)	
re_hako_moon	はこつき＠VR	50	57	https://t.co/EWpedQsFtI The Perfect Match: 三次元点群ための学習ベースの特徴量を提案。局所点群をボクセル化したあとに3D CNNで学習する。対応する局所形状同士の特徴量の差が小さく、対応しない局所形状同士の特徴量の差が大きくなるようにロスを定義する。	2019/7/5 23:18	https://arxiv.org/abs/1811.06879	The Perfect Match: 3D Point Cloud Matching with Smoothed Densities	We propose 3DSmoothNet, a full workflow to match 3D point clouds with asiamese deep learning architecture and fully convolutional layers using avoxelized smoothed density value (SDV) representation. The latter is computedper interest point and aligned to the local reference frame (LRF) to achieverotation invariance. Our compact, learned, rotation invariant 3D point clouddescriptor achieves 94.9% average recall on the 3DMatch benchmark data set,outperforming the state-of-the-art by more than 20 percent points with only 32output dimensions. This very low output dimension allows for near realtimecorrespondence search with 0.1 ms per feature point on a standard PC. Ourapproach is sensor- and sceneagnostic because of SDV, LRF and learning highlydescriptive features with fully convolutional layers. We show that 3DSmoothNettrained only on RGB-D indoor scenes of buildings achieves 79.0% average recallon laser scans of outdoor vegetation, more than double the performance of ourclosest, learning-based competitors. Code, data and pre-trained models areavailable online at this https URL.	Computer Vision and Pattern Recognition (cs.CV)	
tmasada	Tomonari MASADA	474	67	"""In a Variational Auto-Decoder framework, the approximate posterior distribution is not parameterized by a neural network, but rather using a well-known distribution directly.""…て、それ普通の変分ベイズですけど? https://t.co/K8mEss9UVN"	2019/7/5 23:28	https://arxiv.org/abs/1903.00840	Variational Auto-Decoder	Learning a generative model from partial data (data with missingness) is achallenging area of machine learning research. We study a specificimplementation of the Auto-Encoding Variational Bayes (AEVB) algorithm, namedin this paper as a Variational Auto-Decoder (VAD). VAD is a generic frameworkwhich uses Variational Bayes and Markov Chain Monte Carlo (MCMC) methods tolearn a generative model from partial data. The main distinction between VADand Variational Auto-Encoder (VAE) is the encoder component, as VAD does nothave one. Using a proposed efficient inference method from a multivariateGaussian approximate posterior, VAD models allow inference to be performed viasimple gradient ascent rather than MCMC sampling from a probabilistic decoder.This technique reduces the inference computational cost, allows for using morecomplex optimization techniques during latent space inference (which are shownto be crucial due to a high degree of freedom in the VAD latent space), andkeeps the framework simple to implement. Through extensive experiments overseveral datasets and different missing ratios, we show that encoders cannotefficiently marginalize the input volatility caused by imputed missing values.We study multimodal datasets in this paper, which is a particular area ofimpact for VAD models.	Machine Learning (cs.LG)	Artificial Intelligence (cs.AI);Machine Learning (stat.ML)
yoshi_and_aki	Yoshi-aki Shimada	631	832	化学式から超伝導になるかどうかを機械学習で予測すると、人より当たるし、新物資(物質としては知られていたが超伝導になるかどうかは分かって無かった)も見つけられるとな。固体物理の歴史を無視してるというよりかは、「周期表すげー」と読む感じ。 https://t.co/UAW6RXTUg9	2019/7/6 0:10	https://arxiv.org/abs/1812.01995	Deep Learning Model for Finding New Superconductors	Superconductivity has been extensively studied since its discovery in 1911.However, the feasibility of room-temperature superconductivity is unknown.There is no theory of high-temperature superconductors and there are nocomputational methods for strongly correlated systems, in whichhigh-temperature superconductivity emerges. Exploration of new superconductorsstill relies on the experience and intuition of experts, and is largely aprocess of experimental trial and error. In one study, only 3\% of thecandidate materials showed superconductivity~\cite{1468-6996-16-3-033503}. Herewe report an interdisciplinary attempt for finding new superconductors based ondeep learning. We represented the periodic table in a way that allows a deeplearning model to learn it. Although we used only the chemical composition ofmaterials as information, we obtained an $R^{2}$ value of 0.92 for predictingthe superconducting transition temperature, $T_\text{c}$, for materials in adatabase of superconductors. We obtained three remarkable results. The deeplearning method can predict superconductivity for a material with a precisionof 55\%, which shows the usefulness of the model; it found the recentlydiscovered superconductor \ce{CaBi2}, which is not in the superconductordatabase; and it found Fe-based high-temperature superconductors (discovered in2008) from the training data before 2008. These results open the way for thediscovery of new high-temperature superconductor families.	Machine Learning (cs.LG)	Computation and Language (cs.CL)
Ryuhei_Mori	Mori	101	50	今さら FOCS の accepted papers から知った。 https://t.co/sGzUBOfMEn	2019/7/6 0:38	https://arxiv.org/abs/1901.11533	Reed-Muller codes polarize	Reed-Muller (RM) codes and polar codes are generated by the same matrix $G_m=\bigl[\begin{smallmatrix}1 & 0 \\ 1 & 1 \\ \end{smallmatrix}\bigr]^{\otimes m}$but using different subset of rows. RM codes select simply rows having largestweights. Polar codes select instead rows having the largest conditional mutualinformation proceeding top to down in $G_m$; while this is a more elaborate andchannel-dependent rule, the top-to-down ordering has the advantage of makingthe conditional mutual information polarize, giving directly acapacity-achieving code on any binary memoryless symmetric channel (BMSC). RMcodes are yet to be proved to have such property.In this paper, we reconnect RM codes to polarization theory. It is shown thatproceeding in the RM code ordering, i.e., not top-to-down but from the lightestto the heaviest rows in $G_m$, the conditional mutual information againpolarizes. We further demonstrate that it does so faster than for polar codes.This implies that $G_m$ contains another code, different than the polar codeand called here the twin code, that is provably capacity-achieving on any BMSC.This proves a necessary condition for RM codes to achieve capacity on BMSCs. Itfurther gives a sufficient condition if the rows with the largest conditionalmutual information correspond to the heaviest rows, i.e., if the twin code isthe RM code. We show here that the two codes bare similarity with each otherand give further evidence that they are likely the same.	Information Theory (cs.IT)	
shion_honda	Shion Honda	1,243	244	Negative Sampling on Link Prediction [Kotnis+, 2018, KBCOM] 知識グラフのlink predictionで負例サンプリングをすると未観測linkを負例としてしまう問題に対して、6種の工夫を複数モデルに適用し比較した。スパースなKGには埋め込みを用いる手法が効く。 https://t.co/r18uU3XnDZ #NowReading https://t.co/xYL0po36gW	2019/7/6 1:04	https://arxiv.org/abs/1708.06816	Analysis of the Impact of Negative Sampling on Link Prediction in Knowledge Graphs	"Knowledge graphs are large, useful, but incomplete knowledge repositories.They encode knowledge through entities and relations which define each otherthrough the connective structure of the graph. This has inspired methods forthe joint embedding of entities and relations in continuous low-dimensionalvector spaces, that can be used to induce new edges in the graph, i.e., linkprediction in knowledge graphs. Learning these representations relies oncontrasting positive instances with negative ones. Knowledge graphs includeonly positive relation instances, leaving the door open for a variety ofmethods for selecting negative examples. In this paper we present an empiricalstudy on the impact of negative sampling on the learned embeddings, assessedthrough the task of link prediction. We use state-of-the-art knowledge graphembeddings -- \rescal , TransE, DistMult and ComplEX -- and evaluate onbenchmark datasets -- FB15k and WN18. We compare well known methods fornegative sampling and additionally propose embedding based sampling methods. Wenote a marked difference in the impact of these sampling methods on the twodatasets, with the ""traditional"" corrupting positives method leading to bestresults on WN18, while embedding based methods benefiting the task on FB15k."	Artificial Intelligence (cs.AI)	
NieA7_3170	加藤恵と幸せな家庭を築きたい	486	208	https://t.co/2Rz76SWZrX まんまこれですが	2019/7/6 1:06	https://arxiv.org/abs/1812.04342	Learning latent representations for style control and transfer in end-to-end speech synthesis	In this paper, we introduce the Variational Autoencoder (VAE) to anend-to-end speech synthesis model, to learn the latent representation ofspeaking styles in an unsupervised manner. The style representation learnedthrough VAE shows good properties such as disentangling, scaling, andcombination, which makes it easy for style control. Style transfer can beachieved in this framework by first inferring style representation through therecognition network of VAE, then feeding it into TTS network to guide the stylein synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse intraining, several techniques are adopted. Finally, the proposed model showsgood performance of style control and outperforms Global Style Token (GST)model in ABX preference tests on style transfer.	Computation and Language (cs.CL)	Sound (cs.SD);Audio and Speech Processing (eess.AS)
sammy_suyama	須山敦志 Suyama Atsushi	10,049	166	dropoutが使えないなら変分推論を使えばいいじゃない Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning https://t.co/4qWem9y5PM	2019/7/6 1:42	https://arxiv.org/abs/1506.02142	Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning	Deep learning tools have gained tremendous attention in applied machinelearning. However such tools for regression and classification do not capturemodel uncertainty. In comparison, Bayesian models offer a mathematicallygrounded framework to reason about model uncertainty, but usually come with aprohibitive computational cost. In this paper we develop a new theoreticalframework casting dropout training in deep neural networks (NNs) as approximateBayesian inference in deep Gaussian processes. A direct result of this theorygives us tools to model uncertainty with dropout NNs -- extracting informationfrom existing models that has been thrown away so far. This mitigates theproblem of representing uncertainty in deep learning without sacrificing eithercomputational complexity or test accuracy. We perform an extensive study of theproperties of dropout's uncertainty. Various network architectures andnon-linearities are assessed on tasks of regression and classification, usingMNIST as an example. We show a considerable improvement in predictivelog-likelihood and RMSE compared to existing state-of-the-art methods, andfinish by using dropout's uncertainty in deep reinforcement learning.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
fudoumyousan	宮島	191	162	(Google DeepMind) https://t.co/CJzMieRXO2 他者の心を類推し、理解する能力についての「心の理論」に基づいた振る舞いが出来るようなモデルを深層強化学習で実現．	2019/7/6 1:51	https://arxiv.org/abs/1802.07740	Machine Theory of Mind	"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans'ability to represent the mental states of others, including their desires,beliefs, and intentions. We propose to train a machine to build such modelstoo. We design a Theory of Mind neural network -- a ToMnet -- which usesmeta-learning to build models of the agents it encounters, from observations oftheir behaviour alone. Through this process, it acquires a strong prior modelfor agents' behaviour, as well as the ability to bootstrap to richerpredictions about agents' characteristics and mental states using only a smallnumber of behavioural observations. We apply the ToMnet to agents behaving insimple gridworld environments, showing that it learns to model random,algorithmic, and deep reinforcement learning agents from varied populations,and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer &Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can holdfalse beliefs about the world. We argue that this system -- which autonomouslylearns how to model other agents in its world -- is an important step forwardfor developing multi-agent AI systems, for building intermediating technologyfor machine-human interaction, and for advancing the progress on interpretableAI."	Artificial Intelligence (cs.AI)	
tomonarimitiyam	この先、みてぃなりです	225	262	https://t.co/Po7yxk1EHK このシミュレーションやったという人がいた。世間は狭いのお。 https://t.co/63Oky9nzmf	2019/7/6 3:19	https://arxiv.org/abs/1907.00977	Rapid early coeval star formation and assembly of the most massive galaxies in the universe	The current consensus on the formation and evolution of the brightest clustergalaxies is that their stellar mass forms early ($z \gtrsim 4$) in separategalaxies that then eventually assemble the main structure at late times ($z\lesssim 1$). However, advances in observational techniques have led to thediscovery of protoclusters out to $z \sim 7$, suggesting that the late-assemblypicture may not be fully complete. Using a combination of observationallyconstrained hydrodynamical and dark-matter-only simulations, we show that thestellar assembly time of a sub-set of brightest cluster galaxies occurs at highredshifts ($z > 3$) rather than at low redshifts ($z < 1$), as is commonlythough. We find that highly overdense protoclusters assemble their stellar massinto brightest cluster galaxies within $\sim 1$ $\mathrm{Gyr}$ of evolution --producing massive blue elliptical galaxies at high redshifts ($z \gtrsim 3$).We argue that there is a downsizing effect on the cluster scale wherein thebrightest cluster galaxies in the cores of the most-massive clusters assembleearlier than those in lower-mass clusters. The James Webb Space Telescope willbe able to detect and confirm our prediction in the near future, and we discussthe implications to constraining the value of $\sigma_\mathrm{8}$.	Astrophysics of Galaxies (astro-ph.GA)	
__dingdongbell	ロールパンナちゃん	1,127	549	https://t.co/SQ8T6kowp7 なんかよく分からんけど、力学次数は可算くらいしか可能性がないのに超越的なものもあるという話らしい。この可算集合の形が気になってきますね。	2019/7/6 3:57	https://arxiv.org/abs/1907.00675	A transcendental dynamical degree	We give an example of a dominant rational selfmap of the projective planewhose dynamical degree is a transcendental number.	Dynamical Systems (math.DS)	Algebraic Geometry (math.AG);Number Theory (math.NT)
mosaico	mosaico ioscinaga	569	86	Magnitude2019が終わりました。初めて会う人ばかりでした。前から気になっている数値実験 https://t.co/sMDRIc7Qqw について、Mさんから「あれは測度ではなくて超関数の枠組みでとらえるべきだ」という解釈を聞いて、一気に視界が開ける思いをしました。	2019/7/6 4:41	https://arxiv.org/abs/0910.5500	Heuristic and computer calculations for the magnitude of metric spaces	The notion of the magnitude of a compact metric space was considered inarXiv:0908.1582 with Tom Leinster, where the magnitude was calculated for linesegments, circles and Cantor sets. In this paper more evidence is presented fora conjectured relationship with a geometric measure theoretic valuation.Firstly, a heuristic is given for deriving this valuation by considering'large' subspaces of Euclidean space and, secondly, numerical approximations tothe magnitude are calculated for squares, disks, cubes, annuli, tori andSierpinski gaskets. The valuation is seen to be very close to the magnitude forthe convex spaces considered and is seen to be 'asymptotically' close for someother spaces.	Metric Geometry (math.MG)	Category Theory (math.CT)
dakuton	Yuji Tokuda	339	217	Data AugmentationでCutoutとMixupのメリットを兼ね備えたCutMixの提案。分類、物体検出タスクでtop-1/top-5 errorに改善がみられた。ROC曲線で100.0(+89.6)とか評価カンストしてる笑 CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features https://t.co/Q5PloIjz7N	2019/7/6 6:11	https://arxiv.org/abs/1905.04899	CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features	Regional dropout strategies have been proposed to enhance the performance ofconvolutional neural network classifiers. They have proved to be effective forguiding the model to attend on less discriminative parts of objects (\eg leg asopposed to head of a person), thereby letting the network generalize better andhave better object localization capabilities. On the other hand, currentmethods for regional dropout removes informative pixels on training images byoverlaying a patch of either black pixels or random noise. {Such removal is notdesirable because it leads to information loss and inefficiency duringtraining.} We therefore propose the CutMix augmentation strategy: patches arecut and pasted among training images where the ground truth labels are alsomixed proportionally to the area of the patches. By making efficient use oftraining pixels and \mbox{retaining} the regularization effect of regionaldropout, CutMix consistently outperforms the state-of-the-art augmentationstrategies on CIFAR and ImageNet classification tasks, as well as on theImageNet weakly-supervised localization task. Moreover, unlike previousaugmentation methods, our CutMix-trained ImageNet classifier, when used as apretrained model, results in consistent performance gains in Pascal detectionand MS-COCO image captioning benchmarks. We also show that CutMix improves themodel robustness against input corruptions and its out-of-distributiondetection performances.	Computer Vision and Pattern Recognition (cs.CV)	
cv2aaa	okayasu	32	51	Personlabらへんからそんなに手法変わってない? https://t.co/udd7VOdRaI #cvsaisentan	2019/7/6 16:26	https://arxiv.org/abs/1803.08225	PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model	We present a box-free bottom-up approach for the tasks of pose estimation andinstance segmentation of people in multi-person images using an efficientsingle-shot model. The proposed PersonLab model tackles both semantic-levelreasoning and object-part associations using part-based modeling. Our modelemploys a convolutional network which learns to detect individual keypoints andpredict their relative displacements, allowing us to group keypoints intoperson pose instances. Further, we propose a part-induced geometric embeddingdescriptor which allows us to associate semantic person pixels with theircorresponding person instance, delivering instance-level person segmentations.Our system is based on a fully-convolutional architecture and allows forefficient inference, with runtime essentially independent of the number ofpeople present in the scene. Trained on COCO data alone, our system achievesCOCO test-dev keypoint average precision of 0.665 using single-scale inferenceand 0.687 using multi-scale inference, significantly outperforming all previousbottom-up pose estimation systems. We are also the first bottom-up method toreport competitive results for the person class in the COCO instancesegmentation task, achieving a person category average precision of 0.417.	Computer Vision and Pattern Recognition (cs.CV)	
IxtlanJourney	ixtlan journey	48	422	BERTのアテンションが何を見ているのかを解析した論文。依存タイプの判別精度の表を見ると、まだだいぶ改善の余地があることが分かる。 What Does BERT Look At? An Analysis of BERT's Attention https://t.co/Ybt5rIBD6t	2019/7/6 17:44	https://arxiv.org/abs/1906.04341	What Does BERT Look At? An Analysis of BERT's Attention	Large pre-trained neural networks such as BERT have had great recent successin NLP, motivating a growing body of research investigating what aspects oflanguage they are able to learn from unlabeled data. Most recent analysis hasfocused on model outputs (e.g., language model surprisal) or internal vectorrepresentations (e.g., probing classifiers). Complementary to these works, wepropose methods for analyzing the attention mechanisms of pre-trained modelsand apply them to BERT. BERT's attention heads exhibit patterns such asattending to delimiter tokens, specific positional offsets, or broadlyattending over the whole sentence, with heads in the same layer oftenexhibiting similar behaviors. We further show that certain attention headscorrespond well to linguistic notions of syntax and coreference. For example,we find heads that attend to the direct objects of verbs, determiners of nouns,objects of prepositions, and coreferent mentions with remarkably high accuracy.Lastly, we propose an attention-based probing classifier and use it to furtherdemonstrate that substantial syntactic information is captured in BERT'sattention.	Computation and Language (cs.CL)	
ririshida	いっし	189	334	これ完全理解したい Types and Forms of Emergence https://t.co/OXdfZOMje8	2019/7/6 20:15	https://arxiv.org/abs/nlin/0506028	[nlin/0506028] Types and Forms of Emergence	The knowledge of the different types of emergence is essential if we want tounderstand and master complex systems in science and engineering, respectively.This paper specifies a universal taxonomy and comprehensive classification ofthe major types and forms of emergence in Multi-Agent Systems, from simpletypes of intentional and predictable emergence in machines to more complexforms of weak, multiple and strong emergence.	Adaptation and Self-Organizing Systems (nlin.AO)	Pattern Formation and Solitons (nlin.PS)
hrsma2i	ヒ??シです。	159	354	各タスクのlossの割合が均等になるように、自動で重みを決める。 https://t.co/aXmxGF5MwH	2019/7/6 21:21	https://arxiv.org/abs/1705.07115	Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics	Numerous deep learning applications benefit from multi-task learning withmultiple regression and classification objectives. In this paper we make theobservation that the performance of such systems is strongly dependent on therelative weighting between each task's loss. Tuning these weights by hand is adifficult and expensive process, making multi-task learning prohibitive inpractice. We propose a principled approach to multi-task deep learning whichweighs multiple loss functions by considering the homoscedastic uncertainty ofeach task. This allows us to simultaneously learn various quantities withdifferent units or scales in both classification and regression settings. Wedemonstrate our model learning per-pixel depth regression, semantic andinstance segmentation from a monocular input image. Perhaps surprisingly, weshow our model can learn multi-task weightings and outperform separate modelstrained individually on each task.	Computer Vision and Pattern Recognition (cs.CV)	
tonagai	tomo	408	88	中心極限定理の間違った解説が話題ですが、超一般化中心極限定理というのもあったり。レヴィの安定分布が出てくる。 Super Generalized Central Limit Theorem: Limit distributions for sums of non-identical random variables with power-laws https://t.co/fGVBwU0iMC https://t.co/WpTioIznEL	2019/7/6 21:59	https://arxiv.org/abs/1702.02826	Super Generalized Central Limit Theorem: Limit distributions for sums of non-identical random variables with power-laws	In nature or societies, the power-law is present ubiquitously, and then it isimportant to investigate the mathematical characteristics of power-laws in therecent era of big data. In this paper we prove the superposition ofnon-identical stochastic processes with power-laws converges in density to aunique stable distribution. This property can be used to explain theuniversality of stable laws such that the sums of the logarithmic return ofnon-identical stock price fluctuations follow stable distributions.	Statistics Theory (math.ST)	General Economics (econ.GN);Mathematical Physics (math-ph)
tov_glowlight	同志ぐろーらいと	175	204	科学用語についてのロ英辞典  https://t.co/XnrkfKEztF	2019/7/6 22:42	https://arxiv.org/abs/math/0609472	[math/0609472] English Russian Scientific Dictionary	English Russian and Russian English dictionaries presented in this paper arededicated to help translate a scientific text from one language to another. Ialso included the bilingual name index into this book.	History and Overview (math.HO)	
kuto_bopro	きょうへい	67	86	1本目はやはりこれ。 Goodfellow氏が2014年に出したGANの論文。  GANの数式的理解を今日初めてしたんだけれど、最適化のところが面白くて、比較的単純な目的関数でGとDが訓練できることに驚いたなぁ。  GANの基本となる部分をざっくりではあるものの理解できたのは進歩。  https://t.co/2lEs8WmT0G	2019/7/6 23:15	https://arxiv.org/abs/1406.2661	Generative Adversarial Networks	We propose a new framework for estimating generative models via anadversarial process, in which we simultaneously train two models: a generativemodel G that captures the data distribution, and a discriminative model D thatestimates the probability that a sample came from the training data rather thanG. The training procedure for G is to maximize the probability of D making amistake. This framework corresponds to a minimax two-player game. In the spaceof arbitrary functions G and D, a unique solution exists, with G recovering thetraining data distribution and D equal to 1/2 everywhere. In the case where Gand D are defined by multilayer perceptrons, the entire system can be trainedwith backpropagation. There is no need for any Markov chains or unrolledapproximate inference networks during either training or generation of samples.Experiments demonstrate the potential of the framework through qualitative andquantitative evaluation of the generated samples.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
taketo1024	さのたけと	6,353	248	そろそろこれも読めるかな??  [1001.0354] Topological Quantum Information, Khovanov Homology and the Jones Polynomial https://t.co/TQ6aQgnIBW	2019/7/6 23:58	https://arxiv.org/abs/1001.0354	Topological Quantum Information, Khovanov Homology and the Jones Polynomial	In this paper we give a quantum statistical interpretation for the bracketpolynomial state sum <K> and for the Jones polynomial. We use this quantummechanical interpretation to give a new quantum algorithm for computing theJones polynomial. This algorithm is useful for its conceptual simplicity, andit applies to all values of the polynomial variable that lie on the unit circlein the complex plane. Letting C(K) denote the Hilbert space for this model,there is a natural unitary transformation U from C(K) to itself such that <K> =<F|U|F> where |F> is a sum over basis states for C(K). The quantum algorithmarises directly from this formula via the Hadamard Test. We then show that theframework for our quantum model for the bracket polynomial is a natural settingfor Khovanov homology. The Hilbert space C(K) of our model has basis inone-to-one correspondence with the enhanced states of the bracket statesummmation and is isomorphic with the chain complex for Khovanov homology withcoefficients in the complex numbers. We show that for the Khovanov boundaryoperator d defined on C(K) we have the relationship dU + Ud = 0. Consequently,the unitary operator U acts on the Khovanov homology, and we therefore obtain adirect relationship between Khovanov homology and this quantum algorithm forthe Jones polynomial. The formula for the Jones polynomial as a graded Eulercharacteristic is now expressed in terms of the eigenvalues of U and the Eulercharacteristics of the eigenspaces of U in the homology. The quantum algorithmgiven here is inefficient, and so it remains an open problem to determinebetter quantum algorithms that involve both the Jones polynomial and theKhovanov homology.	Geometric Topology (math.GT)	Mathematical Physics (math-ph)
azriel1rf	あずりえる	1,380	468	これ写真の識別タスクでAUC1.0なのかと思ったら、乱数から生成された画像セットのOOD検出でTNR at TPR0.95が1.0だった(AUCは0.997????)。  CutMixの論文に出てくるGaussianやUniformの意味はOOD検出の論文に書いてあった。 https://t.co/wQypbEuxKa https://t.co/Ntg8qLhNjY	2019/7/7 0:13	https://arxiv.org/abs/1802.04865	Learning Confidence for Out-of-Distribution Detection in Neural Networks	Modern neural networks are very powerful predictive models, but they areoften incapable of recognizing when their predictions may be wrong. Closelyrelated to this is the task of out-of-distribution detection, where a networkmust determine whether or not an input is outside of the set on which it isexpected to safely perform. To jointly address these issues, we propose amethod of learning confidence estimates for neural networks that is simple toimplement and produces intuitively interpretable outputs. We demonstrate thaton the task of out-of-distribution detection, our technique surpasses recentlyproposed techniques which construct confidence based on the network's outputdistribution, without requiring any additional labels or access toout-of-distribution examples. Additionally, we address the problem ofcalibrating out-of-distribution detectors, where we demonstrate thatmisclassified in-distribution examples can be used as a proxy forout-of-distribution examples.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
kinako_bourbon	きなこかすてら	23	55	重力は斉しく万物に働くが近年、この力はエントロピック力だと解釈する向きがある（2010のhttps://t.co/HwJtbIwt2a論文を見よ。）	2019/7/7 0:57	https://arxiv.org/abs/1001.0785	On the Origin of Gravity and the Laws of Newton	Starting from first principles and general assumptions Newton's law ofgravitation is shown to arise naturally and unavoidably in a theory in whichspace is emergent through a holographic scenario. Gravity is explained as anentropic force caused by changes in the information associated with thepositions of material bodies. A relativistic generalization of the presentedarguments directly leads to the Einstein equations. When space is emergent evenNewton's law of inertia needs to be explained. The equivalence principle leadsus to conclude that it is actually this law of inertia whose origin isentropic.	High Energy Physics - Theory (hep-th)	
sugar_underkey	さーたー	280	349	内容的にはこれと同じか https://t.co/ZPutfXrgFF  群ζ、ドストライクだ	2019/7/7 0:57	https://arxiv.org/abs/math/0011267	[math/0011267] Analytic properties of zeta functions and subgroup growth	In this paper we introduce some new methods to understand the analyticbehaviour of the zeta function of a group. We can then combine this knowledgewith suitable Tauberian theorems to deduce results about the growth ofsubgroups in a nilpotent group. In order to state our results we introduce thefollowing notation. For \alpha a real number and N a nonnegative integer,defines_N^\alpha(G) = sum_{n=1}^N a_n(G)/n^\alpha.Main Theorem: Let G be a finitely generated nilpotent infinite group.(1) The abscissa of convergence \alpha(G) of \zeta_G(s) is a rational numberand \zeta_G(s) can be meromorphically continued to Re(s)>\alpha(G)-\delta forsome \delta >0. The continued function is holomorphic on the line \Re(s) =(\alpha)G except for a pole at s=\alpha(G).(2) There exist a nonnegative integer b(G) and some real numbers c,c' suchthats_{N}(G) ~ c N^{\alpha(G)}(\log N)^{b(G)}s_{N}^{\alpha(G)}(G) ~ c' (\log N)^{b(G)+1}for N\rightarrow \infty .	Group Theory (math.GR)	
Spike23645	Spike	188	248	https://t.co/UtXEolkBJa  なんとなく面白そうな論文を見つけました。  Abstからすると、DLの原理を群論の視点から解説してるみたい。  読んでみよう。  ｢物理学とディープラーニング｣みたいに面白そうな話かも。	2019/7/7 1:47	https://arxiv.org/abs/1412.6621	Why does Deep Learning work? - A perspective from Group Theory	Why does Deep Learning work? What representations does it capture? How dohigher-order representations emerge? We study these questions from theperspective of group theory, thereby opening a new approach towards a theory ofDeep learning.One factor behind the recent resurgence of the subject is a key algorithmicstep called pre-training: first search for a good generative model for theinput samples, and repeat the process one layer at a time. We show deeperimplications of this simple principle, by establishing a connection with theinterplay of orbits and stabilizers of group actions. Although the neuralnetworks themselves may not form groups, we show the existence of {\em shadow}groups whose elements serve as close approximations.Over the shadow groups, the pre-training step, originally introduced as amechanism to better initialize a network, becomes equivalent to a search forfeatures with minimal orbits. Intuitively, these features are in a way the {\emsimplest}. Which explains why a deep learning network learns simple featuresfirst. Next, we show how the same principle, when repeated in the deeperlayers, can capture higher order representations, and why representationcomplexity increases as the layers get deeper.	Machine Learning (cs.LG)	Neural and Evolutionary Computing (cs.NE);Machine Learning (stat.ML)
hackernewsj	Hacker News記事題日本語翻訳	609	4	C ++ソフトウェアフレームワークのモデルチェック、ケーススタディ https://t.co/fMgwqWjWb1	2019/7/7 2:36	https://arxiv.org/abs/1907.00172	Model Checking a C++ Software Framework, a Case Study	This paper presents a case study on applying two model checkers, SPIN andDIVINE, to verify key properties of a C++ software framework, known as ADAPRO,originally developed at CERN. SPIN was used for verifying properties on thedesign level. DIVINE was used for verifying simple test applications thatinteracted with the implementation. Both model checkers were found to havetheir own respective sets of pros and cons, but the overall experience waspositive. Because both model checkers were used in a complementary manner, theyprovided valuable new insights into the framework, which would arguably havebeen hard to gain by traditional testing and analysis tools only. Translatingthe C++ source code into the modeling language of the SPIN model checker helpedto find flaws in the original design. With DIVINE, defects were found in partsof the code base that had already been subject to hundreds of hours of unittests, integration tests, and acceptance tests. Most importantly, modelchecking was found to be easy to integrate into the workflow of the softwareproject and bring added value, not only as verification, but also validationmethodology. Therefore, using model checking for developing library-level codeseems realistic and worth the effort.	Software Engineering (cs.SE)	
Spike23645	Spike	188	248	@Khronos2106 代数的構造で例をあげるなら、例えば商。群や環を可換群やイデアルで割って分解。  そんな発想に、機械学習でも似たのがある。  CNNっていう深層学習の1種(下画素)は、下みたいに画像の商を作っていく仕組みに見える面がある。  リンクは数学的に深層学習を読み解く論文。 https://t.co/UtXEolkBJa https://t.co/BAaVcch0MK	2019/7/7 3:30	https://arxiv.org/abs/1412.6621	Why does Deep Learning work? - A perspective from Group Theory	Why does Deep Learning work? What representations does it capture? How dohigher-order representations emerge? We study these questions from theperspective of group theory, thereby opening a new approach towards a theory ofDeep learning.One factor behind the recent resurgence of the subject is a key algorithmicstep called pre-training: first search for a good generative model for theinput samples, and repeat the process one layer at a time. We show deeperimplications of this simple principle, by establishing a connection with theinterplay of orbits and stabilizers of group actions. Although the neuralnetworks themselves may not form groups, we show the existence of {\em shadow}groups whose elements serve as close approximations.Over the shadow groups, the pre-training step, originally introduced as amechanism to better initialize a network, becomes equivalent to a search forfeatures with minimal orbits. Intuitively, these features are in a way the {\emsimplest}. Which explains why a deep learning network learns simple featuresfirst. Next, we show how the same principle, when repeated in the deeperlayers, can capture higher order representations, and why representationcomplexity increases as the layers get deeper.	Machine Learning (cs.LG)	Neural and Evolutionary Computing (cs.NE);Machine Learning (stat.ML)
pcneko5	K-neko@秋M3 検討中	279	364	読んでおきたい https://t.co/iiEl1EpibW	2019/7/7 7:08	https://arxiv.org/abs/1406.2661	Generative Adversarial Networks	We propose a new framework for estimating generative models via anadversarial process, in which we simultaneously train two models: a generativemodel G that captures the data distribution, and a discriminative model D thatestimates the probability that a sample came from the training data rather thanG. The training procedure for G is to maximize the probability of D making amistake. This framework corresponds to a minimax two-player game. In the spaceof arbitrary functions G and D, a unique solution exists, with G recovering thetraining data distribution and D equal to 1/2 everywhere. In the case where Gand D are defined by multilayer perceptrons, the entire system can be trainedwith backpropagation. There is no need for any Markov chains or unrolledapproximate inference networks during either training or generation of samples.Experiments demonstrate the potential of the framework through qualitative andquantitative evaluation of the generated samples.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
masahiro_sakai	Masahiro Sakai	1,794	1,333	AI Feynman: a Physics-Inspired Method for Symbolic Regression https://t.co/MV8HnkShrU データからの帰納的プログラム合成で物理法則を見つける話。次元解析やNNを代理モデルに用いての対称性や分割可能性の発見などを通じた単純化に工夫があり、遺伝的アルゴリズムを用いるEureqaよりも優れた性能	2019/7/7 10:46	https://arxiv.org/abs/1905.11481	AI Feynman: a Physics-Inspired Method for Symbolic Regression	A core challenge for both physics and artificial intellicence (AI) issymbolic regression: finding a symbolic expression that matches data from anunknown function. Although this problem is likely to be NP-hard in principle,functions of practical interest often exhibit symmetries, separability,compositionality and other simplifying properties. In this spirit, we develop arecursive multidimensional symbolic regression algorithm that combines neuralnetwork fitting with a suite of physics-inspired techniques. We apply it to 100equations from the Feynman Lectures on Physics, and it discovers all of them,while previous publicly available software cracks only 71; for a more difficulttest set, we improve the state of the art success rate from 15% to 90%.	Computational Physics (physics.comp-ph)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG);High Energy Physics - Theory (hep-th)
Kurahashi16	Taishi Kurahashi	194	43	数日前ですが、新しい論文を arXiv に投稿しました。 神戸大学の岩田君との共著です。 Fixed-point properties for predicate modal logics https://t.co/6a05yBx1Fc  ResearchGate にもおいてあります。 https://t.co/CoE6zKzLMR	2019/7/7 13:45	https://arxiv.org/abs/1907.00306	Fixed-point properties for predicate modal logics	It is well known that the propositional modal logic $\mathbf{GL}$ ofprovability satisfies the de Jongh-Sambin fixed-point property. On the otherhand, Montagna showed that the predicate modal system $\mathbf{QGL}$, which isthe natural variant of $\mathbf{GL}$, loses the fixed-point property. In thispaper, we discuss some versions of the fixed-point property for predicate modallogics. First, we prove that several extensions of $\mathbf{QGL}$ including$\mathbf{NQGL}$ do not have the fixed-point property. Secondly, we prove thefixed-point theorem for the logic $\mathbf{QK} + \Box^{n+1} \bot$. As aconsequence, we obtain that the class $\mathsf{BL}$ of Kripke frames which aretransitive and of bounded length satisfies the fixed-point property locally. Wealso show that the failure of the Craig interpolation property for$\mathbf{NQGL}$ follows from our results. Finally, we give a sufficientcondition for formulas to have a fixed-point in $\mathbf{QGL}$.	Logic (math.LO)	
tentenianbundl2	てん	118	147	観念してFock-Rosly https://t.co/7VEIEnEys6 を眺めるか(読めない)	2019/7/7 14:08	https://arxiv.org/abs/math/9802054	[math/9802054] Poisson structure on moduli of flat connections on Riemann surfaces and $r$-matrix	We consider the space of graph connections (lattice gauge fields) which canbe endowed with a Poisson structure in terms of a ciliated fat graph. (Aciliated fat graph is a graph with a fixed linear order of ends of edges ateach vertex.) Our aim is however to study the Poisson structure on the modulispace of locally flat vector bundles on a Riemann surface with holes (i.e. withboundary). It is shown that this moduli space can be obtained as a quotient ofthe space of graph connections by the Poisson action of a lattice gauge groupendowed with a Poisson-Lie structure. The present paper contains as a part anupdated version of a 1992 preprint ITEP-72-92 which we decided still deservespublishing. We have removed some obsolete inessential remarks and added somenewer ones.	Quantum Algebra (math.QA)	High Energy Physics - Theory (hep-th);Group Theory (math.GR);Representation Theory (math.RT)
dakuton	Yuji Tokuda	339	217	@azriel1rf ありがとうございます。OODをCIFAR-10,100で優劣つけるのはもう難しく、少し規模大きいもの使う必要あるのかなという印象です。下記論文のほうみてたので追加で読んでみます。 A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks https://t.co/FdfdGi1fbd	2019/7/7 14:14	https://arxiv.org/abs/1807.03888	A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks	Detecting test samples drawn sufficiently far away from the trainingdistribution statistically or adversarially is a fundamental requirement fordeploying a good classifier in many real-world machine learning applications.However, deep neural networks with the softmax classifier are known to producehighly overconfident posterior distributions even for such abnormal samples. Inthis paper, we propose a simple yet effective method for detecting any abnormalsamples, which is applicable to any pre-trained softmax neural classifier. Weobtain the class conditional Gaussian distributions with respect to (low- andupper-level) features of the deep models under Gaussian discriminant analysis,which result in a confidence score based on the Mahalanobis distance. Whilemost prior methods have been evaluated for detecting either out-of-distributionor adversarial samples, but not both, the proposed method achieves thestate-of-the-art performances for both cases in our experiments. Moreover, wefound that our proposed method is more robust in harsh cases, e.g., when thetraining dataset has noisy labels or small number of samples. Finally, we showthat the proposed method enjoys broader usage by applying it toclass-incremental learning: whenever out-of-distribution samples are detected,our classification rule can incorporate new classes well without furthertraining deep models.	Machine Learning (stat.ML)	Cryptography and Security (cs.CR);Machine Learning (cs.LG)
KSKSKSKS2	katsugeneration	178	445	推論の計算のみで、識別器がどこを見て画像のラベルを識別しているかを判定する Response-Based Visual Explanationのタスクで、CNNの識別精度を低下させず、既存の一般的なモデルや様々なタスクに簡単に応用できるVisual Explanation手法を提案 https://t.co/SmLegmmwfV	2019/7/7 15:05	https://arxiv.org/abs/1812.10025	Attention Branch Network: Learning of Attention Mechanism for Visual Explanation	Visual explanation enables human to understand the decision making of DeepConvolutional Neural Network (CNN), but it is insufficient to contribute theperformance improvement. In this paper, we focus on the attention map forvisual explanation, which represents high response value as the importantregion in image recognition. This region significantly improves the performanceof CNN by introducing an attention mechanism that focuses on a specific regionin an image. In this work, we propose Attention Branch Network (ABN), whichextends the top-down visual explanation model by introducing a branch structurewith an attention mechanism. ABN can be applicable to several image recognitiontasks by introducing a branch for attention mechanism and is trainable for thevisual explanation and image recognition in end-to-end manner. We evaluate ABNon several image recognition tasks such as image classification, fine-grainedrecognition, and multiple facial attributes recognition. Experimental resultsshow that ABN can outperform the accuracy of baseline models on these imagerecognition tasks while generating an attention map for visual explanation. Ourcode is available atthis https URL.	Computer Vision and Pattern Recognition (cs.CV)	
tmaehara	? ??	6,885	718	アルゴリズム系に出すときは断らず alphabetical にしていて ( https://t.co/G8tbhMQf0Q )，機械学習系に出すときはだいたい contribution 順にしてる．ただし alphabetical order にすることもあって，そういうときは明記してる ( https://t.co/GUAFNtyFIr )	2019/7/7 16:27	https://arxiv.org/abs/1807.04575, https://arxiv.org/abs/1901.08291	Algorithmic Meta-Theorems for Monotone Submodular Maximization, Pretending Fair Decisions via Stealthily Biased Sampling	We consider a monotone submodular maximization problem whose constraint isdescribed by a logic formula on a graph. Formally, we prove the following three`algorithmic metatheorems.'(1) If the constraint is specified by a monadic second-order logic on a graphof bounded treewidth, the problem is solved in $n^{O(1)}$ time with anapproximation factor of $O(\log n)$.(2) If the constraint is specified by a first-order logic on a graph of lowdegree, the problem is solved in $O(n^{1 + \epsilon})$ time for any $\epsilon >0$ with an approximation factor of $2$.(3) If the constraint is specified by a first-order logic on a graph ofbounded expansion, the problem is solved in $n^{O(\log k)}$ time with anapproximation factor of $O(\log k)$, where $k$ is the number of variables and$O(\cdot)$ suppresses only constants independent of $k$., Fairness by decision-makers is believed to be auditable by third parties. Inthis study, we show that this is not always true.We consider the following scenario. Imagine a decision-maker who discloses asubset of his dataset with decisions to make his decisions auditable. If he iscorrupt, and he deliberately selects a subset that looks fair even though theoverall decision is unfair, can we identify this decision-maker's fraud?We answer this question negatively. We first propose a sampling method thatproduces a subset whose distribution is biased from the original (to pretend tobe fair); however, its differentiation from uniform sampling is difficult. Wecall such a sampling method as stealthily biased sampling, which is formulatedas a Wasserstein distance minimization problem, and is solved through aminimum-cost flow computation. We proved that the stealthily biased samplingminimizes an upper-bound of the indistinguishability. We conducted experimentsto see that the stealthily biased sampling is, in fact, difficult to detect.	Data Structures and Algorithms (cs.DS), Machine Learning (stat.ML)	, Cryptography and Security (cs.CR);Machine Learning (cs.LG)
rkakamilan	rkakamilan	591	578	Prediction of Small Molecule Kinase Inhibitors for Chemotherapy Using Deep Learning  色々組み合わせも試しましたと言う話 https://t.co/XpOloHNhAI	2019/7/7 17:00	https://arxiv.org/abs/1907.00329	Prediction of Small Molecule Kinase Inhibitors for Chemotherapy Using Deep Learning	The current state of cancer therapeutics has been moving away fromone-size-fits-all cytotoxic chemotherapy, and towards a more individualized andspecific approach involving the targeting of each tumor's geneticvulnerabilities. Different tumors, even of the same type, may be more relianton certain cellular pathways more than others. With modern advancements in ourunderstanding of cancer genome sequencing, these pathways can be discovered.Investigating each of the millions of possible small molecule inhibitors foreach kinase in vitro, however, would be extremely expensive and time consuming.This project focuses on predicting the inhibition activity of small moleculestargeting 8 different kinases using multiple deep learning models. We trainedfingerprint-based MLPs and simplified molecular-input line-entry specification(SMILES)-based recurrent neural networks (RNNs) and molecular graphconvolutional networks (GCNs) to accurately predict inhibitory activitytargeting these 8 kinases.	Biomolecules (q-bio.BM)	Machine Learning (cs.LG)
ksasao	ミクミンP/Kazuhiro Sasao	7,779	2,632	UMAPの論文ながめた。 https://t.co/b8QdAsOLkl 解釈性が重要なときはPCAとかのほうがいいとか、アルゴリズム的に500件以下のデータセットでつかうときは気をつけろとか書いてあってよかった。Pythonで簡単に使える。 https://t.co/ADUJJjF9lt	2019/7/7 18:24	https://arxiv.org/abs/1802.03426	UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction	UMAP (Uniform Manifold Approximation and Projection) is a novel manifoldlearning technique for dimension reduction. UMAP is constructed from atheoretical framework based in Riemannian geometry and algebraic topology. Theresult is a practical scalable algorithm that applies to real world data. TheUMAP algorithm is competitive with t-SNE for visualization quality, andarguably preserves more of the global structure with superior run timeperformance. Furthermore, UMAP has no computational restrictions on embeddingdimension, making it viable as a general purpose dimension reduction techniquefor machine learning.	Machine Learning (stat.ML)	Computational Geometry (cs.CG);Machine Learning (cs.LG)
asakura_nazuna	朝桜なづな	23	23	@Spike23645 Bayesian CycleGANは一応あるみたいです(私の勉強不足で中身は理解できていませんが…)  https://t.co/coN7CCVxdt	2019/7/7 21:04	https://arxiv.org/abs/1811.07465	Bayesian CycleGAN via Marginalizing Latent Sampling	Recent techniques built on Generative Adversarial Networks (GANs) likeCycleGAN are able to learn mappings between domains from unpaired datasetsthrough min-max optimization games between generators and discriminators.However, it remains challenging to stabilize training process and diversifygenerated results. To address these problems, we present a Bayesian extensionof cyclic model and an integrated cyclic framework for inter-domain mappings.The proposed method stimulated by Bayesian GAN explores the full posteriors ofBayesian cyclic model (with latent sampling) and optimizes the model withmaximum a posteriori (MAP) estimation. Hence, we name it {\tt BayesianCycleGAN}. We perform the proposed Bayesian CycleGAN on multiple benchmarkdatasets, including Cityscapes, Maps, and Monet2photo. The quantitative andqualitative evaluations demonstrate the proposed method can achieve more stabletraining, superior performance and diversified images generating.	Machine Learning (cs.LG)	Computer Vision and Pattern Recognition (cs.CV);Machine Learning (stat.ML)
candidusflumen	Toshihico　Shiracawa	1,503	2,015	「Detecting problematic transactions in a consumer-to-consumer e-commerce network」 eコマースのネットワークについて分析した研究のようだ → https://t.co/EMIQbgsC5b	2019/7/7 22:00	https://arxiv.org/abs/1906.07974	Detecting problematic transactions in a consumer-to-consumer e-commerce network	Providers of online marketplaces are constantly combatting againstproblematic transactions, such as selling illegal items and posting fictiveitems, exercised by some of their users. A typical approach to detect fraudactivity has been to analyze registered user profiles, user's behavior, andtexts attached to individual transactions and the user. However, thistraditional approach may be limited because malicious users can easily concealtheir information. Given this background, network indices have been exploitedfor detecting frauds in various online transaction platforms. In the presentstudy, we analyzed networks of users of an online consumer-to-consumermarketplace in which a seller and the corresponding buyer of a transaction areconnected by a directed edge. We constructed egocentric networks of each ofseveral hundreds of fraudulent users and those of a similar number of normalusers. We calculated eight local network indices based on up to connectivitybetween the neighbors of the focal node. Based on the present descriptiveanalysis of these network indices, we fed twelve features that we constructedfrom the eight network indices to random forest classifiers with the aim ofdistinguishing between normal users and fraudulent users engaged in each one ofthe four types of problematic transactions. We found that the classifieraccurately distinguished the fraudulent users from normal users and that theclassification performance did not depend on the type of problematictransaction.	Social and Information Networks (cs.SI)	
asakura_nazuna	朝桜なづな	23	23	@Spike23645 DNNも隠れ層のノードが無限のとき、ガウス過程として表現できるようですよ  https://t.co/vzK8iD4ZCC	2019/7/7 22:04	https://arxiv.org/abs/1711.00165	Deep Neural Networks as Gaussian Processes	It has long been known that a single-layer fully-connected neural networkwith an i.i.d. prior over its parameters is equivalent to a Gaussian process(GP), in the limit of infinite network width. This correspondence enables exactBayesian inference for infinite width neural networks on regression tasks bymeans of evaluating the corresponding GP. Recently, kernel functions whichmimic multi-layer random neural networks have been developed, but only outsideof a Bayesian framework. As such, previous work has not identified that thesekernels can be used as covariance functions for GPs and allow fully Bayesianprediction with a deep neural network.In this work, we derive the exact equivalence between infinitely wide deepnetworks and GPs. We further develop a computationally efficient pipeline tocompute the covariance function for these GPs. We then use the resulting GPs toperform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.We observe that trained neural network accuracy approaches that of thecorresponding GP with increasing layer width, and that the GP uncertainty isstrongly correlated with trained network prediction error. We further find thattest performance increases as finite-width trained networks are made wider andmore similar to a GP, and thus that GP predictions typically outperform thoseof finite-width networks. Finally we connect the performance of these GPs tothe recent theory of signal propagation in random neural networks.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
KSKSKSKS2	katsugeneration	178	445	誇張した表現の人物画であるカリカチュアを写真から生成するタスクで、Style Transfer的なテクスチャで表現されるスタイルの適用だけでなく、各パーツのジオメトリックな変換も学習により獲得し、任意の特徴を持つカリカチュアのスタイルを画像に適用し変換できる手法を提案 https://t.co/M2d4x9qCQ8	2019/7/7 22:04	https://arxiv.org/abs/1811.10100	WarpGAN: Automatic Caricature Generation	We propose, WarpGAN, a fully automatic network that can generate caricaturesgiven an input face photo. Besides transferring rich texture styles, WarpGANlearns to automatically predict a set of control points that can warp the photointo a caricature, while preserving identity. We introduce anidentity-preserving adversarial loss that aids the discriminator to distinguishbetween different subjects. Moreover, WarpGAN allows customization of thegenerated caricatures by controlling the exaggeration extent and the visualstyles. Experimental results on a public domain dataset, WebCaricature, showthat WarpGAN is capable of generating a diverse set of caricatures whilepreserving the identities. Five caricature experts suggest that caricaturesgenerated by WarpGAN are visually similar to hand-drawn ones and only prominentfacial features are exaggerated.	Computer Vision and Pattern Recognition (cs.CV)	
kuto_bopro	きょうへい	67	86	2本目はKaggleでも現在使用しているDCGANの論文。GANの不安定性を解決するためにCNNやバッチ正則化などを用いた例。  潜在変数Zで生成する画像の特徴を変えれるのが面白い。  潜在空間の内部探索の理解がいまいちなのでそれはまた追々。 https://t.co/ZNKWPNCzX9	2019/7/7 22:36	https://arxiv.org/abs/1511.06434	Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks	In recent years, supervised learning with convolutional networks (CNNs) hasseen huge adoption in computer vision applications. Comparatively, unsupervisedlearning with CNNs has received less attention. In this work we hope to helpbridge the gap between the success of CNNs for supervised learning andunsupervised learning. We introduce a class of CNNs called deep convolutionalgenerative adversarial networks (DCGANs), that have certain architecturalconstraints, and demonstrate that they are a strong candidate for unsupervisedlearning. Training on various image datasets, we show convincing evidence thatour deep convolutional adversarial pair learns a hierarchy of representationsfrom object parts to scenes in both the generator and discriminator.Additionally, we use the learned features for novel tasks - demonstrating theirapplicability as general image representations.	Machine Learning (cs.LG)	Computer Vision and Pattern Recognition (cs.CV)
raven_0717	烏（からす）	105	281	2012年、Physical Review Lettersに『ポニーテールの形状と髪の繊維束の統計物理』というタイトルの論文が発表された。 https://t.co/RhOZctSpeG ポニーテールは量子力学を凌駕し、サイエンステクノロジーを開拓し、もはや人類の英知として存在する。ようするにポニテばんじゃーい ∩(・ω・)∩	2019/7/7 23:59	https://arxiv.org/abs/1204.0371v1	[1204.0371v1] The shape of a ponytail and the statistical physics of hair fiber bundles	A general continuum theory for the distribution of hairs in a bundle isdeveloped, treating individual fibers as elastic filaments with randomintrinsic curvatures. Applying this formalism to the iconic problem of theponytail, the combined effects of bending elasticity, gravity, andorientational disorder are recast as a differential equation for the envelopeof the bundle, in which the compressibility enters through an 'equation ofstate'. From this, we identify the balance of forces in various regions of theponytail, extract a remarkably simple equation of state from laboratorymeasurements of human ponytails, and relate the pressure to the measured randomcurvatures of individual hairs.	Statistical Mechanics (cond-mat.stat-mech)	Soft Condensed Matter (cond-mat.soft)
ssk_nozmi	ssk	30	67	ツイッタで知ったマクスウェルの悪魔の倒し方が載ってるらしい資料 https://t.co/yydXHp375Y	2019/7/8 0:01	https://arxiv.org/abs/1110.4732	Maxwell's Demon and Data Compression	In an asymmetric Szilard engine model of Maxwell's demon, we show theequivalence between information theoretical and thermodynamic entropies whenthe demon erases information optimally. The work gain by the engine can beexactly canceled out by the work necessary to reset demon's memory afteroptimal data compression a la Shannon before the erasure.	Classical Physics (physics.class-ph)	Statistical Mechanics (cond-mat.stat-mech);History and Philosophy of Physics (physics.hist-ph)
maomaocyucyu	まおきゅ	592	560	@udoooom @import_godiva 初心者なので間違っていたら恐縮ですが、テスト云々という部分はネタ元のhttps://t.co/71ckU3PHCxが、学習後別に推論を走らせて出力を見るテストをしている関係で出てきた記載でGAN一般の話ではないかもです。ご指摘ありがとうございます！関連する実装は以下です https://t.co/uQVdutPswO	2019/7/8 1:00	https://arxiv.org/abs/1611.07004	Image-to-Image Translation with Conditional Adversarial Networks	We investigate conditional adversarial networks as a general-purpose solutionto image-to-image translation problems. These networks not only learn themapping from input image to output image, but also learn a loss function totrain this mapping. This makes it possible to apply the same generic approachto problems that traditionally would require very different loss formulations.We demonstrate that this approach is effective at synthesizing photos fromlabel maps, reconstructing objects from edge maps, and colorizing images, amongother tasks. Indeed, since the release of the pix2pix software associated withthis paper, a large number of internet users (many of them artists) have postedtheir own experiments with our system, further demonstrating its wideapplicability and ease of adoption without the need for parameter tweaking. Asa community, we no longer hand-engineer our mapping functions, and this worksuggests we can achieve reasonable results without hand-engineering our lossfunctions either.	Computer Vision and Pattern Recognition (cs.CV)	
hackernewsj	Hacker News記事題日本語翻訳	609	4	構成性における7つのスケッチ：応用カテゴリー理論への招待 https://t.co/QISoDV5LiY	2019/7/8 5:00	https://arxiv.org/abs/1803.05316	Seven Sketches in Compositionality: An Invitation to Applied Category Theory	This book is an invitation to discover advanced topics in category theorythrough concrete, real-world examples. It aims to give a tour: a gentle, quickintroduction to guide later exploration. The tour takes place over sevensketches, each pairing an evocative application, such as databases, electriccircuits, or dynamical systems, with the exploration of a categoricalstructure, such as adjoint functors, enriched categories, or toposes.No prior knowledge of category theory is assumed.A feedback form for typos, comments, questions, and suggestions is availablehere:this https URL	Category Theory (math.CT)	
pasj_jp	日本加速器学会	115	11	KEKの大谷将士さんらの論文がプレプリントサーバで公開されました。 ミュオン加速用のバンチャー空洞について。 https://t.co/RTp8Dtq3O2 #加速器 https://t.co/lU8czgTcig	2019/7/8 6:10	https://arxiv.org/abs/1907.02235	Compact buncher cavity for muons accelerated by a radio-frequency quadrupole	A buncher cavity has been developed for the muons accelerated by aradio-frequency quadrupole linac (RFQ). The buncher cavity is designed for$\beta=v/c=0.04$ at an operational frequency of 324 MHz. It employs adouble-gap structure operated in the TEM mode for the required effectivevoltage with compact dimensions, in order to account for the limited space ofthe experiment. The measured resonant frequency and unloaded quality factor are323.95 MHz and $3.06\times10^3$, respectively. The buncher cavity wassuccessfully operated for longitudinal bunch size measurement of the muonsaccelerated by the RFQ.	Accelerator Physics (physics.acc-ph)	High Energy Physics - Experiment (hep-ex)
hrsma2i	ヒ??シです。	159	354	衣服の自動折りたたみロボットの目。位置の推定とキーポイントの推定をCNNで同時に学習。 https://t.co/KZcV3kID5G https://t.co/iiQlReJngM	2019/7/8 8:20	https://arxiv.org/abs/1907.00408	GarmNet: Improving Global with Local Perception for Robotic Laundry Folding	Developing autonomous assistants to help with domestic tasks is a vital topicin robotics research. Among these tasks, garment folding is one of them that isstill far from being achieved mainly due to the large number of possibleconfigurations that a crumpled piece of clothing may exhibit. Research has beendone on either estimating the pose of the garment as a whole or detecting thelandmarks for grasping separately. However, such works constrain the capabilityof the robots to perceive the states of the garment by limiting therepresentations for one single task. In this paper, we propose a novelend-to-end deep learning model named GarmNet that is able to simultaneouslylocalize the garment and detect landmarks for grasping. The localization of thegarment represents the global information for recognising the category of thegarment, whereas the detection of landmarks can facilitate subsequent graspingactions. We train and evaluate our proposed GarmNet model using the CloPeMaGarment dataset that contains 3,330 images of different garment types indifferent poses. The experiments show that the inclusion of landmark detection(GarmNet-B) can largely improve the garment localization, with an error rate of24.7% lower. Solutions as ours are important for robotics applications, asthese offer scalable to many classes, memory and processing efficientsolutions.	Robotics (cs.RO)	Computer Vision and Pattern Recognition (cs.CV)
jaguring1	小猫遊りょう（たかにゃし・りょう）	2,601	191	甘利先生が途中でしてるのは、Jacotたちのこの論文の話だと思う Neural Tangent Kernel: Convergence and Generalization in Neural Networks https://t.co/EDEB1BPkSB	2019/7/8 10:41	https://arxiv.org/abs/1806.07572	Neural Tangent Kernel: Convergence and Generalization in Neural Networks	At initialization, artificial neural networks (ANNs) are equivalent toGaussian processes in the infinite-width limit, thus connecting them to kernelmethods. We prove that the evolution of an ANN during training can also bedescribed by a kernel: during gradient descent on the parameters of an ANN, thenetwork function $f_\theta$ (which maps input vectors to output vectors)follows the kernel gradient of the functional cost (which is convex, incontrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel(NTK). This kernel is central to describe the generalization features of ANNs.While the NTK is random at initialization and varies during training, in theinfinite-width limit it converges to an explicit limiting kernel and it staysconstant during training. This makes it possible to study the training of ANNsin function space instead of parameter space. Convergence of the training canthen be related to the positive-definiteness of the limiting NTK. We prove thepositive-definiteness of the limiting NTK when the data is supported on thesphere and the non-linearity is non-polynomial.We then focus on the setting of least-squares regression and show that in theinfinite-width limit, the network function $f_\theta$ follows a lineardifferential equation during training. The convergence is fastest along thelargest kernel principal components of the input data with respect to the NTK,hence suggesting a theoretical motivation for early stopping.Finally we study the NTK numerically, observe its behavior for wide networks,and compare it to the infinite-width limit.	Machine Learning (cs.LG)	Neural and Evolutionary Computing (cs.NE);Probability (math.PR);Machine Learning (stat.ML)
ZFPhalanx	phalanx	1,489	414	多分これ DDFlow: Learning Optical Flow with Unlabeled Data Distillation https://t.co/ZHajtFHMmm	2019/7/8 12:22	https://arxiv.org/abs/1902.09145	DDFlow: Learning Optical Flow with Unlabeled Data Distillation	We present DDFlow, a data distillation approach to learning optical flowestimation from unlabeled data. The approach distills reliable predictions froma teacher network, and uses these predictions as annotations to guide a studentnetwork to learn optical flow. Unlike existing work relying on hand-craftedenergy terms to handle occlusion, our approach is data-driven, and learnsoptical flow for occluded pixels. This enables us to train our model with amuch simpler loss function, and achieve a much higher accuracy. We conduct arigorous evaluation on the challenging Flying Chairs, MPI Sintel, KITTI 2012and 2015 benchmarks, and show that our approach significantly outperforms allexisting unsupervised learning methods, while running at real time.	Computer Vision and Pattern Recognition (cs.CV)	
miwamaya_3	Maya3	22	193	https://t.co/15wqcWHxsE ACGAN はクラス間の overap を考慮しないのでクラス数が増えると生成の多様性が損なわれる欠陥がある． ACGAN の構造に新たにもう一つの分類器を追加するアイデアを提案． 追加の分類器は Fake のペアのみで学習し，クラス間のJS-divを最小化することで多様性の劣化を防ぐ． https://t.co/CiV2xolYPj	2019/7/8 13:43	https://arxiv.org/abs/1907.02690v1	[1907.02690v1] Twin Auxiliary Classifiers GAN	Conditional generative models enjoy remarkable progress over the past fewyears. One of the popular conditional models is Auxiliary Classifier GAN(AC-GAN), which generates highly discriminative images by extending the lossfunction of GAN with an auxiliary classifier. However, the diversity of thegenerated samples by AC-GAN tends to decrease as the number of classesincreases, hence limiting its power on large-scale data. In this paper, weidentify the source of the low diversity issue theoretically and propose apractical solution to solve the problem. We show that the auxiliary classifierin AC-GAN imposes perfect separability, which is disadvantageous when thesupports of the class distributions have significant overlap. To address theissue, we propose Twin Auxiliary Classifiers Generative Adversarial Net(TAC-GAN) that further benefits from a new player that interacts with otherplayers (the generator and the discriminator) in GAN. Theoretically, wedemonstrate that TAC-GAN can effectively minimize the divergence between thegenerated and real-data distributions. Extensive experimental results show thatour TAC-GAN can successfully replicate the true data distributions on simulateddata, and significantly improves the diversity of class-conditional imagegeneration on real datasets.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
komiya_atsushi	KOMIYA Atsushi	1,575	125	転置インデックスの圧縮表現に使える新しいアルゴリズムだ。 https://t.co/NKck7I0b8y	2019/7/8 13:45	https://arxiv.org/abs/1907.01032	On Slicing Sorted Integer Sequences	Representing sorted integer sequences in small space is a central problem forlarge-scale retrieval systems such as Web search engines. Efficient queryresolution, e.g., intersection or random access, is achieved by carefullypartitioning the sequences. In this work we describe and compare two differentpartitioning paradigms: partitioning by cardinality and partitioning byuniverse. Although the ideas behind such paradigms have been known in thecoding and algorithmic community since many years, inverted index compressionhas extensively adopted the former paradigm, whereas the latter has receivedonly little attention. As a result, an experimental comparison between thesetwo is missing for the setting of inverted index compression. We also proposeand implement a solution that recursively slices the universe of representationof a sequence to achieve compact storage and attain to fast query execution.Albeit larger than some state-of-the-art representations, this slicing approachsubstantially improves the performance of list intersections and unions whileoperating in compressed space, thus offering an excellent space/time trade-offfor the problem.	Information Retrieval (cs.IR)	Data Structures and Algorithms (cs.DS)
hiroosa	OSAWA, Hirotaka /WoD	1,612	919	機械学習における進化計算の影響。ここ2年の成果で、ぜんぶarXivってのが分野のスピードを感じさせる  https://t.co/8bV2gH06C0 https://t.co/Q4SAAUDbt0 https://t.co/j5DpnzHRF7	2019/7/8 14:16	https://arxiv.org/abs/1703.03864, https://arxiv.org/abs/1712.06567, https://arxiv.org/abs/1803.07055	Evolution Strategies as a Scalable Alternative to Reinforcement Learning, Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning, Simple random search provides a competitive approach to reinforcement learning	We explore the use of Evolution Strategies (ES), a class of black boxoptimization algorithms, as an alternative to popular MDP-based RL techniquessuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari showthat ES is a viable solution strategy that scales extremely well with thenumber of CPUs available: By using a novel communication strategy based oncommon random numbers, our ES implementation only needs to communicate scalars,making it possible to scale to over a thousand parallel workers. This allows usto solve 3D humanoid walking in 10 minutes and obtain competitive results onmost Atari games after one hour of training. In addition, we highlight severaladvantages of ES as a black box optimization technique: it is invariant toaction frequency and delayed rewards, tolerant of extremely long horizons, anddoes not need temporal discounting or value function approximation., Deep artificial neural networks (DNNs) are typically trained viagradient-based learning algorithms, namely backpropagation. Evolutionstrategies (ES) can rival backprop-based algorithms such as Q-learning andpolicy gradients on challenging deep reinforcement learning (RL) problems.However, ES can be considered a gradient-based algorithm because it performsstochastic gradient descent via an operation similar to a finite-differenceapproximation of the gradient. That raises the question of whethernon-gradient-based evolutionary algorithms can work at DNN scales. Here wedemonstrate they can: we evolve the weights of a DNN with a simple,gradient-free, population-based genetic algorithm (GA) and it performs well onhard deep RL problems, including Atari and humanoid locomotion. The Deep GAsuccessfully evolves networks with over four million free parameters, thelargest neural networks ever evolved with a traditional evolutionary algorithm.These results (1) expand our sense of the scale at which GAs can operate, (2)suggest intriguingly that in some cases following the gradient is not the bestchoice for optimizing performance, and (3) make immediately available themultitude of neuroevolution techniques that improve performance. We demonstratethe latter by showing that combining DNNs with novelty search, which encouragesexploration on tasks with deceptive or sparse reward functions, can solve ahigh-dimensional problem on which reward-maximizing algorithms (e.g.\ DQN, A3C,ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN(it can train Atari in ${\raise.17ex\hbox{$\scriptstyle\sim$}}$4 hours on onedesktop or ${\raise.17ex\hbox{$\scriptstyle\sim$}}$1 hour distributed on 720cores), and enables a state-of-the-art, up to 10,000-fold compact encodingtechnique., A common belief in model-free reinforcement learning is that methods based onrandom search in the parameter space of policies exhibit significantly worsesample complexity than those that explore the space of actions. We dispel suchbeliefs by introducing a random search method for training static, linearpolicies for continuous control problems, matching state-of-the-art sampleefficiency on the benchmark MuJoCo locomotion tasks. Our method also finds anearly optimal controller for a challenging instance of the Linear QuadraticRegulator, a classical problem in control theory, when the dynamics are notknown. Computationally, our random search algorithm is at least 15 times moreefficient than the fastest competing model-free methods on these benchmarks. Wetake advantage of this computational efficiency to evaluate the performance ofour method over hundreds of random seeds and many different hyperparameterconfigurations for each benchmark task. Our simulations highlight a highvariability in performance in these benchmark tasks, suggesting that commonlyused estimations of sample efficiency do not adequately evaluate theperformance of RL algorithms.	Machine Learning (stat.ML), Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG);Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Artificial Intelligence (cs.AI);Optimization and Control (math.OC);Machine Learning (stat.ML)
Quantum_Zen	全卓樹	1,629	98	"そおいえば守先生の新論文 https://t.co/Syuezqreir、Physical Review Eにもう掲載されますねえ。批評１（表面的的）：おもしろいイイ論文ですよこれ。  ""Voter model on networks and the multivariate beta distribution"" S. Mori, M. Hisakado, and K. Nakayama https://t.co/Cdr8PPKhOv"	2019/7/8 16:51	https://arxiv.org/abs/1810.05643	A voter model on networks and multivariate beta distribution	In elections, the vote shares or turnout rates show a strong spatialcorrelation. The logarithmic decay with distance suggests that a 2D noisydiffusive equation describes the system. Based on the study of U.S.presidential elections data, it was determined that the fluctuations of voteshares also exhibit a strong and long-range spatial correlation. Previously, itwas considered difficult to induce strong and long-range spatial correlation ofthe vote shares without breaking the empirically observed narrow distribution.We demonstrate that a voter model on networks shows such a behavior. In themodel, there are many voters in a node who are affected by the agents in thenode and by the agents in the linked nodes. A multivariate Wright-Fisherdiffusion equation for the joint probability density of the vote shares isderived. The stationary distribution is a multivariate generalization of thebeta distribution. In addition, we also estimate the equilibrium values and thecovariance matrix of the vote shares and obtain a correspondence with amultivariate normal distribution. This approach largely simplifies thecalibration of the parameters in the modeling of elections.	Physics and Society (physics.soc-ph)	Applications (stat.AP)
colomatico	ｺﾛﾏﾁｺ	152	312	[1811.03792v1] Rigorous analytical formula for freeform singlet lens design free of spherical aberration  あーかいぶ https://t.co/qu4JkJWcit	2019/7/8 17:39	https://arxiv.org/abs/1811.03792v1	[1811.03792v1] Rigorous analytical formula for freeform singlet lens design free of spherical aberration	An analytical closed-form formula for the design of freeform lenses free ofspherical aberration is presented. Given the equation of the freeform inputsurface, the formula gives the equation of the second surface in order tocorrect the spherical aberration. The derivation is based on the formalapplication of the variational Fermat principle under the standard geometricaloptics approximation.	Optics (physics.optics)	
naoki_shigehisa	しげ@プログラミング勉強中	59	165	https://t.co/3nAsD64Kj1 読んでみる。 ((英語苦手...	2019/7/8 18:04	https://arxiv.org/abs/1703.06870v1	[1703.06870v1] Mask R-CNN	We present a conceptually simple, flexible, and general framework for objectinstance segmentation. Our approach efficiently detects objects in an imagewhile simultaneously generating a high-quality segmentation mask for eachinstance. The method, called Mask R-CNN, extends Faster R-CNN by adding abranch for predicting an object mask in parallel with the existing branch forbounding box recognition. Mask R-CNN is simple to train and adds only a smalloverhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy togeneralize to other tasks, e.g., allowing us to estimate human poses in thesame framework. We show top results in all three tracks of the COCO suite ofchallenges, including instance segmentation, bounding-box object detection, andperson keypoint detection. Without tricks, Mask R-CNN outperforms all existing,single-model entries on every task, including the COCO 2016 challenge winners.We hope our simple and effective approach will serve as a solid baseline andhelp ease future research in instance-level recognition. Code will be madeavailable.	Computer Vision and Pattern Recognition (cs.CV)	
Kazk1018	Kazk1018	317	294	アルゴリズムによる採用候補者のスクリーニングにおけるバイアスに関する論文  https://t.co/tn3CwX6geP	2019/7/8 18:31	https://arxiv.org/abs/1906.09208v1	[1906.09208v1] Mitigating Bias in Algorithmic Employment Screening: Evaluating Claims and Practices	"There has been rapidly growing interest in the use of algorithms foremployment assessment, especially as a means to address or mitigate bias inhiring. Yet, to date, little is known about how these methods are being used inpractice. How are algorithmic assessments built, validated, and examined forbias? In this work, we document and assess the claims and practices ofcompanies offering algorithms for employment assessment, using a methodologythat can be applied to evaluate similar applications and issues of bias inother domains. In particular, we identify vendors of algorithmic pre-employmentassessments (i.e., algorithms to screen candidates), document what they havedisclosed about their development and validation procedures, and evaluate theirtechniques for detecting and mitigating bias. We find that companies'formulation of ""bias"" varies, as do their approaches to dealing with it. Wealso discuss the various choices vendors make regarding data collection andprediction targets, in light of the risks and trade-offs that these choicespose. We consider the implications of these choices and we raise a number oftechnical and legal considerations."	Computers and Society (cs.CY)	Artificial Intelligence (cs.AI);Machine Learning (cs.LG)
masashi162	masashi16	83	129	[論文メモ] GraphStar:?https://t.co/o4vci6bhVP https://t.co/o82hYoIqR2 GNNが層を重ね過ぎると、表現が一様になってしまう問題に対して、大域的な情報を持つstar(ノード)を導入。GNNは局所的な情報しか取り込まないが、starを通して、大域的な情報を取り込むことを提案。複数のタスクでSOTA。	2019/7/8 19:18	https://arxiv.org/abs/1906.12330	Graph Star Net for Generalized Multi-Task Learning	In this work, we present graph star net (GraphStar), a novel and unifiedgraph neural net architecture which utilizes message-passing relay andattention mechanism for multiple prediction tasks - node classification, graphclassification and link prediction. GraphStar addresses many earlier challengesfacing graph neural nets and achieves non-local representation withoutincreasing the model depth or bearing heavy computational costs. We alsopropose a new method to tackle topic-specific sentiment analysis based on nodeclassification and text classification as graph classification. Our work showsthat 'star nodes' can learn effective graph-data representation and improve oncurrent methods for the three tasks. Specifically, for graph classification andlink prediction, GraphStar outperforms the current state-of-the-art models by2-5% on several key benchmarks.	Social and Information Networks (cs.SI)	Computation and Language (cs.CL);Machine Learning (cs.LG)
masashi162	masashi16	83	129	PFNが以前出してた論文(https://t.co/XDDZwTmqrM)との違いって何なんだろう。考え方はとても似ている気がする。 Deep Graph Infomaxもそうだったけど、グローバルな情報をGNNにどのようにして付与するかが重要な気がする。	2019/7/8 19:19	https://arxiv.org/abs/1902.01020	Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks in Molecular Graph Analysis	Graph Neural Network (GNN) is a popular architecture for the analysis ofchemical molecules, and it has numerous applications in material and medicinalscience. Current lines of GNNs developed for molecular analysis, however, donot fit well on the training set, and their performance does not scale wellwith the complexity of the network. In this paper, we propose an auxiliarymodule to be attached to a GNN that can boost the representation power of themodel without hindering with the original GNN architecture. Our auxiliarymodule can be attached to a wide variety of GNNs, including those that are usedcommonly in biochemical applications. With our auxiliary architecture, theperformances of many GNNs used in practice improve more consistently, achievingthe state-of-the-art performance on popular molecular graph datasets.	Machine Learning (cs.LG)	Machine Learning (stat.ML)
Ay18OS18	まな板の上の鱸	55	51	action！ってこと？  [1907.02550] 4321... axion! https://t.co/ZAk4q1Z5QH	2019/7/8 19:51	https://arxiv.org/abs/1907.02550	4321... axion!	We analyze the strong CP problem and the implications for axion physics inthe context of $U_1$ vector leptoquark models, recently put forward as anelegant solution to the hints of lepton flavor universality violation inB-meson decays. It is shown that in minimal gauge models containing the $U_1$as a gauge boson, the Peccei-Quinn solution of the strong CP problem requiresthe introduction of two axions. Characteristic predictions for the associatedaxions can be deduced from the model parameter space hinted by B-physics,allowing the new axion sector to account for the dark matter of the Universe.We also provide a specific ultraviolet completion of the axion sector thatconnects the Peccei-Quinn mechanism to the generation of neutrino masses.	High Energy Physics - Phenomenology (hep-ph)	
re_hako_moon	はこつき＠VR	50	57	https://t.co/BN4UAOUONM Quantization-interval-learningの提案。NNの量子化を学習可能にすることで、精度を維持したまま省ビットなネットワークを実現した。入力の変化に応じた量子化間隔＆変化が激しい箇所に量子化の中心を移動するように学習する。	2019/7/8 20:39	https://arxiv.org/abs/1808.05779	Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss	Reducing bit-widths of activations and weights of deep networks makes itefficient to compute and store them in memory, which is crucial in theirdeployments to resource-limited devices, such as mobile phones. However,decreasing bit-widths with quantization generally yields drastically degradedaccuracy. To tackle this problem, we propose to learn to quantize activationsand weights via a trainable quantizer that transforms and discretizes them.Specifically, we parameterize the quantization intervals and obtain theiroptimal values by directly minimizing the task loss of the network. Thisquantization-interval-learning (QIL) allows the quantized networks to maintainthe accuracy of the full-precision (32-bit) networks with bit-width as low as4-bit and minimize the accuracy degeneration with further bit-width reduction(i.e., 3 and 2-bit). Moreover, our quantizer can be trained on a heterogeneousdataset, and thus can be used to quantize pretrained networks without access totheir training data. We demonstrate the effectiveness of our trainablequantizer on ImageNet dataset with various network architectures such asResNet-18, -34 and AlexNet, on which it outperforms existing methods to achievethe state-of-the-art accuracy.	Computer Vision and Pattern Recognition (cs.CV)	
kuto_bopro	きょうへい	67	86	1日1論文の3本目 Improved Technique for Training GANs  GANを効果的に学習するためのテクニックがいくつか紹介されており非常に参考になる論文  理解度は半分くらい。  https://t.co/yFNLgLdXRa	2019/7/8 21:17	https://arxiv.org/abs/1606.03498	Improved Techniques for Training GANs	We present a variety of new architectural features and training proceduresthat we apply to the generative adversarial networks (GANs) framework. We focuson two applications of GANs: semi-supervised learning, and the generation ofimages that humans find visually realistic. Unlike most work on generativemodels, our primary goal is not to train a model that assigns high likelihoodto test data, nor do we require the model to be able to learn well withoutusing any labels. Using our new techniques, we achieve state-of-the-art resultsin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generatedimages are of high quality as confirmed by a visual Turing test: our modelgenerates MNIST samples that humans cannot distinguish from real data, andCIFAR-10 samples that yield a human error rate of 21.3%. We also presentImageNet samples with unprecedented resolution and show that our methods enablethe model to learn recognizable features of ImageNet classes.	Machine Learning (cs.LG)	Computer Vision and Pattern Recognition (cs.CV);Neural and Evolutionary Computing (cs.NE)
myaunraitau	まうん	242	258	代名詞コンペの1stソリューションのACL論文が公開されてたので読んでる https://t.co/cK0GPqLNoo	2019/7/8 23:31	https://arxiv.org/abs/1906.00839	Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling	This paper presents a strong set of results for resolving gendered ambiguouspronouns on the Gendered Ambiguous Pronouns shared task. The model presentedhere draws upon the strengths of state-of-the-art language and coreferenceresolution models, and introduces a novel evidence-based deep learningarchitecture. Injecting evidence from the coreference models compliments thebase architecture, and analysis shows that the model is not hindered by theirweaknesses, specifically gender bias. The modularity and simplicity of thearchitecture make it very easy to extend for further improvement and applicableto other NLP problems. Evaluation on GAP test data results in astate-of-the-art performance at 92.5% F1 (gender bias of 0.97), edging closerto the human performance of 96.6%. The end-to-end solution presented hereplaced 1st in the Kaggle competition, winning by a significant lead. The codeis available at this https URL.	Computation and Language (cs.CL)	
cygnusgm	アルミニ	367	901	@kafukanoochan @UFOprofessor @gigazine arXivのプレプリント Submitted on 9 Nov 2018 https://t.co/DZ9p1JZB2W  Applied Optics誌も2018年11月号なので、内容はほぼ変わってないのかな？ Gigazineに載っている、あの凄まじい数式は見当たりません。	2019/7/8 23:41	https://arxiv.org/abs/1811.03792	Rigorous analytical formula for freeform singlet lens design free of spherical aberration	An analytical closed-form formula for the design of freeform lenses free ofspherical aberration is presented. Given the equation of the freeform inputsurface, the formula gives the equation of the second surface in order tocorrect the spherical aberration. The derivation is based on the formalapplication of the variational Fermat principle under the standard geometricaloptics approximation.	Optics (physics.optics)	
IxtlanJourney	ixtlan journey	48	422	バニラLSTMのgateの値は、実は0から1の間にブロードに分布している。そこで、gateにGumbel-Softmaxを用いて0か1に近づく様にする。精度が落ちず、圧縮が容易になる。  Towards Binary-Valued Gates for Robust LSTM Training https://t.co/bbJ0FYscB6	2019/7/8 23:56	https://arxiv.org/abs/1806.02988	Towards Binary-Valued Gates for Robust LSTM Training	Long Short-Term Memory (LSTM) is one of the most widely used recurrentstructures in sequence modeling. It aims to use gates to control informationflow (e.g., whether to skip some information or not) in the recurrentcomputations, although its practical implementation based on soft gates onlypartially achieves this goal. In this paper, we propose a new way for LSTMtraining, which pushes the output values of the gates towards 0 or 1. By doingso, we can better control the information flow: the gates are mostly open orclosed, instead of in a middle state, which makes the results moreinterpretable. Empirical studies show that (1) Although it seems that werestrict the model capacity, there is no performance drop: we achieve better orcomparable performances due to its better generalization ability; (2) Theoutputs of gates are not sensitive to their inputs: we can easily compress theLSTM unit in multiple ways, e.g., low-rank approximation and low-precisionapproximation. The compressed models are even better than the baseline modelswithout compression.	Machine Learning (cs.LG)	Computation and Language (cs.CL);Machine Learning (stat.ML)
takekujira_	野口篤史	368	92	おおー。 https://t.co/yq26oz2Z60	2019/7/9 0:11	https://arxiv.org/abs/1907.02554	Fault-tolerant thresholds for the surface code in excess of 5% under biased noise	Noise in quantum computing is countered with quantum error correction.Achieving optimal performance will require tailoring codes and decodingalgorithms to account for features of realistic noise, such as the commonsituation where the noise is biased towards dephasing. Here we introduce anefficient high-threshold decoder for a noise-tailored surface code based onminimum-weight perfect matching. The decoder exploits the symmetries of itssyndrome under the action of biased noise and generalises to the fault-tolerantregime where measurements are unreliable. Using this decoder, we obtainfault-tolerant thresholds in excess of $6\%$ for a phenomenological noise modelin the limit where dephasing dominates. These gains persist even for modestnoise biases: we find a threshold of $\sim 5\%$ in an experimentally relevantregime where dephasing errors occur at a rate one hundred times greater thanbit-flip errors.	Quantum Physics (quant-ph)	Strongly Correlated Electrons (cond-mat.str-el)
KSKSKSKS2	katsugeneration	178	445	文字やウォーターマークなどが挿入されている画像から、どのような物体が挿入されているかの情報無しで元画像を復元するBlind Motif Removalのタスクで、半透明なモチーフや複数のモチーフの除去を行える手法を提案。 https://t.co/iDrfYntLUz https://t.co/G4ovg1vmL5	2019/7/9 0:16	https://arxiv.org/abs/1904.02756	Blind Visual Motif Removal from a Single Image	Many images shared over the web include overlaid objects, or visual motifs,such as text, symbols or drawings, which add a description or decoration to theimage. For example, decorative text that specifies where the image was taken,repeatedly appears across a variety of different images. Often, the reoccurringvisual motif, is semantically similar, yet, differs in location, style andcontent (e.g. text placement, font and letters). This work proposes a deeplearning based technique for blind removal of such objects. In the blindsetting, the location and exact geometry of the motif are unknown. Our approachsimultaneously estimates which pixels contain the visual motif, and synthesizesthe underlying latent image. It is applied to a single input image, without anyuser assistance in specifying the location of the motif, achievingstate-of-the-art results for blind removal of both opaque and semi-transparentvisual motifs.	Computer Vision and Pattern Recognition (cs.CV)	Graphics (cs.GR);Machine Learning (cs.LG)
phys27_	うわばき	153	127	[gr-qc/9405057] General relativity as an effective field theory: The leading quantum corrections  これですかね https://t.co/bXNZDPKgEl	2019/7/9 0:36	https://arxiv.org/abs/gr-qc/9405057	[gr-qc/9405057] General relativity as an effective field theory: The leading quantum corrections	I describe the treatment of gravity as a quantum effective field theory. Thisallows a natural separation of the (known) low energy quantum effects from the(unknown) high energy contributions. Within this framework, gravity is a wellbehaved quantum field theory at ordinary energies. In studying the class ofquantum corrections at low energy, the dominant effects at large distance canbe isolated, as these are due to the propagation of the massless particles(including gravitons) of the theory and are manifested in thenonlocal/nonanalytic contributions to vertex functions and propagators. Theseleading quantum corrections are parameter-free and represent necessaryconsequences of quantum gravity. The methodology is illustrated by acalculation of the leading quantum corrections to the gravitational interactionof two heavy masses.	General Relativity and Quantum Cosmology (gr-qc)	High Energy Physics - Phenomenology (hep-ph);High Energy Physics - Theory (hep-th)
i4mwh4ti4m	たつお	312	489	一応当該論文投げとく https://t.co/oPnhDjwgy4	2019/7/9 2:58	https://arxiv.org/abs/1907.01376	Multi-scale GANs for Memory-efficient Generation of High Resolution Medical Images	Currently generative adversarial networks (GANs) are rarely applied tomedical images of large sizes, especially 3D volumes, due to their largecomputational demand. We propose a novel multi-scale patch-based GAN approachto generate large high resolution 2D and 3D images. Our key idea is to firstlearn a low-resolution version of the image and then generate patches ofsuccessively growing resolutions conditioned on previous scales. In a domaintranslation use-case scenario, 3D thorax CTs of size 512x512x512 and thoraxX-rays of size 2048x2048 are generated and we show that, due to the constantGPU memory demand of our method, arbitrarily large images of high resolutioncan be generated. Moreover, compared to common patch-based approaches, ourmulti-resolution scheme enables better image quality and prevents patchartifacts.	Image and Video Processing (eess.IV)	Computer Vision and Pattern Recognition (cs.CV)
GenerateTaiyaki	taiyaki	14	0	BigBiGAN https://t.co/AaqGCYFuwN BigGANと教師なし表現学習手法であるBiGANの組合せ.識別器には画像or潜在変数単独でのスコアを追加することで学習が安定化.教師なし表現学習やImageNetの条件なし生成でsota.強い生成モデルは表現学習に有用である上,良い特徴抽出モデルは生成器の性能を向上させる. https://t.co/6VDXqYM2Lf	2019/7/9 3:19	https://arxiv.org/abs/1907.02544	Large Scale Adversarial Representation Learning	Adversarially trained generative models (GANs) have recently achievedcompelling image synthesis results. But despite early successes in using GANsfor unsupervised representation learning, they have since been superseded byapproaches based on self-supervision. In this work we show that progress inimage generation quality translates to substantially improved representationlearning performance. Our approach, BigBiGAN, builds upon the state-of-the-artBigGAN model, extending it to representation learning by adding an encoder andmodifying the discriminator. We extensively evaluate the representationlearning and generation capabilities of these BigBiGAN models, demonstratingthat these generation-based models achieve the state of the art in unsupervisedrepresentation learning on ImageNet, as well as in unconditional imagegeneration.	Computer Vision and Pattern Recognition (cs.CV)	Machine Learning (cs.LG);Machine Learning (stat.ML)
icoxfog417	piqcy	8,906	127	画像の一部をDropするCutoutと、画像を合成するMixupを複合させて、Dropさせた箇所に別サンプルの画像をはめ込むCutMixという手法を提案。Cutoutは重要な特徴を落としてしまう可能性がある、Mixupは合成画像が不自然になるという双方の課題を克服し、双方を上回る精度を達成  https://t.co/BNqZPG3tS4	2019/7/9 8:26	https://arxiv.org/abs/1905.04899	CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features	Regional dropout strategies have been proposed to enhance the performance ofconvolutional neural network classifiers. They have proved to be effective forguiding the model to attend on less discriminative parts of objects (\eg leg asopposed to head of a person), thereby letting the network generalize better andhave better object localization capabilities. On the other hand, currentmethods for regional dropout removes informative pixels on training images byoverlaying a patch of either black pixels or random noise. {Such removal is notdesirable because it leads to information loss and inefficiency duringtraining.} We therefore propose the CutMix augmentation strategy: patches arecut and pasted among training images where the ground truth labels are alsomixed proportionally to the area of the patches. By making efficient use oftraining pixels and \mbox{retaining} the regularization effect of regionaldropout, CutMix consistently outperforms the state-of-the-art augmentationstrategies on CIFAR and ImageNet classification tasks, as well as on theImageNet weakly-supervised localization task. Moreover, unlike previousaugmentation methods, our CutMix-trained ImageNet classifier, when used as apretrained model, results in consistent performance gains in Pascal detectionand MS-COCO image captioning benchmarks. We also show that CutMix improves themodel robustness against input corruptions and its out-of-distributiondetection performances.	Computer Vision and Pattern Recognition (cs.CV)	
shunk031	しゅんけー	1,706	668	特許データ分析やってたからこういう論文は夢があって良いなぁと思う / Patent Claim Generation by Fine-Tuning OpenAI GPT-2. (arXiv:1907.02052v1 [https://t.co/w0CRTvevrb])  https://t.co/qrqdWY3YGz	2019/7/9 8:47	https://arxiv.org/abs/1907.02052	Patent Claim Generation by Fine-Tuning OpenAI GPT-2	In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model forgenerating patent claims. GPT-2 has demonstrated impressive efficacy ofpre-trained language models on various tasks, particularly coherent textgeneration. Patent claim language itself has rarely been explored in the pastand poses a unique challenge. We are motivated to generate coherent patentclaims automatically so that augmented inventing might be viable someday. Inour implementation, we identified a unique language structure in patent claimsand leveraged its implicit human annotations. We investigated the fine-tuningprocess by probing the first 100 steps and observing the generated text at eachstep. Based on both conditional and unconditional random sampling, we analyzethe overall quality of generated patent claims. Our contributions include: (1)being the first to generate patent claims by machines and being the first toapply GPT-2 to patent claim generation, (2) providing various experimentresults for qualitative analysis and future research, (3) proposing a newsampling approach for text generation, and (4) building an e-mail bot forfuture researchers to explore the fine-tuned GPT-2 model further.	Computation and Language (cs.CL)	Machine Learning (cs.LG);Machine Learning (stat.ML)
yoshikoba113	小林良彦（原子核物理・科学コミュニケーション）	104	25	今日（7月9日）はジョン・ホイーラー博士（1911～2008）が生まれた日。一般相対性理論や量子重力理論の研究で多くの功績を残した物理学者。ニールス・ボーア博士と核分裂の研究も行った。画像は https://t.co/ANpI6MbIL1 より。こちら（https://t.co/o9zDwsNVek）はキップ・ソーン博士による回想録。 https://t.co/EdCVqax0Ed	2019/7/9 9:36	https://arxiv.org/abs/1901.06623	John Archibald Wheeler: A Biographical Memoir	John Archibald Wheeler was a theoretical physicist who worked on bothdown-to-earth projects and highly speculative ideas, and always emphasized theimportance of experiment and observation, even when speculating wildly. Hisresearch and insights had large impacts on nuclear and particle physics, thedesign of nuclear weapons, general relativity and relativistic astrophysics,and quantum gravity and quantum information. But his greatest impacts werethrough the students, postdocs, and mature physicists whom he educated andinspired.He was guided by what he called the principle of radical conservatism,inspired by Niels Bohr: base your research on well established physical laws(be conservative), but push them into the most extreme conceivable domains (beradical). He often pushed far beyond the boundaries of well understood physics,speculating in prescient ways that inspired future generations of physicists.After completing his PhD with Karl Herzfeld at Johns Hopkins University(1933), Wheeler embarked on a postdoctoral year with Gregory Breit at NYU andanother with Niels Bohr in Copenhagen. He then moved to a three-year assistantprofessorship at the University of North Carolina (1935-37), followed by a 40year professorial career at Princeton University (1937-1976) and then ten yearsas a professor at the University of Texas, Austin (1976-1987). He returned toPrinceton in retirement but remained actively and intensely engaged withphysics right up to his death at age 96.	History and Philosophy of Physics (physics.hist-ph)	High Energy Astrophysical Phenomena (astro-ph.HE);General Relativity and Quantum Cosmology (gr-qc);High Energy Physics - Theory (hep-th);Nuclear Theory (nucl-th)
L_H_Sullivan	ふー??じん??	368	320	Pseudo-Spin Versus Magnetic Dipole Moment Ordering in the Isosceles Triangular Lattice Material K3Er(VO4)2 https://t.co/NNKjF7eMRc 薄い結晶	2019/7/9 9:51	https://arxiv.org/abs/1907.03016	Pseudo-Spin Versus Magnetic Dipole Moment Ordering in the Isosceles Triangular Lattice Material K$_3$Er(VO$_4$)$_2$	Spin-1/2 antiferromagnetic triangular lattice models are paradigms ofgeometrical frustration, revealing very different ground states and quantumeffects depending on the nature of anisotropies in the model. Due to strongspin orbit coupling and crystal field effects, rare-earth ions can formpseudo-spin-1/2 magnetic moments with anisotropic single-ion and exchangeproperties. Thus, rare-earth based triangular lattices enable the explorationof this interplay between frustration and anisotropy. Here we study one suchcase, the rare-earth double vanadate glaserite material K$_3$Er(VO$_4$)$_2$,which is a quasi-2D isosceles triangular antiferromagnet. Our specific heat andneutron powder diffraction data from K$_3$Er(VO$_4$)$_2$ reveal a transition tolong range magnetic order at 155 $\pm$ 5 mK which accounts for all R$\ln$2entropy. The quasi-2D magnetic order leads to anisotropic Warren-like Braggpeak profiles, and is best described by alternating layers of b-axis alignedantiferromagnetism and zero moment layers. Our magnetic susceptibility datareveal that Er$^{3+}$ takes on a strong XY single-ion anisotropy inK$_3$Er(VO$_4$)$_2$, leading to vanishing moments when pseudo-spins areoriented along c. Thus, the magnetic structure, when considered from thepseudo-spin point of view comprises alternating layers of b-axis and c-axisaligned antiferromagnetism.	Strongly Correlated Electrons (cond-mat.str-el)	
NIIIDR	NII IDR	266	57	「楽天データセット」に新規データを追加しました。https://t.co/AYbHL7BAWl 今回はRakuten Franceに掲載の書籍情報と，その一部に正規化した著者名表記を付加したデータです。「Who wrote this book? A challenge for e-commerce」の論文で使用されているものです。 https://t.co/88lUWahQXB	2019/7/9 10:49	https://arxiv.org/abs/1905.01973	Who wrote this book? A challenge for e-commerce	Modern e-commerce catalogs contain millions of references, associated withtextual and visual information that is of paramount importance for the productsto be found via search or browsing. Of particular significance is the bookcategory, where the author name(s) field poses a significant challenge. Indeed,books written by a given author (such as F. Scott Fitzgerald) might be listedwith different authors' names in a catalog due to abbreviations and spellingvariants and mistakes, among others. To solve this problem at scale, we designa composite system involving open data sources for books as well as machinelearning components leveraging deep learning-based techniques for naturallanguage processing. In particular, we use Siamese neural networks for anapproximate match with known author names, and direct correction of theprovided author's name using sequence-to-sequence learning with neuralnetworks. We evaluate this approach on product data from the e-commerce websiteRakuten France, and find that the top proposal of the system is the normalizedauthor name with 72% accuracy.	Computation and Language (cs.CL)	Information Retrieval (cs.IR);Machine Learning (cs.LG);Machine Learning (stat.ML)
sei_shinagawa	Seitaro Shinagawa	1,644	1,809	Unsupervised Image Captioning (CVPR2019) https://t.co/owGOtKN67U image--textのcycle系crazy論文じゃん。読むか	2019/7/9 11:00	https://arxiv.org/abs/1811.10787	Unsupervised Image Captioning	Deep neural networks have achieved great successes on the image captioningtask. However, most of the existing models depend heavily on pairedimage-sentence datasets, which are very expensive to acquire. In this paper, wemake the first attempt to train an image captioning model in an unsupervisedmanner. Instead of relying on manually labeled image-sentence pairs, ourproposed model merely requires an image set, a sentence corpus, and an existingvisual concept detector. The sentence corpus is used to teach the captioningmodel how to generate plausible sentences. Meanwhile, the knowledge in thevisual concept detector is distilled into the captioning model to guide themodel to recognize the visual concepts in an image. In order to furtherencourage the generated captions to be semantically consistent with the image,the image and caption are projected into a common latent space so that they canreconstruct each other. Given that the existing sentence corpora are mainlydesigned for linguistic research and are thus with little reference to imagecontents, we crawl a large-scale image description corpus of two millionnatural sentences to facilitate the unsupervised image captioning scenario.Experimental results show that our proposed model is able to produce quitepromising results without any caption annotations.	Computer Vision and Pattern Recognition (cs.CV)	
sei_shinagawa	Seitaro Shinagawa	1,644	1,809	What's to know? Uncertainty as a Guide to Asking Goal-oriented Questions (CVPR2019) Ehsan Abbasnejad, Qi Wu, Javen Shi, Anton van den Hengel https://t.co/J9BrrZvi5a これ良さそう	2019/7/9 11:18	https://arxiv.org/abs/1812.06401	What's to know? Uncertainty as a Guide to Asking Goal-oriented Questions	One of the core challenges in Visual Dialogue problems is asking the questionthat will provide the most useful information towards achieving the requiredobjective. Encouraging an agent to ask the right questions is difficult becausewe don't know a-priori what information the agent will need to achieve itstask, and we don't have an explicit model of what it knows already. We proposea solution to this problem based on a Bayesian model of the uncertainty in theimplicit model maintained by the visual dialogue agent, and in the functionused to select an appropriate output. By selecting the question that minimisesthe predicted regret with respect to this implicit model the agent activelyreduces ambiguity. The Bayesian model of uncertainty also enables a principledmethod for identifying when enough information has been acquired, and an actionshould be selected. We evaluate our approach on two goal-oriented dialoguedatasets, one for visual-based collaboration task and the other for anegotiation-based task. Our uncertainty-aware information-seeking modeloutperforms its counterparts in these two challenging problems.	Artificial Intelligence (cs.AI)	Computation and Language (cs.CL);Machine Learning (cs.LG)
ElectronNest		125	34	“Statistics &gt; Machine Learning To prune, or not to prune: exploring the efficacy of pruning for model compression” https://t.co/VMPMMKQm7g 式(1)が分からん。第一項がs_{t}でないとつじつまが合わない気がするけどどうなんじゃろ？	2019/7/9 11:42	https://arxiv.org/abs/1710.01878	To prune, or not to prune: exploring the efficacy of pruning for model compression	Model pruning seeks to induce sparsity in a deep neural network's variousconnection matrices, thereby reducing the number of nonzero-valued parametersin the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deepnetworks at the cost of only a marginal loss in accuracy and achieve a sizablereduction in model size. This hints at the possibility that the baseline modelsin these experiments are perhaps severely over-parameterized at the outset anda viable alternative for model compression might be to simply reduce the numberof hidden units while maintaining the model's dense connection structure,exposing a similar trade-off in model size and accuracy. We investigate thesetwo distinct paths for model compression within the context of energy-efficientinference in resource-constrained environments and propose a new gradualpruning technique that is simple and straightforward to apply across a varietyof models/datasets with minimal tuning and can be seamlessly incorporatedwithin the training process. We compare the accuracy of large, but prunedmodels (large-sparse) and their smaller, but dense (small-dense) counterpartswith identical memory footprint. Across a broad range of neural networkarchitectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we findlarge-sparse models to consistently outperform small-dense models and achieveup to 10x reduction in number of non-zero parameters with minimal loss inaccuracy.	Machine Learning (stat.ML)	Machine Learning (cs.LG)
